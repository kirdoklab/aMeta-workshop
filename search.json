[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "aMeta Workshop",
    "section": "",
    "text": "Welcome to the workshop on aMeta: an accurate and memory-efficient ancient Metagenomic profiling workflow that will be held at Mersin University, Department of Biotechnology within the scope of the European Union funded NEOMATRIX collaboration project.\nIn this web page, we aim to publish the tutorial material of the aMeta ancient metagenomic profiling workflow.\naMeta Pochon & Bergfeldt et al., 2022 is designed to analyse the metagenomic content of ancient samples from a wide variety of host organisms (including humans) or from environmental samples.\nPlease find the GitHub repository of the pipeline here.\nThe tutorial part of the course is designed for a specific system. So, if you want to follow this tutorial on your own, you have to install the tools by yourself either as environmental modules or by creating a conda environment as described in the official aMeta GitHub, and activate this environment before performing the different commands."
  },
  {
    "objectID": "course-style.html#using-conda-package-manager",
    "href": "course-style.html#using-conda-package-manager",
    "title": "2  How to understand snakemake?",
    "section": "2.1 Using conda package manager",
    "text": "2.1 Using conda package manager\nEach rule can use a specific conda subenvironment from the main aMeta conda environment. So, if you run snakemake with the --use-conda flag, the specified environment will be installed and used. In this case, the rule is using the fastqc conda subenvironment.\n    conda:\n        \"../envs/fastqc.yaml\""
  },
  {
    "objectID": "course-style.html#using-environment-modules",
    "href": "course-style.html#using-environment-modules",
    "title": "2  How to understand snakemake?",
    "section": "2.2 Using environment modules",
    "text": "2.2 Using environment modules\nIf you want to use environmental modules from your server instead of conda, you need to use the envmodules flag when running snakemake. In this case, you will have to create an envmodule file containing the information on how to load the environmental modules, as explained in the official aMeta GitHub.\nThis snakemake parameter looks for the specific information on how to load the right environment modules for the fastqc rule.\n envmodules:\n        *config[\"envmodules\"][\"fastqc\"]"
  },
  {
    "objectID": "course-style.html#benchmarks",
    "href": "course-style.html#benchmarks",
    "title": "2  How to understand snakemake?",
    "section": "2.3 Benchmarks",
    "text": "2.3 Benchmarks\nSome snakemake rules can generate benchmarking statistics and they store it in the benchmarks folder.\n    benchmark:\n        \"benchmarks/FASTQC_BEFORE_TRIMMING/{sample}.benchmark.txt\""
  },
  {
    "objectID": "course-style.html#logs",
    "href": "course-style.html#logs",
    "title": "2  How to understand snakemake?",
    "section": "2.4 Logs",
    "text": "2.4 Logs\nIf you have problems with your snakemake run, each rule will generate a log file. So you can check this file for any error message.\n    log:\n        \"logs/FASTQC_BEFORE_TRIMMING/{sample}.log\",\nSo much for the information about the specific parameters of snakemake in our rules!"
  },
  {
    "objectID": "rulegraph.html#ameta-workflow-graph",
    "href": "rulegraph.html#ameta-workflow-graph",
    "title": "3  aMeta workflow graph",
    "section": "3.1 aMeta workflow graph",
    "text": "3.1 aMeta workflow graph\nHere is an overview of all the rules of the aMeta snakemake pipeline. Such an overview of a workflow in the form of a graph is called a dag or rulegraph. You can consider these rules as steps of the workflow going from up to down.\n\n\n\n\n\nRulegraph"
  },
  {
    "objectID": "info-mersin.html",
    "href": "info-mersin.html",
    "title": "4  Information about the place",
    "section": "",
    "text": "Please find information on accomodation and transport in this Google Document.\nPlease find the food selection information in this Google Document. Please specify your selection if you have not done it yet."
  },
  {
    "objectID": "about.html#about-this-website",
    "href": "about.html#about-this-website",
    "title": "5  About",
    "section": "5.1 About this website",
    "text": "5.1 About this website\nThis website was built for the aMeta Neomatrix workshop, taking place in Mersin Feb 6 – 10 2023."
  },
  {
    "objectID": "about.html#the-team",
    "href": "about.html#the-team",
    "title": "5  About",
    "section": "5.2 The team",
    "text": "5.2 The team\n\n5.2.1 Emrah Kırdök\nEmrah Kırdök is an assistant professor at Mersin University.\nEmrah’s research focuses on understanding human history through ancient DNA (aDNA). Generally, a little proportion of aDNA sequences could be mapped to its host reference genome. The remaining fragments come either from the microbiome of the ancient host or the environment that encapsulates the host.\nBy analyzing the remaining aDNA sequences with the state-of-the-art metagenomic methods it’s possible to (i) construct the microbiome composition of ancient humans, (ii) reconstruct the ancient pathogenic genomes, (iii) understand the evolutionary mechanisms of the ancient pathogens.\n\n\n5.2.2 Zoé Pochon\nZoé started her PhD in Scientific archaeology at the Centre for Palaeogenetics, in Stockholm, in 2021. Zoé was trained in both biology and history and is interested in using archaeogenetics to bring new clues to debates on historical and biological questions. Her current work focuses on using bioinformatics tools to analyse the DNA (and hopefully one day RNA) of ancient viruses, as well as other microbes to get a better picture of the common pathogens at different time periods in history and to help understand past epidemics. Some of her former research projects have included the demographic analysis of combatants from a Bronze Age battlefield and the estimate of the selection coefficient of the lactase persistence, an allelic mutation already present in some of them.\n\n\n5.2.3 Nora Bergfeldt\nNora started her PhD in Zoology at the Centre for Palaeogenetics in 2019. She has a background in ecology, biodiversity and genomics. Her research focuses on using metagenomic tools to study bacteria and other microbes in ancient humans, with a particular interest in studying the evolution of pathogens."
  },
  {
    "objectID": "cluster-connection.html#how-will-we-work",
    "href": "cluster-connection.html#how-will-we-work",
    "title": "6  How to connect to servers?",
    "section": "6.1 How will we work?",
    "text": "6.1 How will we work?\nIn this workshop, we will use our computers, and we will connect to high performance computing (HPC) systems. Therefore, you will need to bring your laptops with you.\nYou should be comfortable using the Unix/Linux command line to take this course. In this page, you will find recommendations on how to connect to high performance computers depending on your laptop’s operating system.\nN.B. During this course, we will use the terms server, cluster and high performance computers interchangeably.\n\n6.1.1 Linux systems\nIf you have a Linux system (e.g. Ubuntu), you’re all set! Most of the command line tools (like the necessary ssh) will be already installed on your system.\n\n\n6.1.2 Mac systems\nMac systems also have a default support on the command line. The terminal application on your Mac system is enough. But you can also install iTerm2 as a terminal replacement for your Mac, if you’d like something fancier.\n\n\n6.1.3 Windows systems\nIf you have a Windows system, the easiest choice to connect to a remote server is Mobaxterm."
  },
  {
    "objectID": "cluster-connection.html#high-performance-computers",
    "href": "cluster-connection.html#high-performance-computers",
    "title": "6  How to connect to servers?",
    "section": "6.2 High performance computers",
    "text": "6.2 High performance computers\nIn this workshop, we have two different cluster systems at our disposal. The first system TRUBA (Turkish National Scientific Cluster) is funded by the TUBİTAK (The Scientific and Technological Research Council of Türkiye). The second system is located at Middle East Technical University.\n\n6.2.1 TRUBA Cluster system\nThis cluster system accepts connections from a Turkish university network only. Therefore, you should be in a university network in order to access TRUBA computers. We will provide a username and a password for your connection.\nYou can connect to TRUBA by writing this command in your terminal:\nssh your_username@levrek1.ulakbim.gov.tr\nN.B. There is a trick to connect to the TRUBA system outside of the university network described at the end of this document.\n\n\n6.2.2 Middle East Technical University computing systems\nThis computer system is located at Middle East Technical University. It consists of two subclusters: baggins and NEOGENE.\nSince the NEOGENE cluster does not accept connections from outside the university networks, we will first connect to baggins via ssh. Then we can connect to NEOGENE from within this system.\nYour username and password will be provided on the first day of the workshop.\nConnect to the baggins server with your username like this:\nssh -p 2275 your_username@144.122.38.49\nAfterwards, you can connect to NEOGENE from withing baggins this way:\nssh your_username@144.122.38.50\n\n\n6.2.3 Connecting to the TRUBA system from outside the university network\nWe can connect to the TRUBA system from outside the university network with a small trick. First, we connect to the baggins cluster as described above, so to be using the Middle East Technical University network. Then we can connect to TRUBA from within that terminal.\nssh -p 2275 your_username@144.122.38.49\n# Enter your password when prompted\n\nssh your_username@levrek1.ulakbim.gov.tr"
  },
  {
    "objectID": "cluster-connection.html#configuration",
    "href": "cluster-connection.html#configuration",
    "title": "6  How to connect to servers?",
    "section": "6.3 Configuration",
    "text": "6.3 Configuration\nAll the tools are installed on these server systems. Before using the tools, we need to set the PATH variable and show the system where these tools are installed, in case we want to run commands from the terminal. The PATH variable will already be set for you in the bash scripts in TRUBA.\n\n6.3.1 Configuration of the TRUBA system\nTo use all the tools for aMeta, please set your PATH variable like this:\nexport PATH=${PATH}:/truba/home/egitim/miniconda3/envs/aMeta/bin/\n\n\n6.3.2 Configuration on the Ankara computers\nTo use all the tools for aMeta, please set your PATH variable like this:\nexport PATH=${PATH}:/usr/local/sw/anaconda3/envs/aMeta/\nThe workshop folder on the Ankara computer will be here:\ncd /mnt/NEOGENE4/projects/microbe_2023/"
  },
  {
    "objectID": "course-setup.html#course-setup",
    "href": "course-setup.html#course-setup",
    "title": "7  Course setup",
    "section": "7.1 Course setup",
    "text": "7.1 Course setup\nOnce you have connected to the TRUBA server with your own username, you can begin to setup your folder for the course.\nCreate the necessary folders:\nmkdir -p workshop/data\nmkdir -p workshop/logs/slurm\nmkdir -p workshop/scripts\nmkdir -p workshop/resources\nmkdir -p workshop/results/KRAKENUNIQ\nmkdir -p workshop/results/MALT_DB\nmkdir -p workshop/results/MALT\nCreate links to the files you will use:\n# Create links for the FastQ files\nln -s /truba/home/egitim/workshop/data/* workshop/data\n# Create links for the bash scripts \nln -s /truba/home/egitim/workshop/*.sh workshop\n# Create links for the R and python scripts \nln -s /truba/home/egitim/workshop/scripts/* workshop/scripts\n# Create links for the necessary resources files\nln -s /truba/home/egitim/workshop/resources/* workshop/resources\n# Create links to the KrakenUniq result\nln -s /truba/home/egitim/workshop/results/KRAKENUNIQ/* workshop/results/KRAKENUNIQ\n# Create links to the MALT_DB result\nln -s /truba/home/egitim/workshop/results/MALT_DB/* workshop/results/MALT_DB\n# Create links to the MALT result\nln -s /truba/home/egitim/workshop/results/MALT/* workshop/results/MALT\nGreat, you have now created your workshop folder. From now on, you will be working only from within this folder. Please enter this folder now by using this command:\ncd workshop\nYou can now follow the pipeline step by step. Good luck !"
  },
  {
    "objectID": "qc.html",
    "href": "qc.html",
    "title": "8  Quality control and trimming",
    "section": "",
    "text": "9 In summary, it loops through the samples and uses Cutadapt to remove the adapters. From now on, we will work using the trimmed FastQ files that it has created.\nPlease run the script provided for this part like this:\nAfter your job has finished, please check the output files:"
  },
  {
    "objectID": "qc.html#introduction",
    "href": "qc.html#introduction",
    "title": "8  Quality control and trimming",
    "section": "8.1 Introduction",
    "text": "8.1 Introduction\nThe pipeline starts with a quality control step. This step contains four main rules:\n\nQuality control of the raw FastQ files using the tool FastQC\nRemoving potential remaining adapters using Cutadapt\nQuality control of the newly trimmed FastQ files using FastQC again\nCombining several FastQC outputs into one file using MultiQC"
  },
  {
    "objectID": "qc.html#quality-control-before-trimming",
    "href": "qc.html#quality-control-before-trimming",
    "title": "8  Quality control and trimming",
    "section": "8.2 Quality control before trimming",
    "text": "8.2 Quality control before trimming\nIn the snakemake pipeline, the name of the FastQ files and their location is retrieved from a file called samples.tsv and that the user prepares before running the pipeline on real data. For more information on the samples.tsv and the config.yaml files that need to be created before running the pipeline on any real data, check the official aMeta GitHub.\nIn our case we are going to run bash scripts that we will submit directly to the slurm system and we have implemented a for loop going through the fastq files available in the data folder, so no need to create a samples.tsv file for this workshop.\nHere is the simplified snakemake rule (without the parameters explained in the “Understand snakemake” section):\nrule FastQC_BeforeTrimming:\n    output:\n        html=\"results/FASTQC_BEFORE_TRIMMING/{sample}_fastqc.html\",\n        zip=\"results/FASTQC_BEFORE_TRIMMING/{sample}_fastqc.zip\",\n    input:\n        fastq=lambda wildcards: samples.loc[wildcards.sample].fastq,\n    shell:\n        \"fastqc {input.fastq} --threads {threads} --nogroup --outdir results/FASTQC_BEFORE_TRIMMING &> {log}\"\nHere is a simplified version of this code:\nfor sample in $(ls data/*.fastq.gz); do\n    sample_name=$(basename $sample .fastq.gz\n    fastqc $sample --threads 4 --nogroup --outdir results/FASTQC_BEFORE_TRIMMING &> logs/FASTQC_BEFORE_TRIMMING/$sample_name.log;\ndone\nIn summary, this rule loops through the sample files and generate statistics using FastQC for each of them in the form of an html and zip file. The html file contains the final report and you can open it to see information about the quality of sequencing and the presence of adapters. Alternatively, you can navigate the FastQC website to see how an html output file looks like and how to interpret it.\nWARNING: If you would be running the shell command above directly on the terminal, you would need to export the PATH to the necessary modules first and to create the necessary output and log folders. You can see how the full command would look like by opening the bash script file FastQC_BeforeTrimming.sh using less less FastQC_BeforeTrimming.sh.\nTo run the different rules on TRUBA, we have prepared bash scripts. Before running it, verify that you have done the steps from the course setup section of this website and that you are located in your own newly created workshop folder. If this is the case, type this command:\nsbatch FastQC_BeforeTrimming.sh --account=your_username\nIt is important to specify your account name anytime you run a bash script. You will get a better priority in the queue than if every user uses the same user name, and you will be able to check the progress status of your job using squeue.\nPlease, remember to check the status of your job using this command. Once your job disappears from this command output, it means that it has terminated.\nsqueue -u your_username\nOnce your job has finished, please check that the output files were created like this:\nls -ltrh results/FASTQC_BEFORE_TRIMMING/"
  },
  {
    "objectID": "qc.html#adapter-trimming-step",
    "href": "qc.html#adapter-trimming-step",
    "title": "8  Quality control and trimming",
    "section": "8.3 Adapter trimming step",
    "text": "8.3 Adapter trimming step\nAfter we run a first quality control, the pipeline will run Cutadapt to remove the adapters and filter out the processed reads that have become too small. This step, was added because if users don’t remove correctly and completely the adapters before running the pipeline, it will increase the amount of false positives. If you have already removed the adapters, no need to worry about this step, because it won’t change much your FastQ file.\nYou can specify which adapters were used in the config/config.yaml file. If no adapter is specified, the default used are Illumina adapters.\nHere is the snakemake rule:\nrule Cutadapt_Adapter_Trimming:\n    output:\n        fastq=\"results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz\",\n    input:\n        fastq=lambda wildcards: samples.loc[wildcards.sample].fastq,\n    params:\n        adapters=\" \".join([f\"-a {x}\" for x in ADAPTERS]),\n    shell:\n        \"cutadapt {params.adapters} --minimum-length 30 -o {output.fastq} {input.fastq} &> {log}\"\nHere is a shell command for this rule:\nfor sample in $(ls data/*.fastq.gz); do\n        sample_name=$(basename $sample .fastq.gz)\n        cutadapt -a AGATCGGAAGAG --minimum-length 30 -o results/CUTADAPT_ADAPTER_TRIMMING/${sample_name}.trimmed.fastq.gz ${sample} &> logs/CUTADAPT_ADAPTER_TRIMMING/${sample_name}.log\ndone"
  },
  {
    "objectID": "qc.html#quality-control-after-trimming",
    "href": "qc.html#quality-control-after-trimming",
    "title": "8  Quality control and trimming",
    "section": "9.1 Quality control after trimming",
    "text": "9.1 Quality control after trimming\nNow that we have removed potentially remaining adapters, we will run a new quality control analysis. Normally, there should be no more adapters visible in the resulting html file:\nrule FastQC_AfterTrimming:\n    output:\n        html=\"results/FASTQC_AFTER_TRIMMING/{sample}.trimmed_fastqc.html\",\n        zip=\"results/FASTQC_AFTER_TRIMMING/{sample}.trimmed_fastqc.zip\",\n    input:\n        fastq=\"results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz\",\n    shell:\n        \"fastqc {input.fastq} --threads {threads} --nogroup --outdir results/FASTQC_AFTER_TRIMMING &> {log}\"\nHere is a shell command for this rule:\nfor sample in $(ls results/CUTADAPT_ADAPTER_TRIMMING/*.fastq.gz); do\n        sample_name=$(basename ${sample} .fastq.gz)\n        fastqc ${sample} --threads 4 --nogroup --outdir results/FASTQC_AFTER_TRIMMING &> logs/FASTQC_AFTER_TRIMMING/${sample_name}.log;\ndone\nPlease run the sbatch command for this part like this:\nsbatch FastQC_AfterTrimming.sh --account=your_username\nPlease check the outputs using this command:\nls -ltrh results/FASTQC_AFTER_TRIMMING/"
  },
  {
    "objectID": "qc.html#combine-quality-control-outputs",
    "href": "qc.html#combine-quality-control-outputs",
    "title": "8  Quality control and trimming",
    "section": "9.2 Combine quality control outputs",
    "text": "9.2 Combine quality control outputs\nAt last, the pipeline combines all the FastQC reports into one file using the tool MultiQC. This part will not be executed in this tutorial.\nrule MultiQC:\n    output:\n        html=\"results/MULTIQC/multiqc_report.html\",\n    input:\n        unpack(multiqc_input),\n    params:\n        config=os.path.join(WORKFLOW_DIR, \"envs\", \"multiqc_config.yaml\"),\n    shell:\n        'echo {input} | tr \" \" \"\\n\" > {output.html}.fof;'\n        \"multiqc -c {params.config} -l {output.html}.fof --verbose --force --outdir results/MULTIQC &> {log}\""
  },
  {
    "objectID": "krakenuniq.html#krakenuniq-prescreening-step",
    "href": "krakenuniq.html#krakenuniq-prescreening-step",
    "title": "9  Prescreening with KrakenUniq",
    "section": "9.1 KrakenUniq prescreening step",
    "text": "9.1 KrakenUniq prescreening step\nThe first part of the pipeline includes a prescreening step with KrakenUniq. It is not an aligner but a k-mer based classifier. It means that it has a database with information about unique regions of the DNA of each organism and classifies the reads by comparing them to this database. It can assign the reads to different taxonomic clade depending on their uniqueness. If a read is specific to a species, it will be assigned to that species, if it is specific to a genus, it will be assigned to that genus and so on.\nKrakenUniq is fast and makes it possible to screen aDNA samples against a database as large as possible. With an aligner, this is rarely possible because it would take too much compute power and time to align to a very large database. The KrakenUniq paper suggests that KrakenUniq is not less accurate compared to alignment tools such as BLAST and MEGAN. However, we still want to have a secondary verification with an aligner later on in the pipeline, since we need to align the reads in order to verify the quality of the alignment and generate statistics."
  },
  {
    "objectID": "krakenuniq.html#krakenuniq-database",
    "href": "krakenuniq.html#krakenuniq-database",
    "title": "9  Prescreening with KrakenUniq",
    "section": "9.2 KrakenUniq database",
    "text": "9.2 KrakenUniq database\nWe have made two KrakenUniq databases available through the SciLifeLab repository. Description of the databases and information on how to download them can be found on the official aMeta GitHub."
  },
  {
    "objectID": "krakenuniq.html#classification-with-krakenuniq",
    "href": "krakenuniq.html#classification-with-krakenuniq",
    "title": "9  Prescreening with KrakenUniq",
    "section": "9.3 Classification with KrakenUniq",
    "text": "9.3 Classification with KrakenUniq\nPlease note that it will not be possible to run this first step during this workshop due to issues with storage space. If you followed the course setup correctly, you should have the expected output files of this rule in your results folder.\nrule KrakenUniq:\n    output:\n        report=\"results/KRAKENUNIQ/{sample}/krakenuniq.output\",\n        seqs=\"results/KRAKENUNIQ/{sample}/sequences.krakenuniq\",\n    input:\n        fastq=\"results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz\",\n    params:\n        DB=config[\"krakenuniq_db\"],\n    log:\n        \"logs/KRAKENUNIQ/{sample}.log\",\n    shell:\n        \"krakenuniq --preload --db {params.DB} --fastq-input {input.fastq} --threads {threads} --output {output.seqs} --report-file {output.report} --gzip-compressed --only-classified-out &> {log}\"\nHere is a shell version of this code:\nfor sample in $(ls results/CUTADAPT_ADAPTER_TRIMMING/*.fastq.gz); do\n       sample_name=$(basename $sample .fastq.gz)\n       krakenuniq --preload --db $DBNAME --fastq-input ${sample_name} --threads 4 --output ${sample_name}.sequences.krakenuniq --report-file ${sample_name}.krakenuniq.output --gzip-compressed --only-classified-out &> logs/KRAKENUNIQ/${sample_name}.log\ndone\nIn summary, this command loops through the sample files created by Cutadapt and classifies the reads using KrakenUniq."
  },
  {
    "objectID": "krakenuniq.html#filtering-the-krakenuniq-output",
    "href": "krakenuniq.html#filtering-the-krakenuniq-output",
    "title": "9  Prescreening with KrakenUniq",
    "section": "9.4 Filtering the KrakenUniq output",
    "text": "9.4 Filtering the KrakenUniq output\nAfter running KrakenUniq, we need to filter its output to remove as many false positives as possible. In order to do so, we select the species taxonomic level for each organism and we filter the results according to the amount of kmers and the amount of taxReads specific to that species. A suggested value for this parameters is 1000 unique kmers to make sure that at least 1000 unique regions of the organim are covered and that 200 taxReads in order to have enough reads to verify if the organism is ancient after alignment.\nrule Filter_KrakenUniq_Output:\n    output:\n        filtered=\"results/KRAKENUNIQ/{sample}/krakenuniq.output.filtered\",\n        pathogens=\"results/KRAKENUNIQ/{sample}/krakenuniq.output.pathogens\",\n        pathogen_tax_id=\"results/KRAKENUNIQ/{sample}`taxID.pathogens\",\n    input:\n        krakenuniq=\"results/KRAKENUNIQ/{sample}/krakenuniq.output\",\n        pathogenomesFound=config[\"pathogenomesFound\"],\n    log:\n        \"logs/FILTER_KRAKENUNIQ_OUTPUT/{sample}.log\",\n    params:\n        exe=WORKFLOW_DIR / \"scripts/filter_krakenuniq.py\",\n        n_unique_kmers=config[\"n_unique_kmers\"],\n        n_tax_reads=config[\"n_tax_reads\"],\n    shell:\n        \"\"\"{params.exe} {input.krakenuniq} {params.n_unique_kmers} {params.n_tax_reads} {input.pathogenomesFound} &> {log}; \"\"\"\n        \"\"\"cut -f7 {output.pathogens} | tail -n +2 > {output.pathogen_tax_id}\"\"\"\n        \nHere is a shell version of this code:\n# Suggested parameters for the number of unique kmers and the number of taxReads\nn_unique_kmers=1000\nn_tax_reads=200\n\n# Create the log folder\nmkdir -p logs/FILTER_KRAKENUNIQ_OUTPUT\n\n# Loop through the samples and filter the KrakenUniq output according to three thresholds. It should have at least 1000 unique kmers and 200 reads and it should be at the Species level (not Genus, not Family, subspecies or else). This part is implemented in the python script.\n# The second command extracts the column containing the taxID information for the species that match a pathogen in the pathogenFound.very_inclusive.tab.\nfor sample in $(ls results/CUTADAPT_ADAPTER_TRIMMING/*.fastq.gz); do\n        sample_name=$(basename $sample .trimmed.fastq.gz)\n        python scripts/filter_krakenuniq.py results/KRAKENUNIQ/${sample_name}/krakenuniq.output ${n_unique_kmers} ${n_tax_reads} resources/pathogensFound.very_inclusive.tab &> logs/FILTER_KRAKENUNIQ_OUTPUT/${sample_name}.log;\n        cut -f7 results/KRAKENUNIQ/${sample_name}/krakenuniq.output.pathogens | tail -n +2 > results/KRAKENUNIQ/${sample_name}/taxID.pathogens\ndone\nIn summary, this rule uses a python script to filter the output from KrakenUniq according to a specified minimum amount of unique kmers and minimum amount of taxReads (reads specific to the taxonomic clade of this species) and extract the information for the species that meet these criteria.\nPlease run this code and do not forget to change your account name:\nsbatch KrakenUniq_Filter.sh --account=your_user_account\nAnd let’s check the output of one sample, sample1:\nls -ltrh results/KRAKENUNIQ/sample1\nLet’s check the output file:\nless results/KRAKENUNIQ/sample1/krakenuniq.output.filtered\n\n9.4.1 Creating an abundance matrix from the KrakenUniq outputs\nAt last, we will combine the filtered outputs and create an abundance matrix. In this part, the inputs are the filtered krakenuniq outputs of all the samples.\nrule KrakenUniq_AbundanceMatrix:\n    output:\n        out_dir=directory(\"results/KRAKENUNIQ_ABUNDANCE_MATRIX\"),\n        unique_species=\"results/KRAKENUNIQ_ABUNDANCE_MATRIX/unique_species_taxid_list.txt\",\n        unique_species_names=\"results/KRAKENUNIQ_ABUNDANCE_MATRIX/unique_species_names_list.txt\",\n        abundance_matrix=\"results/KRAKENUNIQ_ABUNDANCE_MATRIX/krakenuniq_abundance_matrix.txt\",\n        abundance_plot=\"results/KRAKENUNIQ_ABUNDANCE_MATRIX/krakenuniq_absolute_abundance_heatmap.pdf\",\n    input:\n        expand(\"results/KRAKENUNIQ/{sample}/krakenuniq.output.filtered\", sample=SAMPLES),\n    log:\n        \"logs/KRAKENUNIQ_ABUNDANCE_MATRIX/KRAKENUNIQ_ABUNDANCE_MATRIX.log\",\n    params:\n        exe=WORKFLOW_DIR / \"scripts/krakenuniq_abundance_matrix.R\",\n        exe_plot=WORKFLOW_DIR / \"scripts/plot_krakenuniq_abundance_matrix.R\",\n        n_unique_kmers=config[\"n_unique_kmers\"],\n        n_tax_reads=config[\"n_tax_reads\"],\n    message:\n        \"KrakenUniq_AbundanceMatrix: COMPUTING KRAKENUNIQ MICROBIAL ABUNDANCE MATRIX\"\n    shell:\n        \"Rscript {params.exe} results/KRAKENUNIQ {output.out_dir} {params.n_unique_kmers} {params.n_tax_reads} &> {log};\"\n        \"Rscript {params.exe_plot} {output.out_dir} {output.out_dir} &> {log}\"\nThis rule combines krakenuniq abundance output files into one file results/KRAKENUNIQ_ABUNDANCE_MATRIX/krakenuniq_abundance_matrix.txt.\nThen, it creates a heatmap plot using the abundance data: results/KRAKENUNIQ_ABUNDANCE_MATRIX/krakenuniq_absolute_abundance_heatmap.pdf\nPlease run this script using this piece of code:\nsbatch KrakenUniq_AbundanceMatrix.sh --account=your_account_name\nThen let’s check the output:\nls results/KRAKENUNIQ_ABUNDANCE_MATRIX/"
  },
  {
    "objectID": "malt.html#build-a-custom-malt-database-based-on-the-previous-results-from-the-filtering-of-krakenuniq",
    "href": "malt.html#build-a-custom-malt-database-based-on-the-previous-results-from-the-filtering-of-krakenuniq",
    "title": "10  MALT alignment and processing",
    "section": "10.1 Build a custom MALT database based on the previous results from the filtering of KrakenUniq",
    "text": "10.1 Build a custom MALT database based on the previous results from the filtering of KrakenUniq\nFirst we need to build the custom database with the rule Build_Malt_DB. This is done with the use of a python script. Inside the script, python will use the input and output variables provided by snakemake.\nWARNING: We won’t be able to run this command during the workshop because the files seqid2taxid.map.orig, library.fna and nucl_gb.accession2taxid were too large for our workshop folder.\nrule Build_Malt_DB:\n    output:\n        seqid2taxid_project=\"results/MALT_DB/seqid2taxid.project.map\",\n        seqids_project=\"results/MALT_DB/seqids.project\",\n        project_headers=\"results/MALT_DB/project.headers\",\n        project_fasta=\"results/MALT_DB/library.project.fna\",\n        db=directory(\"results/MALT_DB/maltDB.dat\"),\n    input:\n        unique_taxids=ancient(\"results/KRAKENUNIQ_ABUNDANCE_MATRIX/unique_species_taxid_list.txt\"),\n    params:\n        seqid2taxid=config[\"malt_seqid2taxid_db\"],\n        nt_fasta=config[\"malt_nt_fasta\"],\n        accession2taxid=config[\"malt_accession2taxid\"],\n    threads: 20\n    log:\n        \"logs/BUILD_MALT_DB/BUILD_MALT_DB.log\",\n    conda:\n        \"../envs/malt.yaml\"\n    envmodules:\n        *config[\"envmodules\"][\"malt\"],\n    benchmark:\n        \"benchmarks/BUILD_MALT_DB/BUILD_MALT_DB.benchmark.txt\"\n    message:\n        \"Build_Malt_DB: BUILDING MALT DATABASE USING SPECIES DETECTED BY KRAKENUNIQ\"\n    script:\n        \"../scripts/malt-build.py\"\nHere is a simplified version of this code:\npython ../scripts/malt-build.py \\\n    --seqid2taxid seqid2taxid.map.orig \\\n    --nt_fasta library.fna \\\n    --accession2taxid nucl_gb.accession2taxid \\\n    --unique_taxids results/KRAKENUNIQ_ABUNDANCE_MATRIX/unique_species_taxid_list.txt \\\n    --output_dir results/MALT_DB/ \\\n    --threads 16 &> logs/BUILD_MALT_DB/BUILD_MALT_DB.log\nIn summary, this command will build a custom malt database containing all the species detected by KrakenUniq that have passed are threshold. You can find the full list of species in results/KRAKENUNIQ_ABUNDANCE_MATRIX/unique_species_taxid_list.txt."
  },
  {
    "objectID": "malt.html#align-the-fastq-files-using-malt",
    "href": "malt.html#align-the-fastq-files-using-malt",
    "title": "10  MALT alignment and processing",
    "section": "10.2 Align the FastQ files using MALT",
    "text": "10.2 Align the FastQ files using MALT\nrule Malt:\n    output:\n        rma6=\"results/MALT/{sample}.trimmed.rma6\",\n        sam=\"results/MALT/{sample}.trimmed.sam.gz\",\n    input:\n        fastq=ancient(\"results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz\"),\n        db=ancient(\"results/MALT_DB/maltDB.dat\"),\n    params:\n        gunzipped_sam=\"results/MALT/{sample}.trimmed.sam\",\n    threads: 20\n    log:\n        \"logs/MALT/{sample}.log\",\n    conda:\n        \"../envs/malt.yaml\"\n    envmodules:\n        *config[\"envmodules\"][\"malt\"],\n    benchmark:\n        \"benchmarks/MALT/{sample}.benchmark.txt\"\n    message:\n        \"Malt: RUNNING MALT ALIGNMENTS FOR SAMPLE {input.fastq}\"\n    shell:\n        \"unset DISPLAY; malt-run -at SemiGlobal -m BlastN -i {input.fastq} -o {output.rma6} -a {params.gunzipped_sam} -t {threads} -d {input.db} &> {log}\"\nHere is a simplified version of this code:\nfor sample in $(ls results/CUTADAPT_ADAPTER_TRIMMING/*.fastq.gz); do\n        unset DISPLAY\n        malt-run -at SemiGlobal -m BlastN -i results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz \\\n                -o results/MALT/{sample}.trimmed.rma6 \\\n                -a results/MALT/{sample}.trimmed.sam \\\n                -t 20 -d results/MALT_DB/maltDB.dat \\\n                &> logs/MALT/{sample}.log\n        gzip results/MALT/{sample}.trimmed.sam\ndone\nIn summary, we loop through the FastQ files and run an alignment with MALT and gzip the generated sam file. The “unset DISPLAY” command makes it possible to run MALT without it trying to open a graphical interface which can generate an error."
  },
  {
    "objectID": "malt.html#quantify-the-microbial-abundance-using-the-sam-file-generated-by-the-malt-alignment",
    "href": "malt.html#quantify-the-microbial-abundance-using-the-sam-file-generated-by-the-malt-alignment",
    "title": "10  MALT alignment and processing",
    "section": "10.3 Quantify the microbial abundance using the sam file generated by the MALT alignment",
    "text": "10.3 Quantify the microbial abundance using the sam file generated by the MALT alignment\nrule Malt_QuantifyAbundance:\n    output:\n        out_dir=directory(\"results/MALT_QUANTIFY_ABUNDANCE/{sample}\"),\n        counts=\"results/MALT_QUANTIFY_ABUNDANCE/{sample}/sam_counts.txt\",\n    input:\n        sam=ancient(\"results/MALT/{sample}.trimmed.sam.gz\"),\n    params:\n        unique_taxids=\"results/KRAKENUNIQ_ABUNDANCE_MATRIX/unique_species_taxid_list.txt\",\n        exe=WORKFLOW_DIR / \"scripts/malt_quantify_abundance.py\",\n    log:\n        \"logs/MALT_QUANTIFY_ABUNDANCE/{sample}.log\",\n    benchmark:\n        \"benchmarks/MALT_QUANTIFY_ABUNDANCE/{sample}.benchmark.txt\"\n    message:\n        \"Malt_QuantifyAbundance: QUANTIFYING MICROBIAL ABUNDANCE USING MALT SAM-ALIGNMENTS FOR SAMPLE {input.sam}\"\n    shell:\n        \"mkdir -p {output.out_dir}; \"\n        \"{params.exe} {input.sam} {params.unique_taxids} > {output.counts} 2> {log}\"\nHere is a simplified version of this code:\nmkdir -p results/MALT_QUANTIFY_ABUNDANCE logs/MALT_QUANTIFY_ABUNDANCE\n\nfor sample in $(ls results/CUTADAPT_ADAPTER_TRIMMING/*.fastq.gz); do\n        python scripts/malt_quantify_abundance.py results/MALT/{sample}.trimmed.sam.gz results/KRAKENUNIQ_ABUNDANCE_MATRIX/unique_species_taxid_list.txt > results/MALT_QUANTIFY_ABUNDANCE/{sample}/sam_counts.txt 2> logs/MALT_QUANTIFY_ABUNDANCE/{sample}.log\ndone\nIn summary, this python script takes the results of the MALT alignment and counts the amount of reads per species."
  },
  {
    "objectID": "malt.html#compute-the-malt-abundance-matrix-using-the-sam-files-generated-by-malt",
    "href": "malt.html#compute-the-malt-abundance-matrix-using-the-sam-files-generated-by-malt",
    "title": "10  MALT alignment and processing",
    "section": "10.4 Compute the MALT abundance matrix using the SAM files generated by MALT",
    "text": "10.4 Compute the MALT abundance matrix using the SAM files generated by MALT\nrule Malt_AbundanceMatrix_Sam:\n    output:\n        out_dir=directory(\"results/MALT_ABUNDANCE_MATRIX_SAM\"),\n        abundance_matrix=\"results/MALT_ABUNDANCE_MATRIX_SAM/malt_abundance_matrix_sam.txt\",\n    input:\n        sam_counts=expand(\n            \"results/MALT_QUANTIFY_ABUNDANCE/{sample}/sam_counts.txt\", sample=SAMPLES\n        ),\n    log:\n        \"logs/MALT_ABUNDANCE_MATRIX_SAM/MALT_ABUNDANCE_MATRIX_SAM.log\",\n    params:\n        exe=WORKFLOW_DIR / \"scripts/malt_abundance_matrix.R\",\n    conda:\n        \"../envs/r.yaml\"\n    envmodules:\n        *config[\"envmodules\"][\"r\"],\n    benchmark:\n        \"benchmarks/MALT_ABUNDANCE_MATRIX_SAM/MALT_ABUNDANCE_MATRIX_SAM.benchmark.txt\"\n    message:\n        \"Malt_AbundanceMatrix_Sam: COMPUTING MALT MICROBIAL ABUNDANCE MATRIX FROM SAM-FILES\"\n    shell:\n        \"Rscript {params.exe} results/MALT_QUANTIFY_ABUNDANCE {output.out_dir} &> {log}\"\nHere is a simplified version of this code:\nmkdir -p results/MALT_ABUNDANCE_MATRIX_SAM logs/MALT_ABUNDANCE_MATRIX_SAM\n\nfor sample in $(ls results/CUTADAPT_ADAPTER_TRIMMING/*.fastq.gz); do\n        Rscript scripts/malt_abundance_matrix.R results/MALT_QUANTIFY_ABUNDANCE results/MALT_ABUNDANCE_MATRIX_SAM/ > logs/MALT_ABUNDANCE_MATRIX_SAM/MALT_ABUNDANCE_MATRIX_SAM.log 2>&1\ndone\nIn summary, this R scripts takes the abundance of all samples and creates a matrix."
  },
  {
    "objectID": "malt.html#compute-the-malt-abundance-matrix-using-the-rma6-files-generated-by-malt",
    "href": "malt.html#compute-the-malt-abundance-matrix-using-the-rma6-files-generated-by-malt",
    "title": "10  MALT alignment and processing",
    "section": "10.5 Compute the MALT abundance matrix using the rma6 files generated by MALT",
    "text": "10.5 Compute the MALT abundance matrix using the rma6 files generated by MALT\nrule Malt_AbundanceMatrix_Rma6:\n    output:\n        out_dir=directory(\"results/MALT_ABUNDANCE_MATRIX_RMA6\"),\n        abundance_matrix=\"results/MALT_ABUNDANCE_MATRIX_RMA6/malt_abundance_matrix_rma6.txt\",\n    input:\n        rma6=expand(\"results/MALT/{sample}.trimmed.rma6\", sample=SAMPLES),\n    params:\n        exe=WORKFLOW_DIR / \"scripts/rma-tabuliser\",\n    log:\n        \"logs/MALT_ABUNDANCE_MATRIX_RMA6/MALT_ABUNDANCE_MATRIX_RMA6.log\",\n    envmodules:\n        *config[\"envmodules\"][\"malt\"],\n    conda:\n        \"../envs/malt.yaml\"\n    benchmark:\n        \"benchmarks/MALT_ABUNDANCE_MATRIX_RMA6/MALT_ABUNDANCE_MATRIX_RMA6.benchmark.txt\"\n    message:\n        \"Malt_AbundanceMatrix_Rma6: COMPUTING MALT MICROBIAL ABUNDANCE MATRIX FROM RMA6-FILES\"\n    shell:\n        \"{params.exe} -d $(dirname {input.rma6}) -r 'S' &> {log}; \"\n        \"mv results/MALT/count_table.tsv {output.out_dir}; \"\n        \"mv {output.out_dir}/count_table.tsv {output.abundance_matrix}\"\nHere is a simplified version of this code:\n# Create the output folder\nmkdir -p results/MALT_ABUNDANCE_MATRIX_RMA6 logs MALT_ABUNDANCE_MATRIX_RMA6\n\nfor sample in $(ls results/CUTADAPT_ADAPTER_TRIMMING/*.fastq.gz); do\n    sample_name=$(basename $sample .trimmed.fastq.gz)\n    ./scripts/rma-tabuliser -d results/MALT/${sample_name}.trimmed.rma6 -r 'S' &> logs/MALT_ABUNDANCE_MATRIX_RMA6/MALT_ABUNDANCE_MATRIX_RMA6.log;\n        mv results/MALT/count_table.tsv results/MALT_ABUNDANCE_MATRIX_RMA6/;\n        mv results/MALT_ABUNDANCE_MATRIX_RMA6/count_table.tsv results/MALT_ABUNDANCE_MATRIX_RMA6/malt_abundance_matrix_rma6.txt\ndone\nIn summary, we use the tool rma-tabuliser to process the rma6 file generated by the MALT alignment. This file contains LCA (latest common ancestor) information for the reads. This command will generate a taxonomy table, with node names and filtered at the species (S) level inside the MALT folder called count_table.tsv. We then move that table to the designated output folder of our rule. We also rename the file to better describe what it contains."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "aMeta Workshop",
    "section": "",
    "text": "For a sample, create taxid for each entry in krakenuniq output taxID.pathogens. Downstream rules use the taxid directories as input, but it is not known beforehand which these are; they are determined by the finds in krakenuniq.\nrule Authentication:\n    \"\"\"Run Authentication\"\"\"\n    output:\n        done=\"results/AUTHENTICATION/{sample}/.extract_taxids_done\",\n    input:\n        pathogens=\"results/KRAKENUNIQ/{sample}/taxID.pathogens\",\n    shell:\n        \"mkdir -p {params.dir}; \"\n        \"while read taxid; do mkdir -p {params.dir}/$taxid; touch {params.dir}/$taxid/.done; done<{input.pathogens};\"\n        \"touch {output.done}\"\nHere is a simplified version of this code:\n\"mkdir -p {params.dir}; \"\n        \"while read taxid; do mkdir -p {params.dir}/$taxid; touch {params.dir}/$taxid/.done; done<{input.pathogens};\"\n        \"touch {output.done}\"\n``` for BASE in \\({SAMPLES} do  sbatch --time=12:00:00 --job-name=auth_\\){BASE} –ntasks-per-node=${THREADS} -A \\({PROJECT} --mail-user=\\){MAIL} –mail-type=${NOTIFICATION} ${MALTOUTPUT}/to_run_authenticate.sh ${BASE} \\({MALTOUTPUT}/\\){BASE} ${FILE} done"
  },
  {
    "objectID": "align.html#bowtie2",
    "href": "align.html#bowtie2",
    "title": "12  Fast alignment with Bowtie2",
    "section": "12.1 Bowtie2",
    "text": "12.1 Bowtie2\nThe pipeline has a side branch for rapid analysis of the results based on an alignment with Bowtie2 and a post-mortem damage estimation with MapDamage. For this project, we won’t be able to use the real Bowtie2 databases, because there were too heavy for the resources allocated for this workshop. So we are only going to look at the code but not run it and the output will be provided for you in the folder containing the expected results."
  },
  {
    "objectID": "align.html#bowtie2-build-l-to-build-the-index-files",
    "href": "align.html#bowtie2-build-l-to-build-the-index-files",
    "title": "12  Fast alignment with Bowtie2",
    "section": "12.2 Bowtie2-build-l to build the index files",
    "text": "12.2 Bowtie2-build-l to build the index files\nIn order to run a Bowtie2 alignment, one needs a complete Bowtie2 database, in other words a .fna (fasta) file that has been indexed using the command bowtie2-build-l. This is the first part of the pipeline for the alignment step. You can therefore provide your own merged fna file for Bowtie2 to index if you wish to. However, if you are using one of our own custom databases, this step will be automatically ignored because its output already exists.\nYou can find the Bowtie2_build rule generating the index files below. It takes as input the path to the library.fna file that you have provided for the variable bowtie2_patho_db in your config.yaml file. For more information, refer to the main aMeta GitHub in the section about the config file. The rule then generates the index files of the Bowtie2 database if they don’t already exist.\nrule Bowtie2_Index:\n    output:\n        expand(\n            f\"{config['bowtie2_patho_db']}{{ext}}\",\n            ext=[\n                \".1.bt2l\",\n                \".2.bt2l\",\n                \".3.bt2l\",\n                \".4.bt2l\",\n                \".rev.1.bt2l\",\n                \".rev.2.bt2l\",\n            ],\n        ),\n    input:\n        ref=ancient(config[\"bowtie2_patho_db\"]),\n    log:\n        f\"{config['bowtie2_patho_db']}_BOWTIE2_BUILD.log\",\n    shell:\n        \"\"bowtie2-build-l --threads {threads} {input.ref} {input.ref} > {log} 2>&1\nHere is a simplified version of this code:\nbowtie2-build-l --threads 1 resources/library.fna resources/library.fna > logs/Bowtie2_Build.log 2>&1\nWARNING: No need to execute that line of code as we haven’t downloaded the real databases to the server for this workshop and it would overwrite the index files! The trick for this rule is that the input file is declared as ancient with the “ancient” function of snakemake, making snakemake not rerun the rule if the output files already exists. But if you only run the bash command provided, you will overwrite the files.\n\n12.2.1 Regarding real life projects with the actual databases\nFor real life projects, these are the Bowtie2 databases we have made available for download:\n\nPathogenome: Bowtie2 index and helping files for following up on microbial pathogens\nBowtie2_Full_NT: Bowtie2 index for full NCBI NT (for quick follow up of prokaryotes and eukaryotes; also contains helping files for building the Malt database)\n\nFor more information and links to download the databases, please refer to the official GitHub of aMeta.\nWARNING: if you are using the Bowtie2_Full_NT database, make sure that you have unzipped the files before running the pipeline, otherwise it will automatically overwrite the index files to regenerate them and will need a huge amount of memory and time to succeed, which you probably don’t want to happen ;-)"
  },
  {
    "objectID": "align.html#bowtie2-alignment",
    "href": "align.html#bowtie2-alignment",
    "title": "12  Fast alignment with Bowtie2",
    "section": "12.3 Bowtie2 alignment",
    "text": "12.3 Bowtie2 alignment\nrule Bowtie2_Pathogenome_Alignment:\n    output:\n        bam=\"results/BOWTIE2/{sample}/AlignedToPathogenome.bam\",\n        bai=\"results/BOWTIE2/{sample}/AlignedToPathogenome.bam.bai\",\n    input:\n        fastq=\"results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz\",\n        db=rules.Bowtie2_Index.output,\n    params:\n        PATHO_DB=lambda wildcards, input: config[\"bowtie2_patho_db\"],\n    log:\n        \"logs/BOWTIE2/{sample}.log\",\n    shell:\n        \"\"\"bowtie2 --large-index -x {params.PATHO_DB} --end-to-end --threads 10 --very-sensitive -U {input.fastq} 2> {log} | samtools view -bS -q 1 -h -@ 10 - | samtools sort -@ 10 -o {output.bam} >> {log};\"\"\"\n        \"\"\"samtools index {output.bam}\"\"\"\nHere is a simplified version of this code:\nbowtie2 --large-index -x resources/library.fna --end-to-end --threads 10 --very-sensitive -U results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz 2> logs/BOWTIE2/{sample}.log | samtools view -bS -q 1 -h -@ 10 - | samtools sort - -@ 10 -o results/BOWTIE2/{sample}/AlignedToPathogenome.bam >> logs/BOWTIE2/{sample}.log\nsamtools index results/BOWTIE2/{sample}/AlignedToPathogenome.bam\nAgain, no need to run that line of code. You can find how the output is supposed to look like in the folder with expected results. Basically, this line asks Bowtie2 to align each fastq file to the Bowtie2 database. This will generate a sam file containing the information of the alignment for each DNA sequence and the reference genome to which it aligns to. The sam file is then directly changed into a bam file using samtools, sorted and later indexed so that it is ready to be used. In order to run this line, you would still need to replace {sample} with the name of each sample that you have provided in the samples.tsv file and run this line for each of them."
  },
  {
    "objectID": "damage.html",
    "href": "damage.html",
    "title": "aMeta Workshop",
    "section": "",
    "text": "We run MapDamage:\nrule MapDamage:\n    \"\"\"Run Mapdamage on extracted pathogens\"\"\"\n    output:\n        dir=directory(\"results/MAPDAMAGE/{sample}\"),\n    input:\n        pathogen_tax_id=\"results/KRAKENUNIQ/{sample}/taxID.pathogens\",\n        bam=\"results/BOWTIE2/{sample}/AlignedToPathogenome.bam\",\n        bai=\"results/BOWTIE2/{sample}/AlignedToPathogenome.bam.bai\",\n    params:\n        pathogenome_path=os.path.dirname(config[\"pathogenomesFound\"]),\n        PATHO_DB=config[\"bowtie2_patho_db\"],\n        options=config[\"options\"].get(\"MapDamage\", \"\"),\n    threads: 10\n    message:\n        \"MapDamage: RUNNING MAPDAMAGE ON PATHOGENS IDENTIFIED IN SAMPLE {input.bam}\"\n    shell:\n        \"mkdir {output.dir}; \"\n        \"if [ -s {input.pathogen_tax_id} ]; then \"\n        'cat {input.pathogen_tax_id} | parallel -j {threads} \"grep -w {{}} {params.pathogenome_path}/seqid2taxid.pathogen.map | cut -f1 > {output.dir}/{{}}.seq_ids\" ; '\n        \"for i in $(cat {input.pathogen_tax_id}); do xargs --arg-file={output.dir}/${{i}}.seq_ids samtools view -bh {input.bam} --write-index -@ {threads} -o {output.dir}/${{i}}.tax.bam; done >> {log} 2>&1; \"\n        \"find {output.dir} -name '*.tax.bam' | parallel -j {threads} \\\"mapDamage {params.options} -i {{}} -r {params.PATHO_DB} --merge-reference-sequences -d {output.dir}/mapDamage_{{}}\\\" >> {log} 2>&1 || true; \"\n        \"for filename in {output.dir}/*.tax.bam; do newname=`echo $filename | sed 's/tax\\.//g'`; mv $filename $newname; done >> {log} 2>&1; \"\n        \"mv {output.dir}/mapDamage_{output.dir}/* {output.dir} >> {log} 2>&1; \"\n        \"rm -r {output.dir}/mapDamage_results >> {log} 2>&1; \"\n        \"else echo NO MICROBES TO AUTHENTICATE > {output.dir}/README.txt; fi\"\nHere is a simplified version of the code:\ncat {input.pathogen_tax_id} | parallel -j {threads} \"grep -w {{}} {params.pathogenome_path}/seqid2taxid.pathogen.map | cut -f1 > {output.dir}/{{}}.seq_ids\" ; '\n        \"for i in $(cat {input.pathogen_tax_id}); do xargs --arg-file={output.dir}/${{i}}.seq_ids samtools view -bh {input.bam} --write-index -@ {threads} -o {output.dir}/${{i}}.tax.bam; done >> {log} 2>&1; \"\n        \"find {output.dir} -name '*.tax.bam' | parallel -j {threads} \\\"mapDamage {params.options} -i {{}} -r {params.PATHO_DB} --merge-reference-sequences -d {output.dir}/mapDamage_{{}}\\\" >> {log} 2>&1 || true; \"\n        \"for filename in {output.dir}/*.tax.bam; do newname=`echo $filename | sed 's/tax\\.//g'`; mv $filename $newname; done >> {log} 2>&1; \"\n        \"mv {output.dir}/mapDamage_{output.dir}/* {output.dir} >> {log} 2>&1; \"\n        \"rm -r {output.dir}/mapDamage_results >> {log} 2>&1; \""
  },
  {
    "objectID": "aMeta-pipeline-setup.html#introduction",
    "href": "aMeta-pipeline-setup.html#introduction",
    "title": "14  aMeta pipeline setup",
    "section": "14.1 Introduction",
    "text": "14.1 Introduction\nOkay, that’s all well and good, but when are we really going to learn how to use the snakemake aMeta pipeline?\nNow that you know exactly what the different rules of aMeta do, this pipeline is no longer a black box for you and you can start using the official snakemake version.\nBut to do so, you should not rush into it but think about different criteria for its installation."
  },
  {
    "objectID": "aMeta-pipeline-setup.html#decisions",
    "href": "aMeta-pipeline-setup.html#decisions",
    "title": "14  aMeta pipeline setup",
    "section": "14.2 Decisions",
    "text": "14.2 Decisions\nThere are several decisions to be made:\n\nWhich databases do I need and where will I store them?\nWhere do I install the packages for the conda environment?\nHow do I configure snakemake to use the system on my high-performance computer?\n\nThese are some of the questions we will try to answer here.\n\n14.2.1 The necessary databases\nDepending on the type of project you want to do, there are different databases to download that we have made available to you.\nIf your project is only focused on prokaryotes, the KrakenUniq and Bowtie2 microbial databases will be sufficient, but you will still need to download the Bowtie2_FULL_NT to get the files needed to help build the MALT database.\nIf your project is interested in all organisms, you will need to download the KrakenUniq_FULL_NT and the Bowtie2_FULL_NT.\nLinks to the databases and a process for downloading them are available on the README of the official aMeta GitHub.\nIf several people in your cluster intend to use the same databases, it is better to store them in a folder accessible to all. It is also important to note that these databases are very large and will require a lot of storage space.\n\n\n14.2.2 Conda packages location\nIt may be worth thinking about the location of packages and environments installed by conda in several situations: for example, when several users will be using these packages or when the number of files or the memory of your home folder is limited.\nIn these cases, you may want to specify the desired location of the conda environment when creating it with the option –prefix.\nconda env create --prefix /path/to/your/conda-dirs workflow/envs/environment.yaml\nIf you do use the option –prefix to choose a location for your conda packages and environments, don’t forget to create a hidden “.condarc” file in your home directory containing information about the path like this:\npkgs_dirs:\n    - /path/to/your/conda-dirs/pkgs\nenvs_dirs:\n    - /path/to/your/conda-dirs/envs\n\n\n14.2.3 Make snakemake use the queue system of your high-performance computer (HPC, example here with slurm)\nOnce you have followed the installation steps described in the README file of the official aMeta GitHub, you will need to make your snakemake setup compatible with the queuing system of your high-performance computer (HPC system). Do this step before running the first snakemake command for a project involving real data. If you don’t, and run the snakemake pipeline for real data (not the dummy data from the .test folder) you risk to receive complaints from the server maintenance team for overusing the logging node.\nThere is a small sentence in the README of the official aMeta GitHub about advanced profiles for HPC systems: “For more advanced profiles for different hpc systems, see Snakemake-Profiles github page”. Click on this link. This will bring you to another GitHub with snakemake profiles available for different cluster environments.\nHere, I will explain how to setup a slurm snakemake profile because it is the most common cluster queuing system I have encountered so far. If your server uses sbatch, squeue and such, it means that it is using a slurm system too.\nIn that GitHub page click on the “slurm” repository. To install that cluster repository, you will need to use cookiecutter from Python. Load the python module of your server and follow the instruction from the Snakemake-Profiles/slurm webpage for a quickstart.\nWARNING: This will prompt a lot of questions about your computing system, so be ready with the user account name, the partition name used as default in your server, etc. Also, think through where you want to install this snakemake profile. Your home directory as suggested in the quickstart ? A private folder ? A shared folder ? It will be useful to fine-tune the config.yaml file for each rule in a different way over time (runtime, memory, partition) and to share it with colleagues so that they don’t have to redo the fine-tuning again. But keep in mind that if they modify the config.yaml file you are using while you are running an analysis, it may crash.\nSo, we leave you with these thoughts and decisions to make and we think you are good to go! If you need more information, or if something is not clear, don’t hesitate to contact us. We aim to improve this website over time. Good luck!"
  }
]