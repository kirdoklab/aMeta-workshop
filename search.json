[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "aMeta Workshop",
    "section": "",
    "text": "Welcome to the aMeta: an accurate and memory-efficient ancient Metagenomic profiling workflow (Pochon et al. 2022) course that will be held in Mersin University, Department of Biotechnology in the scope of the European Union funded NEOMATRIX collaboration project.\nIn this web page, we aim to publish the tutorial material of aMeta ancient metagenomic profiling workflow.\naMeta is designed to analyse the microbial content in ancient specimens from wide variety of source organisms (including humans).\nPlease find our preprint through this link.\nPlease find the whole pipline through this github repo\n\n\n\n\nPochon, Zoé, Nora Bergfeldt, Emrah Kırdök, Mário Vicente, Thijessen Naidoo, Tom van der Valk, N. Ezgi Altınışık, et al. 2022. “aMeta: An Accurate and Memory-Efficient Ancient Metagenomic Profiling Workflow.” bioRxiv. https://doi.org/10.1101/2022.10.03.510579."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "2  About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "course-style.html#using-conda-package-manager",
    "href": "course-style.html#using-conda-package-manager",
    "title": "3  How to follow the Course?",
    "section": "3.1 Using conda package manager",
    "text": "3.1 Using conda package manager\nEach rule can conain a conda environment. So, if you will run snakemake with --use-conda flag, the specified environment will be installed given you have the conda package manager already installed:\n    conda:\n        \"../envs/fastqc.yaml\""
  },
  {
    "objectID": "course-style.html#using-environment-modules",
    "href": "course-style.html#using-environment-modules",
    "title": "3  How to follow the Course?",
    "section": "3.2 Using environment modules",
    "text": "3.2 Using environment modules\nIf you want to use the environmental modules, then you need to use envmodules flag with the snakemake. In this case, you would like to modify the envirınmental module file.\n envmodules:\n        *config[\"envmodules\"][\"fastqc\"]"
  },
  {
    "objectID": "course-style.html#benchmarks",
    "href": "course-style.html#benchmarks",
    "title": "3  How to follow the Course?",
    "section": "3.3 Benchmarks",
    "text": "3.3 Benchmarks\nYou could find benchmarking stats with under the benchmarking folder.\n    benchmark:\n        \"benchmarks/FASTQC_BEFORE_TRIMMING/{sample}.benchmark.txt\""
  },
  {
    "objectID": "course-style.html#logs",
    "href": "course-style.html#logs",
    "title": "3  How to follow the Course?",
    "section": "3.4 Logs",
    "text": "3.4 Logs\nIf you have problems with your snakemake run, each rule will have a log file. So you can check the errors:\n    log:\n        \"logs/FASTQC_BEFORE_TRIMMING/{sample}.log\","
  },
  {
    "objectID": "qc.html#quality-control-before-trimming",
    "href": "qc.html#quality-control-before-trimming",
    "title": "4  Quality control and trimming",
    "section": "4.1 Quality control before trimming",
    "text": "4.1 Quality control before trimming\nThe pipeline starts with quality control step. We will first run FastQC tool to check the qualitu of the fastq files.\nThis is the rule in the workflow/rules/qc.smk file. Input files are retrieved from the samplesheet that you provided. And you will have two output files html and zip files. The html file contains the final report.\n\nrule FastQC_BeforeTrimming:\n    \"\"\"Run fastq before trimming\"\"\"\n    output:\n        html=\"results/FASTQC_BEFORE_TRIMMING/{sample}_fastqc.html\",\n        zip=\"results/FASTQC_BEFORE_TRIMMING/{sample}_fastqc.zip\",\n    input:\n        fastq=lambda wildcards: samples.loc[wildcards.sample].fastq,\n    message:\n        \"FastQC_BeforeTrimming: RUNNING QUALITY CONTROL WITH FASTQC FOR SAMPLE {input.fastq} BEFORE TRIMMING ADAPTERS\"\n    threads: 2\n    shell:\n        \"fastqc {input.fastq} --threads {threads} --nogroup --outdir results/FASTQC_BEFORE_TRIMMING &> {log}\"\n\nHere is a simplified version of this code:\n\nfastqc FASTQ --threads 2 --nogroup --outdir results/FASTQC_BEFORE_TRIMMING/\n\n## Quality trimming step\nAfter we run quality controls. We will run cutadapt tool to remove the leftover adapters and low quality bases. Here is the overview of this rule.\nYou can add or remove adapters using the config/config.yaml file.\nIn this file the inputs will be retrieved again from the samplesheet file. And the outputs will be placed to the results/CUTADAPT_ADAPTER_TRIMMING/ folder.\nrule Cutadapt_Adapter_Trimming:\n    output:\n        fastq=\"results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz\",\n    input:\n        fastq=lambda wildcards: samples.loc[wildcards.sample].fastq,\n    params:\n        adapters=\" \".join([f\"-a {x}\" for x in ADAPTERS]),\n    message:\n        \"Cutadapt_Adapter_Trimming: TRIMMING ADAPTERS FOR SAMPLE {input.fastq} WITH CUTADAPT\"\n    threads: 1\n    shell:\n        \"cutadapt {params.adapters} --minimum-length 30 -o {output.fastq} {input.fastq} &> {log}\"\n\nHere is how we will run it for one file:\ncutadapt -a ADAPTERS --minimum-length 30 -o results/CUTADAPT_ADAPTER_TRIMMING/sample"
  },
  {
    "objectID": "qc.html#quality-control-after-trimming",
    "href": "qc.html#quality-control-after-trimming",
    "title": "4  Quality control and trimming",
    "section": "4.2 Quality control after trimming",
    "text": "4.2 Quality control after trimming\nThen we will run the quality control after the trimming step:\nrule FastQC_AfterTrimming: output: html=“results/FASTQC_AFTER_TRIMMING/{sample}.trimmed_fastqc.html”, zip=“results/FASTQC_AFTER_TRIMMING/{sample}.trimmed_fastqc.zip”, input: fastq=“results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz”, threads: 2 message: “FastQC_AfterTrimming: RUNNING QUALITY CONTROL WITH FASTQC FOR SAMPLE {input.fastq} AFTER TRIMMING ADAPTERS” shell: “fastqc {input.fastq} –threads {threads} –nogroup –outdir results/FASTQC_AFTER_TRIMMING &> {log}”\nHere is how we will run it for one file:\nfastqc results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz --threads 2 --nogroup --outdir results/FASTQC_AFTER_TRIMMING"
  },
  {
    "objectID": "qc.html#combine-quality-control-outputs",
    "href": "qc.html#combine-quality-control-outputs",
    "title": "4  Quality control and trimming",
    "section": "4.3 Combine quality control outputs",
    "text": "4.3 Combine quality control outputs\nAt last we will combine all the fastqc reports into one file using Multifastqc tool:\nrule MultiQC:\n    \"\"\"Run MultiQC\"\"\"\n    output:\n        html=\"results/MULTIQC/multiqc_report.html\",\n    input:\n        unpack(multiqc_input),\n    params:\n        config=os.path.join(WORKFLOW_DIR, \"envs\", \"multiqc_config.yaml\"),\n    envmodules:\n        *config[\"envmodules\"][\"multiqc\"],\n    benchmark:\n        \"benchmarks/MULTIQC/MULTIQC.benchmark.txt\"\n    message:\n        \"MultiQC: COMBINING QUALITY CONTROL METRICS WITH MULTIQC\"\n    shell:\n        'echo {input} | tr \" \" \"\\n\" > {output.html}.fof;'\n        \"multiqc -c {params.config} -l {output.html}.fof --verbose --force --outdir results/MULTIQC &> {log}\""
  },
  {
    "objectID": "krakenuniq.html#krakenuniq-database",
    "href": "krakenuniq.html#krakenuniq-database",
    "title": "5  KrakenUniq",
    "section": "5.1 KrakenUniq database",
    "text": "5.1 KrakenUniq database\nA full NT database for KrakenUniq is [available for download] (https://www.biorxiv.org/node/2777891.external-links.html) through SciLifeLab. Other databases are available online and for this workshop, we will be using MinusB. The database is installed on the cluster.\nKrakenUniq is relatively fast and allows to screen aDNA samples against as large database as possible. The KrakenUniq paper suggests that KrakenUniq is not less accurate compared to alignment tools such as BLAST and MEGAN.\nKrakenUniq is very handy as it provides k-mer coverage information that is equivalent to “breadth of coverage” that one can extract via alignment.\nHere we show an example of how to run a sample using the full NT KrakenUniq database:"
  },
  {
    "objectID": "krakenuniq.html#run-krakenuniq",
    "href": "krakenuniq.html#run-krakenuniq",
    "title": "5  KrakenUniq",
    "section": "5.2 Run KrakenUniq",
    "text": "5.2 Run KrakenUniq\nDBNAME=/proj/mnt/NEOGENE4/share/KrakenUniqDatabase\nPATH_TO_KRAKENUNIQ=/usr/local/sw/anaconda3/envs/aMeta/KrakenUniq\ntime $PATH_TO_KRAKENUNIQ/./krakenuniq --db $DBNAME --fastq-input sample_name.fastq.gz --threads 80 --output sample_name.fastq.gz_sequences.krakenuniq_Full_NT --report-file sample_name.fastq.gz_krakenuniq.output_Full_NT --gzip-compressed --only-classified-out\nHere we demonstrate how to filter the KrakenUniq output using the kmers=1000 threshold, extract sequence IDs associated with the taxIDs that passed the kmers=1000 threshold and visualize the abundances with Krona:"
  },
  {
    "objectID": "krakenuniq.html#screening-workflow",
    "href": "krakenuniq.html#screening-workflow",
    "title": "5  KrakenUniq",
    "section": "5.3 Screening workflow",
    "text": "5.3 Screening workflow\necho \"STEP1: PREPARING PIPELINE\"\nmodule load bioinfo-tools; module load perl; module load bowtie2; module load samtools; module load mapDamage/2.0.9; module load gnuparallel; module load Kraken2; module load FastQC\nFASTQ_FILE=$1; cd $SNIC_TMP\nif [ ! -d $SNIC_TMP/${FASTQ_FILE}_PipelineOutput ];\nthen\ncp /proj/snic2018-8-150/uppstore2018095/private/NBIS_Demo/fastq_files/${FASTQ_FILE} $SNIC_TMP; mkdir $SNIC_TMP/${FASTQ_FILE}_PipelineOutput\nfi\nif [ ! -d $SNIC_TMP/DBDIR ];\nthen\ncp -r /proj/snic2018-8-150/uppstore2018095/private/NBIS_Demo/KrakenUniq/krakenuniq/KrakenUniq/DBDIR $SNIC_TMP; cp -r /proj/snic2018-8-150/uppstore2018095/private/NBIS_Demo/PathoGenome $SNIC_TMP\nfi\nSAMPLE=$SNIC_TMP/${FASTQ_FILE}; DBNAME=$SNIC_TMP/DBDIR; OUTPUT=$SNIC_TMP/${FASTQ_FILE}_PipelineOutput; PATH_TO_PATHO_GENOME=$SNIC_TMP/PathoGenome; PATHO_GENOME=$PATH_TO_PATHO_GENOME/library.pathogen.fna\nPATH_TO_KRAKENUNIQ=/proj/snic2018-8-150/uppstore2018095/private/NBIS_Demo/KrakenUniq/krakenuniq/KrakenUniq; PATH_TO_SCRIPTS=/proj/snic2018-8-150/uppstore2018095/private/NBIS_Demo/scripts\nKRAKEN2_DB=/proj/snic2018-8-150/uppstore2018095/private/NBIS_Demo/DBDIR_Kraken2_MicrobialNT\n \nprintf \"\\n\"; echo \"STEP2: RUNNING QUALITY CONTROL WITH FASTQC\"\nif [ ! -f $OUTPUT/*_fastqc.zip ];\nthen\ntime fastqc ${SAMPLE} --outdir $OUTPUT\nelse\necho \"SKIPPING RUNNING QUALITY CONTROL WITH FASTQC BECAUSE OUTPUT EXISTS\"\nfi\n \nprintf \"\\n\"; echo \"STEP3: RUNNING KRAKEN2\"\nif [ ! -f $OUTPUT/kraken2.output ];\nthen\ntime kraken2 ${SAMPLE} --db $KRAKEN2_DB --gzip-compressed --threads 10 --classified-out $OUTPUT/classified_sequences.kraken2 --unclassified-out $OUTPUT/unclassified_sequences.kraken2 --output $OUTPUT/sequences.kraken2 --use-names --report $OUTPUT/kraken2.output\nelse\necho \"SKIPPING RUNNING KRAKEN2 BECAUSE OUTPUT EXISTS\"\nfi\n \nprintf \"\\n\"; echo \"STEP4: RUNNING KRAKEN-UNIQ\"\nif [ ! -f $OUTPUT/krakenuniq.output ];\nthen\ntime $PATH_TO_KRAKENUNIQ/./krakenuniq --db $DBNAME --fastq-input $SAMPLE --threads 10 --output $OUTPUT/sequences.krakenuniq --report-file $OUTPUT/krakenuniq.output --gzip-compressed --only-classified-out\nelse\necho \"SKIPPING RUNNING KRAKEN-UNIQ BECAUSE OUTPUT EXISTS\"\nfi\n \nprintf \"\\n\"; echo \"STEP5: FILTERING KRAKEN-UNIQ OUTPUT\"\ncd $OUTPUT; Rscript $PATH_TO_SCRIPTS/pipeline.R\ncut -f7 $OUTPUT/krakenuniq.output.pathogens | tail -n +2 > $OUTPUT/taxID.pathogens\n \nprintf \"\\n\"; echo \"STEP6: COMPUTING MEAN READ LENGTH\"\nif [ ! -f $OUTPUT/mean.reads.length ];\nthen\ncat $OUTPUT/taxID.pathogens | parallel \"${PATH_TO_KRAKENUNIQ}/./krakenuniq-extract-reads {} $OUTPUT/sequences.krakenuniq ${SAMPLE} > $OUTPUT/{}.temp.fq\"\necho \"MeanReadLength\" > $OUTPUT/mean.reads.length; cd $OUTPUT\nfor i in $(cat taxID.pathogens); do awk '{if(NR%4==2) print length($1)}' ${i}.temp.fq | awk '{ sum += $0 } END { if (NR > 0) print sum / NR }' >> mean.reads.length; done; rm *.temp.fq\nelse\necho \"SKIPPING COMPUTING MEAN READ LENGTH BECAUSE OUTPUT EXISTS\"\nfi\n \nprintf \"\\n\"; echo \"STEP7: GENERATING FINAL KRAKENUNIQ OUTPUT\"\npaste krakenuniq.output.pathogens mean.reads.length > krakenuniq.output.pathogens_with_mean_read_length\ncat krakenuniq.output.pathogens_with_mean_read_length\n \nprintf \"\\n\"; echo \"STEP8: PERFORMING ALIGNMENT TO PATHO-GENOME\"\ntime bowtie2 --large-index -x $PATHO_GENOME --threads 10 --end-to-end --very-sensitive -U $SAMPLE | samtools view -bS -q 1 -h -@ 10 - > $OUTPUT/test_sample_AlignedToSpecies.bam\nsamtools sort $OUTPUT/test_sample_AlignedToSpecies.bam -@ 10 > $OUTPUT/test_sample_AlignedToSpecies.sorted.bam; samtools index $OUTPUT/test_sample_AlignedToSpecies.sorted.bam\nsamtools markdup -r $OUTPUT/test_sample_AlignedToSpecies.sorted.bam $OUTPUT/test_sample_AlignedToSpecies.sorted.dedup.bam; samtools index $OUTPUT/test_sample_AlignedToSpecies.sorted.dedup.bam\n \nprintf \"\\n\"; echo \"STEP9: EXTRACTING ALIGNMENTS FOR CANDIDATES\"\ncat $OUTPUT/taxID.pathogens | parallel \"grep -w {} ${PATH_TO_PATHO_GENOME}/seqid2taxid.pathogen.map | cut -f1 > ${OUTPUT}/{}.seq.ids\"\nfor i in $(cat $OUTPUT/taxID.pathogens); do samtools view -bh $OUTPUT/test_sample_AlignedToSpecies.sorted.dedup.bam -@ 10 $(cat $OUTPUT/${i}.seq.ids | tr \"\\n\" \" \") > $OUTPUT/${i}.output.bam; done\n \nprintf \"\\n\"; echo \"STEP10: RUNNING MAPDAMAGE ANCIENT STATUS ANALYSIS\"\nfind . -name '*.output.bam' | parallel \"mapDamage -i {} -r ${PATHO_GENOME} --merge-reference-sequences -d ${OUTPUT}/results_{}\"\n \nprintf \"\\n\"; echo \"STEP11: ASSIGN ANCIENT STATUS\"\nRscript $PATH_TO_SCRIPTS/ancient_status.R 0.05 0.9 $OUTPUT\n \nprintf \"\\n\"; echo \"STEP12: COMPUTE DEPTH AND BREADTH OF COVERAGE FROM ALIGNMENTS\"\necho \"NUMBER_OF_READS\" > DepthOfCoverage.${FASTQ_FILE}.txt; echo \"GENOME_LENGTH\" > GenomeLength.${FASTQ_FILE}.txt; echo \"BREADTH_OF_COVERAGE\" > BreadthOfCoverage.${FASTQ_FILE}.txt\nfor j in $(cut -f7 final_output.txt | sed '1d')\ndo\necho \"Organism $j\"\nif [ -s ${j}.output.bam ] && [ \"$(samtools view ${j}.output.bam | wc -l)\" -ne \"0\" ];\nthen\nsamtools sort ${j}.output.bam > ${j}.output.sorted.bam; samtools depth ${j}.output.sorted.bam | cut -f1 | uniq > Genomes_${j}.txt\nGENOME_LENGTH=$(grep -wFf Genomes_${j}.txt $PATH_TO_PATHO_GENOME/GenomeLength.txt | cut -f2 | awk '{ sum += $1; } END { print sum; }')\nif [ -s ${j}.output.sorted.bam ];\nthen\nNUMBER_OF_READS=$(samtools view ${j}.output.sorted.bam | wc -l); NUMBER_OF_COVERED_POSITIONS=$(samtools depth ${j}.output.sorted.bam | wc -l)\nelse\necho \"NA\" >> DepthOfCoverage.${FASTQ_FILE}.txt; echo \"NA\" >> BreadthOfCoverage.${FASTQ_FILE}.txt; echo \"NA\" >> GenomeLength.${FASTQ_FILE}.txt\ncontinue\nfi\necho $NUMBER_OF_READS >> DepthOfCoverage.${FASTQ_FILE}.txt; echo $GENOME_LENGTH >> GenomeLength.${FASTQ_FILE}.txt\necho \"scale=10 ; ($NUMBER_OF_COVERED_POSITIONS / $GENOME_LENGTH)*100\" | bc >> BreadthOfCoverage.${FASTQ_FILE}.txt\nelse\necho \"NA\" >> DepthOfCoverage.${FASTQ_FILE}.txt; echo \"NA\" >> BreadthOfCoverage.${FASTQ_FILE}.txt; echo \"NA\" >> GenomeLength.${FASTQ_FILE}.txt\ncontinue\nfi\ndone; rm Genomes_*.txt; paste final_output.txt DepthOfCoverage.${FASTQ_FILE}.txt GenomeLength.${FASTQ_FILE}.txt BreadthOfCoverage.${FASTQ_FILE}.txt > final_output_corrected.txt\n \ncp -r $OUTPUT /proj/snic2018-8-150/uppstore2018095/private/NBIS_Demo; cat final_output_corrected.txt\nprintf \"\\n\"; echo \"PIPELINE FINISHED SUCCESSFULLY\"\n\n5.3.1 Kraken2 vs KrakenUniq\nKraken2 is incredibly fast but unfortunately has a high False Positive Rate. KrakenUniq reduces the fraction of false discoveries (compared to Kraken2) by reporting the number of unique k-mers that is analogous to breadth of coverage information. Therefore filtering KrakenUniq output by number of unique k-mers, we get a more reliable list of candidates compared to Kraken2 (the output from Kraken2 can only be filtered by microbial abundance, i.e. depth of coverage). Considering filtered KrakenUniq output as a ground truth, we investigated how many records from Kraken2 output should be retained in order to capture all the species identified with KareknUniq, i.e. how long down the Kraken2 list one needs to go in order to detect all the species reported by KrakenUniq.\nWhat we can conclude after having screened ~100 samples is that a typical sample contains ~100 species reported (after filtering) by KrakenUniq. This corresponds to approximately ~1000 species in Kraken2 output (or 0.01% assigned reads). This implies that if we select top 1000 species in Kraken2, 900 species will probably be false positives and only 10% of all species will be true positives. If we consider KrakenUniq output as a ground truth."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Pochon, Zoé, Nora Bergfeldt, Emrah Kırdök, Mário Vicente, Thijessen\nNaidoo, Tom van der Valk, N. Ezgi Altınışık, et al. 2022. “aMeta:\nAn Accurate and Memory-Efficient Ancient Metagenomic Profiling\nWorkflow.” bioRxiv. https://doi.org/10.1101/2022.10.03.510579."
  }
]