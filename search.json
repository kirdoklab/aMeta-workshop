[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "aMeta Workshop",
    "section": "",
    "text": "Welcome to the aMeta: an accurate and memory-efficient ancient Metagenomic profiling workflow course that will be held in Mersin University, Department of Biotechnology in the scope of the European Union funded NEOMATRIX collaboration project.\nIn this web page, we aim to publish the tutorial material of aMeta ancient metagenomic profiling workflow.\naMeta (Pochon et al. 2022) is designed to analyse the microbial content in ancient specimens from a wide variety of source organisms (including humans).\nPlease find the whole pipeline through this github repo\n\n\n\n\n\nPochon, Zoé, Nora Bergfeldt, Emrah Kırdök, Mário Vicente, Thijessen Naidoo, Tom van der Valk, N. Ezgi Altınışık, et al. 2022. “aMeta: An Accurate and Memory-Efficient Ancient Metagenomic Profiling Workflow.” bioRxiv. https://doi.org/10.1101/2022.10.03.510579."
  },
  {
    "objectID": "course-style.html#using-conda-package-manager",
    "href": "course-style.html#using-conda-package-manager",
    "title": "2  How to understand snakemake?",
    "section": "2.1 Using conda package manager",
    "text": "2.1 Using conda package manager\nEach rule can contain a conda environment. So, if you will run snakemake with --use-conda flag, the specified environment will be installed given you have the conda package manager already installed:\n    conda:\n        \"../envs/fastqc.yaml\""
  },
  {
    "objectID": "course-style.html#using-environment-modules",
    "href": "course-style.html#using-environment-modules",
    "title": "2  How to understand snakemake?",
    "section": "2.2 Using environment modules",
    "text": "2.2 Using environment modules\nIf you want to use the environmental modules, then you need to use envmodules flag with the snakemake. In this case, you would like to modify the envirınmental module file.\n envmodules:\n        *config[\"envmodules\"][\"fastqc\"]"
  },
  {
    "objectID": "course-style.html#benchmarks",
    "href": "course-style.html#benchmarks",
    "title": "2  How to understand snakemake?",
    "section": "2.3 Benchmarks",
    "text": "2.3 Benchmarks\nYou could find benchmarking stats with under the benchmarking folder.\n    benchmark:\n        \"benchmarks/FASTQC_BEFORE_TRIMMING/{sample}.benchmark.txt\""
  },
  {
    "objectID": "course-style.html#logs",
    "href": "course-style.html#logs",
    "title": "2  How to understand snakemake?",
    "section": "2.4 Logs",
    "text": "2.4 Logs\nIf you have problems with your snakemake run, each rule will have a log file. So you can check the errors:\n    log:\n        \"logs/FASTQC_BEFORE_TRIMMING/{sample}.log\","
  },
  {
    "objectID": "rulegraph.html#ameta-workflow-graph",
    "href": "rulegraph.html#ameta-workflow-graph",
    "title": "3  aMeta workflow graph",
    "section": "3.1 aMeta workflow graph",
    "text": "3.1 aMeta workflow graph\nHere is an overview of all the rules of the aMeta snakemake pipeline. Such an overview of a workflow in the form of a graph is called a dag or rulegraph. You can consider these rules as steps of the workflow going from up to down.\n\n\n\n\n\nRulegraph"
  },
  {
    "objectID": "organization.html",
    "href": "organization.html",
    "title": "NEOMATRIX Workshop",
    "section": "",
    "text": "On this page, we would like to give information on the details of the workshop:\n\nWorkshop information\nThe team\nComputing details\n\nPlease check this page frequently, there will be new updates."
  },
  {
    "objectID": "info-mersin.html",
    "href": "info-mersin.html",
    "title": "4  Information about the place",
    "section": "",
    "text": "Please find the accomodation and flight information through this Google Document.\nPlease find the food selection information in this Google Document. Please specify your selection if you have not selected yet."
  },
  {
    "objectID": "about.html#about-this-site",
    "href": "about.html#about-this-site",
    "title": "5  About",
    "section": "5.1 About this site",
    "text": "5.1 About this site\nThis site is built for the aMeta workshop in Mersin Feb 6 – 10 2023."
  },
  {
    "objectID": "about.html#the-team",
    "href": "about.html#the-team",
    "title": "5  About",
    "section": "5.2 The team",
    "text": "5.2 The team\n\n5.2.1 Emrah Kırdök\nEmrah Kırdök is an assistant professor at Mersin University.\nEmrah’s research focuses on understanding human history through ancient DNA (aDNA). Generally, a little proportion of aDNA sequences could be mapped to its host reference genome. The remaining fragments come either from the microbiome of the ancient host or the environment that encapsulates the host.\nBy analyzing the remaining aDNA sequences with the state-of-the-art metagenomic methods it’s possible to (i) construct the microbiome composition of ancient humans, (ii) reconstruct the ancient pathogenic genomes, (iii) understand the evolutionary mechanisms of the ancient pathogens.\n\n\n5.2.2 Zoé Pochon\nZoé started her PhD in Archaeology at the Centre for Palaeogenetics in Stockholm in 2021. Zoé has training in biology and history and is interested in using archaeogenetics to bring new clues to debates on historical and biological questions. Her current work focuses on using bioinformatics tools to analyse the DNA and RNA of ancient viruses, as well as other microbes to get a better picture of the common pathogens at different time periods in history and to help understand past epidemics.\nSome of her research projects have included the DNA of combatants from a Bronze Age battlefield to infer their origin and estimate the selection coefficient of the lactase persistence, an allelic mutation already present in some of them.\n\n\n5.2.3 Nora Bergfeldt\nNora started her PhD in Zoology at the Centre for Palaeogenetics in 2019. She has a background in ecology, biodiversity and genomics. Her research focuses on using metagenomic tools to study bacteria and other microbes in ancient humans, with a particular interest in studying the evolution of pathogens."
  },
  {
    "objectID": "cluster-connection.html#how-we-will-work",
    "href": "cluster-connection.html#how-we-will-work",
    "title": "6  How to connect to servers?",
    "section": "6.1 How we will work?",
    "text": "6.1 How we will work?\nIn this workshop, we will use our computers, and we will connect to high performance computing systems. Therefore, you will need to bring your laptops with you.\nYou should have been comfortable using the Unix/Linux command line to take the course. We know that there will be several operating systems. So please check these recomendations based on your operating system.\n\n6.1.1 Linux systems\nIf you have a Linux system (e.g. Ubuntu), you do not need to worry a lot. Most of the command line tools (like ssh) will be already installed on your system\n\n\n6.1.2 Mac systems\nMac systems also have a default support on the command line. The terminal application on your Mac system would be enough. But we advise you to install iTerm2 as a terminal replacement for your Mac.\n\n\n6.1.3 Windows systems\nIf you have a Windows system, the easiest choice to connect a remote server is Mobaxterm tool."
  },
  {
    "objectID": "cluster-connection.html#cluster-systems",
    "href": "cluster-connection.html#cluster-systems",
    "title": "6  How to connect to servers?",
    "section": "6.2 Cluster systems",
    "text": "6.2 Cluster systems\nIn this workshop, we have two different cluster systems at our disposal. The first system TRUBA (Turkish National Scientific Cluster) that is funded by the TUBİTAK (The Scientific and Technological Research Council of Türkiye). The second system is located at Middle East Technical University.\n\n6.2.1 TRUBA Cluster system\nThis cluster system accepts connections from University networks. Therefore, you should be in a University network in order to access TRUBA computers. We will provide a username and a password for your connection.\nThen, you can connect to this system like this:\nssh egitim@levrek1.ulakbim.gov.tr\nThere is a trick to connect TRUBA system outside of the university network. We will describe it in the document.\n\n\n6.2.2 Middle East Technical University computing systems\nThis computer system is located at Middle East Technical University. It consists of two clusters: baggins and NEOGENE. We will work mostly on baggings to explain tools. At the end, we plan to send snakemake jobs on the NEOGENE system.\nHowever, NEOGENE system does not accept connections outside from university networks, but baggins can. So we will first connect to baggins via ssh first. Then we can connect to NEOGENE from this system.\nYour username and password will be provided on the first day of the workshop.\nConnect to baggins server with your username like this:\nssh -p 2275 username@144.122.38.49\nAfterwards, you can connect to NEOGENE like this:\nssh username@144.122.38.50\n\n\n6.2.3 Connecting to TRUBA system outside of the universiy network\nWe can connect to TRUBA system with a small trick. First we will connect to Baggins cluster, so we will be on the Middle East Technical Univerisy network. Then we can connect to TRUBA.\n\nssh -p 2275 username@144.122.38.49\n\nssh egitim@levrek1.ulakbim.gov.tr"
  },
  {
    "objectID": "cluster-connection.html#configuration",
    "href": "cluster-connection.html#configuration",
    "title": "6  How to connect to servers?",
    "section": "6.3 Configuration",
    "text": "6.3 Configuration\nAll the tools are installed on these server systems. Before using the tools, we need to set the PATH variable and show the system where these tools are installed.\n\n6.3.1 Configuration of the TRUBA system\nTo use all the tools for aMeta, please set your PATH variable like this:\nexport PATH=${PATH}:/truba/home/egitim/miniconda3/envs/aMeta/bin/\n\n\n6.3.2 Configuration on the Ankara computers\nTo use all the tools for aMeta, please set your PATH variable like this:\nexport PATH=${PATH}:/usr/local/sw/anaconda3/envs/aMeta/"
  },
  {
    "objectID": "course-setup.html#course-setup",
    "href": "course-setup.html#course-setup",
    "title": "7  Course setup",
    "section": "7.1 Course setup",
    "text": "7.1 Course setup\nOnce you have connected to the TRUBA server with your own username, you can begin to setup your folder for the course.\nCreate the necessary folders:\nmkdir -p workshop/data\nmkdir -p workshop/logs/slurm\nCreate links to the files you will use:\n# Create links for the FastQ files\nln -s /truba/home/egitim/workshop/data/* workshop/data\n# Create links for the scripts \nln -s /truba/home/egitim/workshop/*.sh workshop\nRight now, you have created your workshop folder. From now on, you will be working on this folder. Please change your directory now:\ncd workshop\nThen you can follow step after step the pipeline. Good luck !"
  },
  {
    "objectID": "qc.html#introduction",
    "href": "qc.html#introduction",
    "title": "8  Quality control and trimming",
    "section": "8.1 Introduction",
    "text": "8.1 Introduction\nThe pipeline starts with a quality controlling step. This step contains four main rules:\n\nQuality controlling of raw fastq files using fastqc tool\nRemoving leftover adapters an low quality bases using cutadapt\nQuality controlling of the trimmed fastq files using fastqc tool\nCombining several fastqc outputs into one file using MultiQC tool"
  },
  {
    "objectID": "qc.html#quality-control-before-trimming",
    "href": "qc.html#quality-control-before-trimming",
    "title": "8  Quality control and trimming",
    "section": "8.2 Quality control before trimming",
    "text": "8.2 Quality control before trimming\nInput files are retrieved from the sample sheet that you provided. And you will have two output files html and zip files. The html file contains the final report. Here is the simplified snakemake rule:\nrule FastQC_BeforeTrimming:\n    \"\"\"Run fastq before trimming\"\"\"\n    output:\n        html=\"results/FASTQC_BEFORE_TRIMMING/{sample}_fastqc.html\",\n        zip=\"results/FASTQC_BEFORE_TRIMMING/{sample}_fastqc.zip\",\n    input:\n        fastq=lambda wildcards: samples.loc[wildcards.sample].fastq,\n    message:\n        \"FastQC_BeforeTrimming: RUNNING QUALITY CONTROL WITH FASTQC FOR SAMPLE {input.fastq} BEFORE TRIMMING ADAPTERS\"\n    threads: 2\n    shell:\n        \"fastqc {input.fastq} --threads {threads} --nogroup --outdir results/FASTQC_BEFORE_TRIMMING &> {log}\"\n\nHere is a simplified version of this code:\nfastqc FASTQ \\\n    --threads 2 \\\n    --nogroup \\\n    --outdir results/FASTQC_BEFORE_TRIMMING/\nTo run this part using the fastqc tool, please copy and paste this command on your terminal. Remember to change your account name, with the username provided for you:\nsbatch FastQC_BeforeTrimming.sh --account=egitim\nPlease, remember to check the status of your job using this command:\nsqueue -u egitim\nOnce your job has finished, please check the output files like this:\nls -ltrh results/FASTQC_BEFORE_TRIMMING/"
  },
  {
    "objectID": "qc.html#quality-trimming-step",
    "href": "qc.html#quality-trimming-step",
    "title": "8  Quality control and trimming",
    "section": "8.3 Quality trimming step",
    "text": "8.3 Quality trimming step\nAfter we run quality controls, the pipeline will run cutadapt tool to remove the leftover adapters and low quality bases. Here is the overview of this rule.\nYou can add or remove adapters using the config/config.yaml file.\nIn this file the inputs will be retrieved again from the samplesheet file. And the outputs will be placed to the results/CUTADAPT_ADAPTER_TRIMMING/ folder.\nrule Cutadapt_Adapter_Trimming:\n    output:\n        fastq=\"results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz\",\n    input:\n        fastq=lambda wildcards: samples.loc[wildcards.sample].fastq,\n    params:\n        adapters=\" \".join([f\"-a {x}\" for x in ADAPTERS]),\n    message:\n        \"Cutadapt_Adapter_Trimming: TRIMMING ADAPTERS FOR SAMPLE {input.fastq} WITH CUTADAPT\"\n    threads: 1\n    shell:\n        \"cutadapt {params.adapters} --minimum-length 30 -o {output.fastq} {input.fastq} &> {log}\"\n\nHere is how we will run it for one file:\ncutadapt -a ADAPTERS --minimum-length 30 -o results/CUTADAPT_ADAPTER_TRIMMING/sample  \nPlease run the command for this part like this:\nsbatch Cutadapt_Adapter_Trimming.sh --account=egitim\nAfter your job has finished, please check the output files:\nls -ltrh results/CUTADAPT_ADAPTER_TRIMMING/"
  },
  {
    "objectID": "qc.html#quality-control-after-trimming",
    "href": "qc.html#quality-control-after-trimming",
    "title": "8  Quality control and trimming",
    "section": "8.4 Quality control after trimming",
    "text": "8.4 Quality control after trimming\nThen we will run the quality control after the trimming step:\nrule FastQC_AfterTrimming:\n    output:\n        html=\"results/FASTQC_AFTER_TRIMMING/{sample}.trimmed_fastqc.html\",\n        zip=\"results/FASTQC_AFTER_TRIMMING/{sample}.trimmed_fastqc.zip\",\n    input:\n        fastq=\"results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz\",\n    threads: 2\n    message:\n        \"FastQC_AfterTrimming: RUNNING QUALITY CONTROL WITH FASTQC FOR SAMPLE {input.fastq} AFTER TRIMMING ADAPTERS\"\n    shell:\n        \"fastqc {input.fastq} --threads {threads} --nogroup --outdir results/FASTQC_AFTER_TRIMMING &> {log}\"\nHere is how we will run it for one file:\nfastqc results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz --threads 2 --nogroup --outdir results/FASTQC_AFTER_TRIMMING\nPlease run the sbatch command for this part like this:\nsbatch FastQC_AfterTrimming.sh --account=egitim\nPlease check the outputs using this command:\nls -ltrh results/FASTQC_AFTER_TRIMMING/"
  },
  {
    "objectID": "qc.html#combine-quality-control-outputs",
    "href": "qc.html#combine-quality-control-outputs",
    "title": "8  Quality control and trimming",
    "section": "8.5 Combine quality control outputs",
    "text": "8.5 Combine quality control outputs\nAt last we will combine all the fastqc reports into one file using Multifastqc tool. This part will not be executed in this tutorial.\nrule MultiQC:\n    \"\"\"Run MultiQC\"\"\"\n    output:\n        html=\"results/MULTIQC/multiqc_report.html\",\n    input:\n        unpack(multiqc_input),\n    params:\n        config=os.path.join(WORKFLOW_DIR, \"envs\", \"multiqc_config.yaml\"),\n    envmodules:\n        *config[\"envmodules\"][\"multiqc\"],\n    benchmark:\n        \"benchmarks/MULTIQC/MULTIQC.benchmark.txt\"\n    message:\n        \"MultiQC: COMBINING QUALITY CONTROL METRICS WITH MULTIQC\"\n    shell:\n        'echo {input} | tr \" \" \"\\n\" > {output.html}.fof;'\n        \"multiqc -c {params.config} -l {output.html}.fof --verbose --force --outdir results/MULTIQC &> {log}\""
  },
  {
    "objectID": "krakenuniq.html#krakenuniq-workflow",
    "href": "krakenuniq.html#krakenuniq-workflow",
    "title": "9  KrakenUniq database",
    "section": "9.1 KrakenUniq workflow",
    "text": "9.1 KrakenUniq workflow\n\n9.1.1 Run KrakenUniq\nPlease note that it will not be possible to run this first step during this workshop due to issues with storage space. The output files will be provided by the course leaders.\nTo start, we need to set up the path to the database– we call it DBNAME.\nThis is the rule in the workflow/rules/krakenuniq.smk file. Input files are retrieved from the output of the Quality Control step.\nrule KrakenUniq:\n    \"\"\"Run KrakenUniq on trimmed fastq data\"\"\"\n    output:\n        report=\"results/KRAKENUNIQ/{sample}/krakenuniq.output\",\n        seqs=\"results/KRAKENUNIQ/{sample}/sequences.krakenuniq\",\n    input:\n        fastq=\"results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz\",\n    params:\n        DB=config[\"krakenuniq_db\"],\n    threads: 10\n    log:\n        \"logs/KRAKENUNIQ/{sample}.log\",\n    conda:\n        \"../envs/krakenuniq.yaml\"\n    envmodules:\n        *config[\"envmodules\"][\"krakenuniq\"],\n    benchmark:\n        \"benchmarks/KRAKENUNIQ/{sample}.benchmark.txt\"\n    message:\n        \"KrakenUniq: PERFORMING TAXONOMIC CLASSIFICATION OF SAMPLE {input.fastq} WITH KRAKENUNIQ\"\n    shell:\n        \"krakenuniq --preload --db {params.DB} --fastq-input {input.fastq} --threads {threads} --output {output.seqs} --report-file {output.report} --gzip-compressed --only-classified-out &> {log}\"\nHere is a simplified version of this code:\nDBNAME=db/\nPATH=${PATH}:/truba/home/egitim/miniconda3/envs/aMeta/bin/\nkrakenuniq --preload --db $DBNAME --fastq-input ${sample_name} --threads 4 --output ${sample_name}.sequences.krakenuniq --report-file ${sample_name}.krakenuniq.output --gzip-compressed --only-classified-out &> logs/KRAKENUNIQ/${sample_name}.log\n\n\n9.1.2 Filtering the output\nAfter running KrakenUniq, the next step is to filter the output. The input is the krakenuniq.output generated in the previous step.\nrule Filter_KrakenUniq_Output:\n    output:\n        filtered=\"results/KRAKENUNIQ/{sample}/krakenuniq.output.filtered\",\n        pathogens=\"results/KRAKENUNIQ/{sample}/krakenuniq.output.pathogens\",\n        pathogen_tax_id=\"results/KRAKENUNIQ/{sample}/taxID.pathogens\",\n    input:\n        krakenuniq=\"results/KRAKENUNIQ/{sample}/krakenuniq.output\",\n        pathogenomesFound=config[\"pathogenomesFound\"],\n    log:\n        \"logs/FILTER_KRAKENUNIQ_OUTPUT/{sample}.log\",\n    params:\n        exe=WORKFLOW_DIR / \"scripts/filter_krakenuniq.py\",\n        n_unique_kmers=config[\"n_unique_kmers\"],\n        n_tax_reads=config[\"n_tax_reads\"],\n    conda:\n        \"../envs/krakenuniq.yaml\"\n    envmodules:\n        *config[\"envmodules\"][\"krakenuniq\"],\n    benchmark:\n        \"benchmarks/FILTER_KRAKENUNIQ_OUTPUT/{sample}.benchmark.txt\"\n    message:\n        \"Filter_KrakenUniq_Output: APPLYING DEPTH AND BREADTH OF COVERAGE FILTERS TO KRAKENUNIQ OUTPUT FOR SAMPLE {input}\"\n    shell:\n        \"\"\"{params.exe} {input.krakenuniq} {params.n_unique_kmers} {params.n_tax_reads} {input.pathogenomesFound} &> {log}; \"\"\"\n        \"\"\"cut -f7 {output.pathogens} | tail -n +2 > {output.pathogen_tax_id}\"\"\"\n        \nBreadth and depth of coverage filters default thresholds are very conservative, can be tuned by users\nn_unique_kmers: 1000\nn_tax_reads: 200\n\ncd $OUTPUT; Rscript $PATH_TO_SCRIPTS/pipeline.R\ncut -f7 $OUTPUT/krakenuniq.output.pathogens | tail -n +2 > $OUTPUT/taxID.pathogens\ncat $OUTPUT/taxID.pathogens | parallel \"${PATH_TO_KRAKENUNIQ}/./krakenuniq-extract-reads {} $OUTPUT/sequences.krakenuniq ${SAMPLE} > $OUTPUT/{}.temp.fq\"\necho \"MeanReadLength\" > $OUTPUT/mean.reads.length; cd $OUTPUT\nfor i in $(cat taxID.pathogens); do awk '{if(NR%4==2) print length($1)}' ${i}.temp.fq | awk '{ sum += $0 } END { if (NR > 0) print sum / NR }' >> mean.reads.length; done; rm *.temp.fq\n\npaste krakenuniq.output.pathogens mean.reads.length > krakenuniq.output.pathogens_with_mean_read_length\ncat krakenuniq.output.pathogens_with_mean_read_length\n\n\n9.1.3 KrakenUniq to Krona\nWe then visualize our filtered output using Krona:\nrule KrakenUniq2Krona:\n    output:\n        tax_ids=\"results/KRAKENUNIQ/{sample}/krakenuniq.output.filtered_taxIDs_kmers1000.txt\",\n        seqs=\"results/KRAKENUNIQ/{sample}/sequences.krakenuniq_kmers1000.txt\",\n        krona=\"results/KRAKENUNIQ/{sample}/sequences.krakenuniq_kmers1000.krona\",\n        html=\"results/KRAKENUNIQ/{sample}/taxonomy.krona.html\",\n    input:\n        report=\"results/KRAKENUNIQ/{sample}/krakenuniq.output.filtered\",\n        seqs=\"results/KRAKENUNIQ/{sample}/sequences.krakenuniq\",\n    log:\n        \"logs/KRAKENUNIQ2KRONA/{sample}.log\",\n    conda:\n        \"../envs/krona.yaml\"\n    envmodules:\n        *config[\"envmodules\"][\"krona\"],\n    params:\n        exe=WORKFLOW_DIR / \"scripts/krakenuniq2krona.py\",\n        DB=f\"--tax {config['krona_db']}\" if \"krona_db\" in config else \"\",\n    benchmark:\n        \"benchmarks/KRAKENUNIQ2KRONA/{sample}.benchmark.txt\"\n    message:\n        \"KrakenUniq2Krona: VISUALIZING KRAKENUNIQ RESULTS WITH KRONA FOR SAMPLE {input.report}\"\n    shell:\n        \"{params.exe} {input.report} {input.seqs} &> {log}; \"\n        \"cat {output.seqs} | cut -f 2,3 > {output.krona}; \"\n        \"ktImportTaxonomy {output.krona} -o {output.html} {params.DB} &>> {log}\"\n\n\n9.1.4 AbundanceMatrix\nLastly, we create an Abundance matrix:\nrule KrakenUniq_AbundanceMatrix:\n    output:\n        out_dir=directory(\"results/KRAKENUNIQ_ABUNDANCE_MATRIX\"),\n        unique_species=\"results/KRAKENUNIQ_ABUNDANCE_MATRIX/unique_species_taxid_list.txt\",\n        unique_species_names=\"results/KRAKENUNIQ_ABUNDANCE_MATRIX/unique_species_names_list.txt\",\n        abundance_matrix=\"results/KRAKENUNIQ_ABUNDANCE_MATRIX/krakenuniq_abundance_matrix.txt\",\n        abundance_plot=\"results/KRAKENUNIQ_ABUNDANCE_MATRIX/krakenuniq_absolute_abundance_heatmap.pdf\",\n    input:\n        expand(\"results/KRAKENUNIQ/{sample}/krakenuniq.output.filtered\", sample=SAMPLES),\n    log:\n        \"logs/KRAKENUNIQ_ABUNDANCE_MATRIX/KRAKENUNIQ_ABUNDANCE_MATRIX.log\",\n    params:\n        exe=WORKFLOW_DIR / \"scripts/krakenuniq_abundance_matrix.R\",\n        exe_plot=WORKFLOW_DIR / \"scripts/plot_krakenuniq_abundance_matrix.R\",\n        n_unique_kmers=config[\"n_unique_kmers\"],\n        n_tax_reads=config[\"n_tax_reads\"],\n    conda:\n        \"../envs/r.yaml\"\n    envmodules:\n        *config[\"envmodules\"][\"r\"],\n    benchmark:\n        \"benchmarks/KRAKENUNIQ_ABUNDANCE_MATRIX/KRAKENUNIQ_ABUNDANCE_MATRIX.benchmark.txt\"\n    message:\n        \"KrakenUniq_AbundanceMatrix: COMPUTING KRAKENUNIQ MICROBIAL ABUNDANCE MATRIX\"\n    shell:\n        \"Rscript {params.exe} results/KRAKENUNIQ {output.out_dir} {params.n_unique_kmers} {params.n_tax_reads} &> {log};\"\n        \"Rscript {params.exe_plot} {output.out_dir} {output.out_dir} &> {log}\"\nThis rule combines krakenuniq abundance output files into one file results/KRAKENUNIQ_ABUNDANCE_MATRIX/krakenuniq_abundance_matrix.txt.\nThen, it creates a heatmap plot using the abundance data: results/KRAKENUNIQ_ABUNDANCE_MATRIX/krakenuniq_absolute_abundance_heatmap.pdf\nPlease run this script using this piece of code:\nsbatch KrakenUniq_AbundanceMatrix.sh --account=egitim"
  },
  {
    "objectID": "align.html#bowtie2",
    "href": "align.html#bowtie2",
    "title": "10  Fast alignment with Bowtie2",
    "section": "10.1 Bowtie2",
    "text": "10.1 Bowtie2\nThe pipeline has a side branch for rapid analysis of the results based on an alignment with Bowtie2 and a post-mortem damage estimation with MapDamage. For this project, we won’t be able to use the real Bowtie2 databases, because there were too heavy for the resources allocated for this workshop. So we are only going to look at the code but not run it and the output will be provided for you in the folder containing the expected results."
  },
  {
    "objectID": "align.html#bowtie2-build-l-to-build-the-index-files",
    "href": "align.html#bowtie2-build-l-to-build-the-index-files",
    "title": "10  Fast alignment with Bowtie2",
    "section": "10.2 Bowtie2-build-l to build the index files",
    "text": "10.2 Bowtie2-build-l to build the index files\nIn order to run a Bowtie2 alignment, one needs a complete Bowtie2 database, in other words a .fna (fasta) file that has been indexed using the command bowtie2-build-l. This is the first part of the pipeline for the alignment step. You can therefore provide your own merged fna file for Bowtie2 to index if you wish to. However, if you are using one of our own custom databases, this step will be automatically ignored because its output already exists.\nYou can find the Bowtie2_build rule generating the index files below. It takes as input the path to the library.fna file that you have provided for the variable bowtie2_patho_db in your config.yaml file. For more information, refer to the main aMeta GitHub in the section about the config file. The rule then generates the index files of the Bowtie2 database if they don’t already exist.\nrule Bowtie2_Index:\n    output:\n        expand(\n            f\"{config['bowtie2_patho_db']}{{ext}}\",\n            ext=[\n                \".1.bt2l\",\n                \".2.bt2l\",\n                \".3.bt2l\",\n                \".4.bt2l\",\n                \".rev.1.bt2l\",\n                \".rev.2.bt2l\",\n            ],\n        ),\n    input:\n        ref=ancient(config[\"bowtie2_patho_db\"]),\n    log:\n        f\"{config['bowtie2_patho_db']}_BOWTIE2_BUILD.log\",\n    shell:\n        \"\"bowtie2-build-l --threads {threads} {input.ref} {input.ref} > {log} 2>&1\nHere is a simplified version of this code:\nbowtie2-build-l --threads 1 resources/library.fna resources/library.fna > logs/Bowtie2_Build.log 2>&1\nWARNING: No need to execute that line of code as we haven’t downloaded the real databases to the server for this workshop and it would overwrite the index files! The trick for this rule is that the input file is declared as ancient with the “ancient” function of snakemake, making snakemake not rerun the rule if the output files already exists. But if you only run the bash command provided, you will overwrite the files.\n\n10.2.1 Regarding real life projects with the actual databases\nFor real life projects, these are the Bowtie2 databases we have made available for download:\n\nPathogenome: Bowtie2 index and helping files for following up on microbial pathogens\nBowtie2_Full_NT: Bowtie2 index for full NCBI NT (for quick follow up of prokaryotes and eukaryotes; also contains helping files for building the Malt database)\n\nFor more information and links to download the databases, please refer to the official GitHub of aMeta.\nWARNING: if you are using the Bowtie2_Full_NT database, make sure that you have unzipped the files before running the pipeline, otherwise it will automatically overwrite the index files to regenerate them and will need a huge amount of memory and time to succeed, which you probably don’t want to happen ;-)"
  },
  {
    "objectID": "align.html#bowtie2-alignment",
    "href": "align.html#bowtie2-alignment",
    "title": "10  Fast alignment with Bowtie2",
    "section": "10.3 Bowtie2 alignment",
    "text": "10.3 Bowtie2 alignment\nrule Bowtie2_Pathogenome_Alignment:\n    output:\n        bam=\"results/BOWTIE2/{sample}/AlignedToPathogenome.bam\",\n        bai=\"results/BOWTIE2/{sample}/AlignedToPathogenome.bam.bai\",\n    input:\n        fastq=\"results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz\",\n        db=rules.Bowtie2_Index.output,\n    params:\n        PATHO_DB=lambda wildcards, input: config[\"bowtie2_patho_db\"],\n    log:\n        \"logs/BOWTIE2/{sample}.log\",\n    shell:\n        \"\"\"bowtie2 --large-index -x {params.PATHO_DB} --end-to-end --threads 10 --very-sensitive -U {input.fastq} 2> {log} | samtools view -bS -q 1 -h -@ 10 - | samtools sort -@ 10 -o {output.bam} >> {log};\"\"\"\n        \"\"\"samtools index {output.bam}\"\"\"\nHere is a simplified version of this code:\nbowtie2 --large-index -x resources/library.fna --end-to-end --threads 10 --very-sensitive -U results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz 2> logs/BOWTIE2/{sample}.log | samtools view -bS -q 1 -h -@ 10 - | samtools sort - -@ 10 -o results/BOWTIE2/{sample}/AlignedToPathogenome.bam >> logs/BOWTIE2/{sample}.log\nsamtools index results/BOWTIE2/{sample}/AlignedToPathogenome.bam\nAgain, no need to run that line of code. You can find how the output is supposed to look like in the folder with expected results. Basically, this line asks Bowtie2 to align each fastq file to the Bowtie2 database. This will generate a sam file containing the information of the alignment for each DNA sequence and the reference genome to which it aligns to. The sam file is then directly changed into a bam file using samtools, sorted and later indexed so that it is ready to be used. In order to run this line, you would still need to replace {sample} with the name of each sample that you have provided in the samples.tsv file and run this line for each of them."
  },
  {
    "objectID": "damage.html",
    "href": "damage.html",
    "title": "aMeta Workshop",
    "section": "",
    "text": "We run MapDamage:\nrule MapDamage:\n    \"\"\"Run Mapdamage on extracted pathogens\"\"\"\n    output:\n        dir=directory(\"results/MAPDAMAGE/{sample}\"),\n    input:\n        pathogen_tax_id=\"results/KRAKENUNIQ/{sample}/taxID.pathogens\",\n        bam=\"results/BOWTIE2/{sample}/AlignedToPathogenome.bam\",\n        bai=\"results/BOWTIE2/{sample}/AlignedToPathogenome.bam.bai\",\n    params:\n        pathogenome_path=os.path.dirname(config[\"pathogenomesFound\"]),\n        PATHO_DB=config[\"bowtie2_patho_db\"],\n        options=config[\"options\"].get(\"MapDamage\", \"\"),\n    threads: 10\n    message:\n        \"MapDamage: RUNNING MAPDAMAGE ON PATHOGENS IDENTIFIED IN SAMPLE {input.bam}\"\n    shell:\n        \"mkdir {output.dir}; \"\n        \"if [ -s {input.pathogen_tax_id} ]; then \"\n        'cat {input.pathogen_tax_id} | parallel -j {threads} \"grep -w {{}} {params.pathogenome_path}/seqid2taxid.pathogen.map | cut -f1 > {output.dir}/{{}}.seq_ids\" ; '\n        \"for i in $(cat {input.pathogen_tax_id}); do xargs --arg-file={output.dir}/${{i}}.seq_ids samtools view -bh {input.bam} --write-index -@ {threads} -o {output.dir}/${{i}}.tax.bam; done >> {log} 2>&1; \"\n        \"find {output.dir} -name '*.tax.bam' | parallel -j {threads} \\\"mapDamage {params.options} -i {{}} -r {params.PATHO_DB} --merge-reference-sequences -d {output.dir}/mapDamage_{{}}\\\" >> {log} 2>&1 || true; \"\n        \"for filename in {output.dir}/*.tax.bam; do newname=`echo $filename | sed 's/tax\\.//g'`; mv $filename $newname; done >> {log} 2>&1; \"\n        \"mv {output.dir}/mapDamage_{output.dir}/* {output.dir} >> {log} 2>&1; \"\n        \"rm -r {output.dir}/mapDamage_results >> {log} 2>&1; \"\n        \"else echo NO MICROBES TO AUTHENTICATE > {output.dir}/README.txt; fi\"\nHere is a simplified version of the code:\ncat {input.pathogen_tax_id} | parallel -j {threads} \"grep -w {{}} {params.pathogenome_path}/seqid2taxid.pathogen.map | cut -f1 > {output.dir}/{{}}.seq_ids\" ; '\n        \"for i in $(cat {input.pathogen_tax_id}); do xargs --arg-file={output.dir}/${{i}}.seq_ids samtools view -bh {input.bam} --write-index -@ {threads} -o {output.dir}/${{i}}.tax.bam; done >> {log} 2>&1; \"\n        \"find {output.dir} -name '*.tax.bam' | parallel -j {threads} \\\"mapDamage {params.options} -i {{}} -r {params.PATHO_DB} --merge-reference-sequences -d {output.dir}/mapDamage_{{}}\\\" >> {log} 2>&1 || true; \"\n        \"for filename in {output.dir}/*.tax.bam; do newname=`echo $filename | sed 's/tax\\.//g'`; mv $filename $newname; done >> {log} 2>&1; \"\n        \"mv {output.dir}/mapDamage_{output.dir}/* {output.dir} >> {log} 2>&1; \"\n        \"rm -r {output.dir}/mapDamage_results >> {log} 2>&1; \""
  },
  {
    "objectID": "malt.html#build-a-custom-malt-database-based-on-the-previous-results-from-the-filtering-of-krakenuniq",
    "href": "malt.html#build-a-custom-malt-database-based-on-the-previous-results-from-the-filtering-of-krakenuniq",
    "title": "12  MALT alignment and processing",
    "section": "12.1 Build a custom MALT database based on the previous results from the filtering of KrakenUniq",
    "text": "12.1 Build a custom MALT database based on the previous results from the filtering of KrakenUniq\nFirst we need to build the custom database with the rule Build_Malt_DB. This is done with the use of a python script. Inside the script, python will use the input and output variables provided by snakemake.\nWARNING: We won’t be able to run this command during the workshop because the files seqid2taxid.map.orig, library.fna and nucl_gb.accession2taxid were too large for our workshop folder.\nrule Build_Malt_DB:\n    output:\n        seqid2taxid_project=\"results/MALT_DB/seqid2taxid.project.map\",\n        seqids_project=\"results/MALT_DB/seqids.project\",\n        project_headers=\"results/MALT_DB/project.headers\",\n        project_fasta=\"results/MALT_DB/library.project.fna\",\n        db=directory(\"results/MALT_DB/maltDB.dat\"),\n    input:\n        unique_taxids=ancient(\"results/KRAKENUNIQ_ABUNDANCE_MATRIX/unique_species_taxid_list.txt\"),\n    params:\n        seqid2taxid=config[\"malt_seqid2taxid_db\"],\n        nt_fasta=config[\"malt_nt_fasta\"],\n        accession2taxid=config[\"malt_accession2taxid\"],\n    threads: 20\n    log:\n        \"logs/BUILD_MALT_DB/BUILD_MALT_DB.log\",\n    conda:\n        \"../envs/malt.yaml\"\n    envmodules:\n        *config[\"envmodules\"][\"malt\"],\n    benchmark:\n        \"benchmarks/BUILD_MALT_DB/BUILD_MALT_DB.benchmark.txt\"\n    message:\n        \"Build_Malt_DB: BUILDING MALT DATABASE USING SPECIES DETECTED BY KRAKENUNIQ\"\n    script:\n        \"../scripts/malt-build.py\"\nHere is a simplified version of this code:\npython ../scripts/malt-build.py \\\n    --seqid2taxid seqid2taxid.map.orig \\\n    --nt_fasta library.fna \\\n    --accession2taxid nucl_gb.accession2taxid \\\n    --unique_taxids results/KRAKENUNIQ_ABUNDANCE_MATRIX/unique_species_taxid_list.txt \\\n    --output_dir results/MALT_DB/ \\\n    --threads 16 &> logs/BUILD_MALT_DB/BUILD_MALT_DB.log\nIn summary, this command will build a custom malt database containing all the species detected by KrakenUniq that have passed are threshold. You can find the full list of species in results/KRAKENUNIQ_ABUNDANCE_MATRIX/unique_species_taxid_list.txt."
  },
  {
    "objectID": "malt.html#align-the-fastq-files-using-malt",
    "href": "malt.html#align-the-fastq-files-using-malt",
    "title": "12  MALT alignment and processing",
    "section": "12.2 Align the FastQ files using MALT",
    "text": "12.2 Align the FastQ files using MALT\nrule Malt:\n    output:\n        rma6=\"results/MALT/{sample}.trimmed.rma6\",\n        sam=\"results/MALT/{sample}.trimmed.sam.gz\",\n    input:\n        fastq=ancient(\"results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz\"),\n        db=ancient(\"results/MALT_DB/maltDB.dat\"),\n    params:\n        gunzipped_sam=\"results/MALT/{sample}.trimmed.sam\",\n    threads: 20\n    log:\n        \"logs/MALT/{sample}.log\",\n    conda:\n        \"../envs/malt.yaml\"\n    envmodules:\n        *config[\"envmodules\"][\"malt\"],\n    benchmark:\n        \"benchmarks/MALT/{sample}.benchmark.txt\"\n    message:\n        \"Malt: RUNNING MALT ALIGNMENTS FOR SAMPLE {input.fastq}\"\n    shell:\n        \"unset DISPLAY; malt-run -at SemiGlobal -m BlastN -i {input.fastq} -o {output.rma6} -a {params.gunzipped_sam} -t {threads} -d {input.db} &> {log}\"\nHere is a simplified version of this code:\nfor sample in $(ls results/CUTADAPT_ADAPTER_TRIMMING/*.fastq.gz); do\n        unset DISPLAY\n        malt-run -at SemiGlobal -m BlastN -i results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz \\\n                -o results/MALT/{sample}.trimmed.rma6 \\\n                -a results/MALT/{sample}.trimmed.sam \\\n                -t 20 -d results/MALT_DB/maltDB.dat \\\n                &> logs/MALT/{sample}.log\n        gzip results/MALT/{sample}.trimmed.sam\ndone\nIn summary, we loop through the FastQ files and run an alignment with MALT and gzip the generated sam file. The “unset DISPLAY” command makes it possible to run MALT without it trying to open a graphical interface which can generate an error."
  },
  {
    "objectID": "malt.html#quantify-the-microbial-abundance-using-the-sam-file-generated-by-the-malt-alignment",
    "href": "malt.html#quantify-the-microbial-abundance-using-the-sam-file-generated-by-the-malt-alignment",
    "title": "12  MALT alignment and processing",
    "section": "12.3 Quantify the microbial abundance using the sam file generated by the MALT alignment",
    "text": "12.3 Quantify the microbial abundance using the sam file generated by the MALT alignment\nrule Malt_QuantifyAbundance:\n    output:\n        out_dir=directory(\"results/MALT_QUANTIFY_ABUNDANCE/{sample}\"),\n        counts=\"results/MALT_QUANTIFY_ABUNDANCE/{sample}/sam_counts.txt\",\n    input:\n        sam=ancient(\"results/MALT/{sample}.trimmed.sam.gz\"),\n    params:\n        unique_taxids=\"results/KRAKENUNIQ_ABUNDANCE_MATRIX/unique_species_taxid_list.txt\",\n        exe=WORKFLOW_DIR / \"scripts/malt_quantify_abundance.py\",\n    log:\n        \"logs/MALT_QUANTIFY_ABUNDANCE/{sample}.log\",\n    benchmark:\n        \"benchmarks/MALT_QUANTIFY_ABUNDANCE/{sample}.benchmark.txt\"\n    message:\n        \"Malt_QuantifyAbundance: QUANTIFYING MICROBIAL ABUNDANCE USING MALT SAM-ALIGNMENTS FOR SAMPLE {input.sam}\"\n    shell:\n        \"mkdir -p {output.out_dir}; \"\n        \"{params.exe} {input.sam} {params.unique_taxids} > {output.counts} 2> {log}\"\nHere is a simplified version of this code:\nmkdir -p results/MALT_QUANTIFY_ABUNDANCE logs/MALT_QUANTIFY_ABUNDANCE\n\nfor sample in $(ls results/CUTADAPT_ADAPTER_TRIMMING/*.fastq.gz); do\n        python scripts/malt_quantify_abundance.py results/MALT/{sample}.trimmed.sam.gz results/KRAKENUNIQ_ABUNDANCE_MATRIX/unique_species_taxid_list.txt > results/MALT_QUANTIFY_ABUNDANCE/{sample}/sam_counts.txt 2> logs/MALT_QUANTIFY_ABUNDANCE/{sample}.log\ndone\nIn summary, this python script takes the results of the MALT alignment and counts the amount of reads per species."
  },
  {
    "objectID": "malt.html#compute-the-malt-abundance-matrix-using-the-sam-files-generated-by-malt",
    "href": "malt.html#compute-the-malt-abundance-matrix-using-the-sam-files-generated-by-malt",
    "title": "12  MALT alignment and processing",
    "section": "12.4 Compute the MALT abundance matrix using the SAM files generated by MALT",
    "text": "12.4 Compute the MALT abundance matrix using the SAM files generated by MALT\nrule Malt_AbundanceMatrix_Sam:\n    output:\n        out_dir=directory(\"results/MALT_ABUNDANCE_MATRIX_SAM\"),\n        abundance_matrix=\"results/MALT_ABUNDANCE_MATRIX_SAM/malt_abundance_matrix_sam.txt\",\n    input:\n        sam_counts=expand(\n            \"results/MALT_QUANTIFY_ABUNDANCE/{sample}/sam_counts.txt\", sample=SAMPLES\n        ),\n    log:\n        \"logs/MALT_ABUNDANCE_MATRIX_SAM/MALT_ABUNDANCE_MATRIX_SAM.log\",\n    params:\n        exe=WORKFLOW_DIR / \"scripts/malt_abundance_matrix.R\",\n    conda:\n        \"../envs/r.yaml\"\n    envmodules:\n        *config[\"envmodules\"][\"r\"],\n    benchmark:\n        \"benchmarks/MALT_ABUNDANCE_MATRIX_SAM/MALT_ABUNDANCE_MATRIX_SAM.benchmark.txt\"\n    message:\n        \"Malt_AbundanceMatrix_Sam: COMPUTING MALT MICROBIAL ABUNDANCE MATRIX FROM SAM-FILES\"\n    shell:\n        \"Rscript {params.exe} results/MALT_QUANTIFY_ABUNDANCE {output.out_dir} &> {log}\"\nHere is a simplified version of this code:\nmkdir -p results/MALT_ABUNDANCE_MATRIX_SAM logs/MALT_ABUNDANCE_MATRIX_SAM\n\nfor sample in $(ls results/CUTADAPT_ADAPTER_TRIMMING/*.fastq.gz); do\n        Rscript scripts/malt_abundance_matrix.R results/MALT_QUANTIFY_ABUNDANCE results/MALT_ABUNDANCE_MATRIX_SAM/ > logs/MALT_ABUNDANCE_MATRIX_SAM/MALT_ABUNDANCE_MATRIX_SAM.log 2>&1\ndone\nIn summary, this R scripts takes the abundance of all samples and creates a matrix."
  },
  {
    "objectID": "malt.html#compute-the-malt-abundance-matrix-using-the-rma6-files-generated-by-malt",
    "href": "malt.html#compute-the-malt-abundance-matrix-using-the-rma6-files-generated-by-malt",
    "title": "12  MALT alignment and processing",
    "section": "12.5 Compute the MALT abundance matrix using the rma6 files generated by MALT",
    "text": "12.5 Compute the MALT abundance matrix using the rma6 files generated by MALT\nrule Malt_AbundanceMatrix_Rma6:\n    output:\n        out_dir=directory(\"results/MALT_ABUNDANCE_MATRIX_RMA6\"),\n        abundance_matrix=\"results/MALT_ABUNDANCE_MATRIX_RMA6/malt_abundance_matrix_rma6.txt\",\n    input:\n        rma6=expand(\"results/MALT/{sample}.trimmed.rma6\", sample=SAMPLES),\n    params:\n        exe=WORKFLOW_DIR / \"scripts/rma-tabuliser\",\n    log:\n        \"logs/MALT_ABUNDANCE_MATRIX_RMA6/MALT_ABUNDANCE_MATRIX_RMA6.log\",\n    envmodules:\n        *config[\"envmodules\"][\"malt\"],\n    conda:\n        \"../envs/malt.yaml\"\n    benchmark:\n        \"benchmarks/MALT_ABUNDANCE_MATRIX_RMA6/MALT_ABUNDANCE_MATRIX_RMA6.benchmark.txt\"\n    message:\n        \"Malt_AbundanceMatrix_Rma6: COMPUTING MALT MICROBIAL ABUNDANCE MATRIX FROM RMA6-FILES\"\n    shell:\n        \"{params.exe} -d $(dirname {input.rma6}) -r 'S' &> {log}; \"\n        \"mv results/MALT/count_table.tsv {output.out_dir}; \"\n        \"mv {output.out_dir}/count_table.tsv {output.abundance_matrix}\"\nHere is a simplified version of this code:\nrma-tabuliser -d results/MALT/{sample}.trimmed.rma6 -r 'S' &> logs/MALT_ABUNDANCE_MATRIX_RMA6/MALT_ABUNDANCE_MATRIX_RMA6.log;\nmv results/MALT/count_table.tsv results/MALT_ABUNDANCE_MATRIX_RMA6/;\nmv results/MALT_ABUNDANCE_MATRIX_RMA6/count_table.tsv results/MALT_ABUNDANCE_MATRIX_RMA6/malt_abundance_matrix_rma6.txt"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "aMeta Workshop",
    "section": "",
    "text": "To plot the authentication score and summarise the output of the workflow, we use the following code:\nrule Plot_Authentication_Score:\n\"\"\"Plot authentication score\"\"\"\n    output:\n        heatmap=\"results/overview_heatmap_scores.pdf\",    \n    input:\n        scores=expand(\"results/AUTHENTICATION/.{sample}_done\",sample=SAMPLES)\n    message:\n        \"Plot_Authentication_Score: PLOTTING HEATMAP OF AUTHENTICATION SCORES\"\n    params:\n        exe=WORKFLOW_DIR / \"scripts/plot_score.R\",\n    shell:\n        \"Rscript {params.exe} results/AUTHENTICATION $(dirname {output.heatmap}) &> {log}\"\nHere is a simplified version of this code:\n\"Rscript {params.exe} results/AUTHENTICATION $(dirname {output.heatmap})\""
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Pochon, Zoé, Nora Bergfeldt, Emrah Kırdök, Mário Vicente, Thijessen\nNaidoo, Tom van der Valk, N. Ezgi Altınışık, et al. 2022. “aMeta:\nAn Accurate and Memory-Efficient Ancient Metagenomic Profiling\nWorkflow.” bioRxiv. https://doi.org/10.1101/2022.10.03.510579."
  }
]