[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "aMeta Workshop",
    "section": "",
    "text": "Welcome to the aMeta: an accurate and memory-efficient ancient Metagenomic profiling workflow course that will be held in Mersin University, Department of Biotechnology in the scope of the European Union funded NEOMATRIX collaboration project.\nIn this web page, we aim to publish the tutorial material of aMeta ancient metagenomic profiling workflow.\naMeta (Pochon et al. 2022) is designed to analyse the microbial content in ancient specimens from a wide variety of source organisms (including humans).\nPlease find the whole pipeline through this github repo\nextending\n\n\n\n\nPochon, Zoé, Nora Bergfeldt, Emrah Kırdök, Mário Vicente, Thijessen Naidoo, Tom van der Valk, N. Ezgi Altınışık, et al. 2022. “aMeta: An Accurate and Memory-Efficient Ancient Metagenomic Profiling Workflow.” bioRxiv. https://doi.org/10.1101/2022.10.03.510579."
  },
  {
    "objectID": "organization.html",
    "href": "organization.html",
    "title": "Workhsop Organization",
    "section": "",
    "text": "On this page, we would like to give information on the details of the workshop:\n\nWorkshop information\nThe team\nComputing details\n\nPlease check this page frequently, there will be new updates."
  },
  {
    "objectID": "info-mersin.html",
    "href": "info-mersin.html",
    "title": "2  Workshop Organization",
    "section": "",
    "text": "Please find the accomodation and flight information through this Google Document.\nPlease find the food selection information in this Google Document. Please specify your selection if you have not selected yet."
  },
  {
    "objectID": "about.html#about-this-site",
    "href": "about.html#about-this-site",
    "title": "3  About",
    "section": "3.1 About this site",
    "text": "3.1 About this site\nThis site is built for the aMeta workshop in Mersin Feb 6 – 10 2023."
  },
  {
    "objectID": "about.html#the-team",
    "href": "about.html#the-team",
    "title": "3  About",
    "section": "3.2 The team",
    "text": "3.2 The team\n\n3.2.1 Emrah Kırdök\nEmrah Kırdök is an assistant professor at Mersin University.\nEmrah’s research focuses on understanding human history through ancient DNA (aDNA). Generally, a little proportion of aDNA sequences could be mapped to its host reference genome. The remaining fragments come either from the microbiome of the ancient host or the environment that encapsulates the host.\nBy analyzing the remaining aDNA sequences with the state-of-the-art metagenomic methods it’s possible to (i) construct the microbiome composition of ancient humans, (ii) reconstruct the ancient pathogenic genomes, (iii) understand the evolutionary mechanisms of the ancient pathogens.\n\n\n3.2.2 Zoé Pochon\nZoé started her PhD in Archaeology at the Centre for Palaeogenetics in Stockholm in 2021. Zoé has training in biology and history and is interested in using archaeogenetics to bring new clues to debates on historical and biological questions. Her current work focuses on using bioinformatics tools to analyse the DNA and RNA of ancient viruses, as well as other microbes to get a better picture of the common pathogens at different time periods in history and to help understand past epidemics.\nSome of her research projects have included the DNA of combatants from a Bronze Age battlefield to infer their origin and estimate the selection coefficient of the lactase persistence, an allelic mutation already present in some of them.\n\n\n3.2.3 Nora Bergfeldt\nNora started her PhD in Zoology at the Centre for Palaeogenetics in 2019. She has a background in ecology, biodiversity and genomics. Her research focuses on using metagenomic tools to study bacteria and other microbes in ancient humans, with a particular interest in studying the evolution of pathogens."
  },
  {
    "objectID": "cluster-connection.html#how-we-will-work",
    "href": "cluster-connection.html#how-we-will-work",
    "title": "4  Cluster Details",
    "section": "4.1 How we will work?",
    "text": "4.1 How we will work?\nIn this workshop, we will use our computers, and we will connect to high performance computing systems. Therefore, you will need to bring your laptops with you.\nYou should have been comfortable using the Unix/Linux command line to take the course. We know that there will be several operating systems. So please check these recomendations based on your operating system.\n\n4.1.1 Linux systems\nIf you have a Linux system (e.g. Ubuntu), you do not need to worry a lot. Most of the command line tools (like ssh) will be already installed on your system\n\n\n4.1.2 Mac systems\nMac systems also have a default support on the command line. The terminal application on your Mac system would be enough. But we advise you to install iTerm2 as a terminal replacement for your Mac.\n\n\n4.1.3 Windows systems\nIf you have a Windows system, the easiest choice to connect a remote server is Mobaxterm tool."
  },
  {
    "objectID": "cluster-connection.html#cluster-systems",
    "href": "cluster-connection.html#cluster-systems",
    "title": "4  Cluster Details",
    "section": "4.2 Cluster systems",
    "text": "4.2 Cluster systems\nIn this workshop, we have two different cluster systems at our disposal. The first system TRUBA (Turkish National Scientific Cluster) that is funded by the TUBİTAK (The Scientific and Technological Research Council of Türkiye). The second system is located at Middle East Technical University.\n\n4.2.1 TRUBA Cluster system\nThis cluster system accepts connections from University networks. Therefore, you should be in a University network in order to access TRUBA computers. We will provide a username and a password for your connection.\nThen, you can connect to this system like this:\nssh egitim@levrek1.ulakbim.gov.tr\nThere is a trick to connect TRUBA system outside of the university network. We will describe it in the document.\n\n\n4.2.2 Middle East Technical University computing systems\nThis computer system is located at Middle East Technical University. It consists of two clusters: baggins and NEOGENE. We will work mostly on baggings to explain tools. At the end, we plan to send snakemake jobs on the NEOGENE system.\nHowever, NEOGENE system does not accept connections outside from university networks, but baggins can. So we will first connect to baggins via ssh first. Then we can connect to NEOGENE from this system.\nYour username and password will be provided on the first day of the workshop.\nConnect to baggins server with your username like this:\nssh -p 2275 username@144.122.38.49\nAfterwards, you can connect to NEOGENE like this:\nssh username@144.122.38.50\n\n\n4.2.3 Connecting to TRUBA system outside of the universiy network\nWe can connect to TRUBA system with a small trick. First we will connect to Baggins cluster, so we will be on the Middle East Technical Univerisy network. Then we can connect to TRUBA.\n\nssh -p 2275 username@144.122.38.49\n\nssh egitim@levrek1.ulakbim.gov.tr"
  },
  {
    "objectID": "cluster-connection.html#configuration",
    "href": "cluster-connection.html#configuration",
    "title": "4  Cluster Details",
    "section": "4.3 Configuration",
    "text": "4.3 Configuration\nAll the tools are installed on these server systems. Before using the tools, we need to set the PATH variable and show the system where these tools are installed.\n\n4.3.1 Configuration of the TRUBA system\nTo use all the tools for aMeta, please set your PATH variable like this:\nexport PATH=${PATH}:/truba/home/egitim/miniconda3/envs/aMeta/bin/\n\n\n4.3.2 Configuration on the Ankara computers\nTo use all the tools for aMeta, please set your PATH variable like this:\nexport PATH=${PATH}:/usr/local/sw/anaconda3/envs/aMeta/"
  },
  {
    "objectID": "course-style.html#using-conda-package-manager",
    "href": "course-style.html#using-conda-package-manager",
    "title": "5  How to follow the Course?",
    "section": "5.1 Using conda package manager",
    "text": "5.1 Using conda package manager\nEach rule can contain a conda environment. So, if you will run snakemake with --use-conda flag, the specified environment will be installed given you have the conda package manager already installed:\n    conda:\n        \"../envs/fastqc.yaml\""
  },
  {
    "objectID": "course-style.html#using-environment-modules",
    "href": "course-style.html#using-environment-modules",
    "title": "5  How to follow the Course?",
    "section": "5.2 Using environment modules",
    "text": "5.2 Using environment modules\nIf you want to use the environmental modules, then you need to use envmodules flag with the snakemake. In this case, you would like to modify the envirınmental module file.\n envmodules:\n        *config[\"envmodules\"][\"fastqc\"]"
  },
  {
    "objectID": "course-style.html#benchmarks",
    "href": "course-style.html#benchmarks",
    "title": "5  How to follow the Course?",
    "section": "5.3 Benchmarks",
    "text": "5.3 Benchmarks\nYou could find benchmarking stats with under the benchmarking folder.\n    benchmark:\n        \"benchmarks/FASTQC_BEFORE_TRIMMING/{sample}.benchmark.txt\""
  },
  {
    "objectID": "course-style.html#logs",
    "href": "course-style.html#logs",
    "title": "5  How to follow the Course?",
    "section": "5.4 Logs",
    "text": "5.4 Logs\nIf you have problems with your snakemake run, each rule will have a log file. So you can check the errors:\n    log:\n        \"logs/FASTQC_BEFORE_TRIMMING/{sample}.log\","
  },
  {
    "objectID": "rulegraph.html#ameta-workflow-graph",
    "href": "rulegraph.html#ameta-workflow-graph",
    "title": "6  aMeta workflow graph",
    "section": "6.1 aMeta workflow graph",
    "text": "6.1 aMeta workflow graph\nHere is an overview of all the rules of the aMeta snakemake pipeline. Such an overview of a workflow in the form of a graph is called a dag or rulegraph. You can consider these rules as steps of the workflow going from up to down.\n\n\n\n\n\nRulegraph"
  },
  {
    "objectID": "qc.html#quality-control-before-trimming",
    "href": "qc.html#quality-control-before-trimming",
    "title": "7  Quality control and trimming",
    "section": "7.1 Quality control before trimming",
    "text": "7.1 Quality control before trimming\nThe pipeline starts with a quality control step. We will first run FastQC tool to check the quality of the fastq files.\nThis is the rule in the workflow/rules/qc.smk file. Input files are retrieved from the sample sheet that you provided. And you will have two output files html and zip files. The html file contains the final report.\nrule FastQC_BeforeTrimming:\n    \"\"\"Run fastq before trimming\"\"\"\n    output:\n        html=\"results/FASTQC_BEFORE_TRIMMING/{sample}_fastqc.html\",\n        zip=\"results/FASTQC_BEFORE_TRIMMING/{sample}_fastqc.zip\",\n    input:\n        fastq=lambda wildcards: samples.loc[wildcards.sample].fastq,\n    message:\n        \"FastQC_BeforeTrimming: RUNNING QUALITY CONTROL WITH FASTQC FOR SAMPLE {input.fastq} BEFORE TRIMMING ADAPTERS\"\n    threads: 2\n    shell:\n        \"fastqc {input.fastq} --threads {threads} --nogroup --outdir results/FASTQC_BEFORE_TRIMMING &> {log}\"\n\nHere is a simplified version of this code:\nfastqc FASTQ \\\n    --threads 2 \\\n    --nogroup \\\n    --outdir results/FASTQC_BEFORE_TRIMMING/"
  },
  {
    "objectID": "qc.html#quality-trimming-step",
    "href": "qc.html#quality-trimming-step",
    "title": "7  Quality control and trimming",
    "section": "7.2 Quality trimming step",
    "text": "7.2 Quality trimming step\nAfter we run quality controls. We will run cutadapt tool to remove the leftover adapters and low quality bases. Here is the overview of this rule.\nYou can add or remove adapters using the config/config.yaml file.\nIn this file the inputs will be retrieved again from the samplesheet file. And the outputs will be placed to the results/CUTADAPT_ADAPTER_TRIMMING/ folder.\nrule Cutadapt_Adapter_Trimming:\n    output:\n        fastq=\"results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz\",\n    input:\n        fastq=lambda wildcards: samples.loc[wildcards.sample].fastq,\n    params:\n        adapters=\" \".join([f\"-a {x}\" for x in ADAPTERS]),\n    message:\n        \"Cutadapt_Adapter_Trimming: TRIMMING ADAPTERS FOR SAMPLE {input.fastq} WITH CUTADAPT\"\n    threads: 1\n    shell:\n        \"cutadapt {params.adapters} --minimum-length 30 -o {output.fastq} {input.fastq} &> {log}\"\n\nHere is how we will run it for one file:\ncutadapt -a ADAPTERS --minimum-length 30 -o results/CUTADAPT_ADAPTER_TRIMMING/sample"
  },
  {
    "objectID": "qc.html#quality-control-after-trimming",
    "href": "qc.html#quality-control-after-trimming",
    "title": "7  Quality control and trimming",
    "section": "7.3 Quality control after trimming",
    "text": "7.3 Quality control after trimming\nThen we will run the quality control after the trimming step:\nrule FastQC_AfterTrimming:\n    output:\n        html=\"results/FASTQC_AFTER_TRIMMING/{sample}.trimmed_fastqc.html\",\n        zip=\"results/FASTQC_AFTER_TRIMMING/{sample}.trimmed_fastqc.zip\",\n    input:\n        fastq=\"results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz\",\n    threads: 2\n    message:\n        \"FastQC_AfterTrimming: RUNNING QUALITY CONTROL WITH FASTQC FOR SAMPLE {input.fastq} AFTER TRIMMING ADAPTERS\"\n    shell:\n        \"fastqc {input.fastq} --threads {threads} --nogroup --outdir results/FASTQC_AFTER_TRIMMING &> {log}\"\nHere is how we will run it for one file:\nfastqc results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz --threads 2 --nogroup --outdir results/FASTQC_AFTER_TRIMMING"
  },
  {
    "objectID": "qc.html#combine-quality-control-outputs",
    "href": "qc.html#combine-quality-control-outputs",
    "title": "7  Quality control and trimming",
    "section": "7.4 Combine quality control outputs",
    "text": "7.4 Combine quality control outputs\nAt last we will combine all the fastqc reports into one file using Multifastqc tool:\nrule MultiQC:\n    \"\"\"Run MultiQC\"\"\"\n    output:\n        html=\"results/MULTIQC/multiqc_report.html\",\n    input:\n        unpack(multiqc_input),\n    params:\n        config=os.path.join(WORKFLOW_DIR, \"envs\", \"multiqc_config.yaml\"),\n    envmodules:\n        *config[\"envmodules\"][\"multiqc\"],\n    benchmark:\n        \"benchmarks/MULTIQC/MULTIQC.benchmark.txt\"\n    message:\n        \"MultiQC: COMBINING QUALITY CONTROL METRICS WITH MULTIQC\"\n    shell:\n        'echo {input} | tr \" \" \"\\n\" > {output.html}.fof;'\n        \"multiqc -c {params.config} -l {output.html}.fof --verbose --force --outdir results/MULTIQC &> {log}\""
  },
  {
    "objectID": "krakenuniq.html#run-krakenuniq",
    "href": "krakenuniq.html#run-krakenuniq",
    "title": "8  KrakenUniq database",
    "section": "8.1 Run KrakenUniq",
    "text": "8.1 Run KrakenUniq\nThis is the rule in the workflow/rules/krakenuniq.smk file. Input files are retrieved from the output of the Quality Control step.\nrule KrakenUniq:\n    \"\"\"Run KrakenUniq on trimmed fastq data\"\"\"\n    output:\n        report=\"results/KRAKENUNIQ/{sample}/krakenuniq.output\",\n        seqs=\"results/KRAKENUNIQ/{sample}/sequences.krakenuniq\",\n    input:\n        fastq=\"results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz\",\n    threads: 10\n    message:\n        \"KrakenUniq: PERFORMING TAXONOMIC CLASSIFICATION OF SAMPLE {input.fastq} WITH KRAKENUNIQ\"\n    shell:\n        \"krakenuniq --preload --db {params.DB} --fastq-input {input.fastq} --threads {threads} --output {output.seqs} --report-file {output.report} --gzip-compressed --only-classified-out &> {log}\"\nHere is a simplified version of this code:\nkrakenuniq --db $DBNAME --fastq-input $SAMPLE --threads 10 --output $SAMPLE.sequences.krakenuniq --report-file $SAMPLE.krakenuniq.output --gzip-compressed --only-classified-out\nAfter running KrakenUniq, the next step is to filter the output. The input is the krakenuniq.output generated in the previous step.\nrule Filter_KrakenUniq_Output:\n    output:\n        filtered=\"results/KRAKENUNIQ/{sample}/krakenuniq.output.filtered\",\n        pathogens=\"results/KRAKENUNIQ/{sample}/krakenuniq.output.pathogens\",\n        pathogen_tax_id=\"results/KRAKENUNIQ/{sample}/taxID.pathogens\",\n    input:\n        krakenuniq=\"results/KRAKENUNIQ/{sample}/krakenuniq.output\",\n        pathogenomesFound=config[\"pathogenomesFound\"],\n    message:\n        \"Filter_KrakenUniq_Output: APPLYING DEPTH AND BREADTH OF COVERAGE FILTERS TO KRAKENUNIQ OUTPUT FOR SAMPLE {input}\"\n    shell:\n        \"\"\"{params.exe} {input.krakenuniq} {params.n_unique_kmers} {params.n_tax_reads} {input.pathogenomesFound} &> {log}; \"\"\"\n        \"\"\"cut -f7 {output.pathogens} | tail -n +2 > {output.pathogen_tax_id}\"\"\"\ncd $OUTPUT; Rscript $PATH_TO_SCRIPTS/pipeline.R\ncut -f7 $OUTPUT/krakenuniq.output.pathogens | tail -n +2 > $OUTPUT/taxID.pathogens\ncat $OUTPUT/taxID.pathogens | parallel \"${PATH_TO_KRAKENUNIQ}/./krakenuniq-extract-reads {} $OUTPUT/sequences.krakenuniq ${SAMPLE} > $OUTPUT/{}.temp.fq\"\necho \"MeanReadLength\" > $OUTPUT/mean.reads.length; cd $OUTPUT\nfor i in $(cat taxID.pathogens); do awk '{if(NR%4==2) print length($1)}' ${i}.temp.fq | awk '{ sum += $0 } END { if (NR > 0) print sum / NR }' >> mean.reads.length; done; rm *.temp.fq\n\npaste krakenuniq.output.pathogens mean.reads.length > krakenuniq.output.pathogens_with_mean_read_length\ncat krakenuniq.output.pathogens_with_mean_read_length\nWe then visualize our filtered output using Krona:\nrule KrakenUniq2Krona:\n    output:\n        tax_ids=\"results/KRAKENUNIQ/{sample}/krakenuniq.output.filtered_taxIDs_kmers1000.txt\",\n        seqs=\"results/KRAKENUNIQ/{sample}/sequences.krakenuniq_kmers1000.txt\",\n        krona=\"results/KRAKENUNIQ/{sample}/sequences.krakenuniq_kmers1000.krona\",\n        html=\"results/KRAKENUNIQ/{sample}/taxonomy.krona.html\",\n    input:\n        report=\"results/KRAKENUNIQ/{sample}/krakenuniq.output.filtered\",\n        seqs=\"results/KRAKENUNIQ/{sample}/sequences.krakenuniq\",\n    params:\n        exe=WORKFLOW_DIR / \"scripts/krakenuniq2krona.py\",\n        DB=f\"--tax {config['krona_db']}\" if \"krona_db\" in config else \"\",\n    message:\n        \"KrakenUniq2Krona: VISUALIZING KRAKENUNIQ RESULTS WITH KRONA FOR SAMPLE {input.report}\"\n    shell:\n        \"{params.exe} {input.report} {input.seqs} &> {log}; \"\n        \"cat {output.seqs} | cut -f 2,3 > {output.krona}; \"\n        \"ktImportTaxonomy {output.krona} -o {output.html} {params.DB} &>> {log}\"\n\nLastly, we create an Abundance matrix:\nrule KrakenUniq_AbundanceMatrix:\n    output:\n        out_dir=directory(\"results/KRAKENUNIQ_ABUNDANCE_MATRIX\"),\n        unique_species=\"results/KRAKENUNIQ_ABUNDANCE_MATRIX/unique_species_taxid_list.txt\",\n        unique_species_names=\"results/KRAKENUNIQ_ABUNDANCE_MATRIX/unique_species_names_list.txt\",\n        abundance_matrix=\"results/KRAKENUNIQ_ABUNDANCE_MATRIX/krakenuniq_abundance_matrix.txt\",\n        abundance_plot=\"results/KRAKENUNIQ_ABUNDANCE_MATRIX/krakenuniq_absolute_abundance_heatmap.pdf\",\n    input:\n        expand(\"results/KRAKENUNIQ/{sample}/krakenuniq.output.filtered\", sample=SAMPLES),\n    params:\n        exe=WORKFLOW_DIR / \"scripts/krakenuniq_abundance_matrix.R\",\n        exe_plot=WORKFLOW_DIR / \"scripts/plot_krakenuniq_abundance_matrix.R\",\n        n_unique_kmers=config[\"n_unique_kmers\"],\n        n_tax_reads=config[\"n_tax_reads\"],\n    message:\n        \"KrakenUniq_AbundanceMatrix: COMPUTING KRAKENUNIQ MICROBIAL ABUNDANCE MATRIX\"\n    shell:\n        \"Rscript {params.exe} results/KRAKENUNIQ {output.out_dir} {params.n_unique_kmers} {params.n_tax_reads} &> {log};\"\n        \"Rscript {params.exe_plot} {output.out_dir} {output.out_dir} &> {log}\"\nprintf \"\\n\"; echo \"PERFORMING ALIGNMENT TO PATHO-GENOME\"\ntime bowtie2 --large-index -x $PATHO_GENOME --threads 10 --end-to-end --very-sensitive -U $SAMPLE | samtools view -bS -q 1 -h -@ 10 - > $OUTPUT/test_sample_AlignedToSpecies.bam\nsamtools sort $OUTPUT/test_sample_AlignedToSpecies.bam -@ 10 > $OUTPUT/test_sample_AlignedToSpecies.sorted.bam; samtools index $OUTPUT/test_sample_AlignedToSpecies.sorted.bam\nsamtools markdup -r $OUTPUT/test_sample_AlignedToSpecies.sorted.bam $OUTPUT/test_sample_AlignedToSpecies.sorted.dedup.bam; samtools index $OUTPUT/test_sample_AlignedToSpecies.sorted.dedup.bam\n \nprintf \"\\n\"; echo \"EXTRACTING ALIGNMENTS FOR CANDIDATES\"\ncat $OUTPUT/taxID.pathogens | parallel \"grep -w {} ${PATH_TO_PATHO_GENOME}/seqid2taxid.pathogen.map | cut -f1 > ${OUTPUT}/{}.seq.ids\"\nfor i in $(cat $OUTPUT/taxID.pathogens); do samtools view -bh $OUTPUT/test_sample_AlignedToSpecies.sorted.dedup.bam -@ 10 $(cat $OUTPUT/${i}.seq.ids | tr \"\\n\" \" \") > $OUTPUT/${i}.output.bam; done\n \nprintf \"\\n\"; echo \"RUNNING MAPDAMAGE ANCIENT STATUS ANALYSIS\"\nfind . -name '*.output.bam' | parallel \"mapDamage -i {} -r ${PATHO_GENOME} --merge-reference-sequences -d ${OUTPUT}/results_{}\"\n \nprintf \"\\n\"; echo \"ASSIGN ANCIENT STATUS\"\nRscript $PATH_TO_SCRIPTS/ancient_status.R 0.05 0.9 $OUTPUT\nprintf \"\\n\"; echo \"COMPUTE DEPTH AND BREADTH OF COVERAGE FROM ALIGNMENTS\"\necho \"NUMBER_OF_READS\" > DepthOfCoverage.${FASTQ_FILE}.txt; echo \"GENOME_LENGTH\" > GenomeLength.${FASTQ_FILE}.txt; echo \"BREADTH_OF_COVERAGE\" > BreadthOfCoverage.${FASTQ_FILE}.txt\nfor j in $(cut -f7 final_output.txt | sed '1d')\ndo\necho \"Organism $j\"\nif [ -s ${j}.output.bam ] && [ \"$(samtools view ${j}.output.bam | wc -l)\" -ne \"0\" ];\nthen\nsamtools sort ${j}.output.bam > ${j}.output.sorted.bam; samtools depth ${j}.output.sorted.bam | cut -f1 | uniq > Genomes_${j}.txt\nGENOME_LENGTH=$(grep -wFf Genomes_${j}.txt $PATH_TO_PATHO_GENOME/GenomeLength.txt | cut -f2 | awk '{ sum += $1; } END { print sum; }')\nif [ -s ${j}.output.sorted.bam ];\nthen\nNUMBER_OF_READS=$(samtools view ${j}.output.sorted.bam | wc -l); NUMBER_OF_COVERED_POSITIONS=$(samtools depth ${j}.output.sorted.bam | wc -l)\nelse\necho \"NA\" >> DepthOfCoverage.${FASTQ_FILE}.txt; echo \"NA\" >> BreadthOfCoverage.${FASTQ_FILE}.txt; echo \"NA\" >> GenomeLength.${FASTQ_FILE}.txt\ncontinue\nfi\necho $NUMBER_OF_READS >> DepthOfCoverage.${FASTQ_FILE}.txt; echo $GENOME_LENGTH >> GenomeLength.${FASTQ_FILE}.txt\necho \"scale=10 ; ($NUMBER_OF_COVERED_POSITIONS / $GENOME_LENGTH)*100\" | bc >> BreadthOfCoverage.${FASTQ_FILE}.txt\nelse\necho \"NA\" >> DepthOfCoverage.${FASTQ_FILE}.txt; echo \"NA\" >> BreadthOfCoverage.${FASTQ_FILE}.txt; echo \"NA\" >> GenomeLength.${FASTQ_FILE}.txt\ncontinue\nfi\ndone; rm Genomes_*.txt; paste final_output.txt DepthOfCoverage.${FASTQ_FILE}.txt GenomeLength.${FASTQ_FILE}.txt BreadthOfCoverage.${FASTQ_FILE}.txt > final_output_corrected.txt\n \ncp -r $OUTPUT /proj/snic2018-8-150/uppstore2018095/private/NBIS_Demo; cat final_output_corrected.txt\nprintf \"\\n\"; echo \"PIPELINE FINISHED SUCCESSFULLY\""
  },
  {
    "objectID": "align.html#bowtie2",
    "href": "align.html#bowtie2",
    "title": "9  Fast alignment with Bowtie2",
    "section": "9.1 Bowtie2",
    "text": "9.1 Bowtie2\nThe pipeline has a side branch for rapid analysis of the results based on an alignment with Bowtie2 and a post-mortem damage estimation with MapDamage. For this project, we won’t be able to use the real Bowtie2 databases, because there were too heavy for the resources allocated for this workshop. So we are only going to look at the code but not run it and the output will be provided for you in the folder containing the expected results."
  },
  {
    "objectID": "align.html#bowtie2-build-l-to-build-the-index-files",
    "href": "align.html#bowtie2-build-l-to-build-the-index-files",
    "title": "9  Fast alignment with Bowtie2",
    "section": "9.2 Bowtie2-build-l to build the index files",
    "text": "9.2 Bowtie2-build-l to build the index files\nIn order to run a Bowtie2 alignment, one needs a complete Bowtie2 database, in other words a .fna (fasta) file that has been indexed using the command bowtie2-build-l. This is the first part of the pipeline for the alignment step. You can therefore provide your own merged fna file for Bowtie2 to index if you wish to. However, if you are using one of our own custom databases, this step will be automatically ignored because its output already exists.\nYou can find the Bowtie2_build rule generating the index files below. It takes as input the path to the library.fna file that you have provided for the variable bowtie2_patho_db in your config.yaml file. For more information, refer to the main aMeta GitHub in the section about the config file. The rule then generates the index files of the Bowtie2 database if they don’t already exist.\nrule Bowtie2_Index:\n    output:\n        expand(\n            f\"{config['bowtie2_patho_db']}{{ext}}\",\n            ext=[\n                \".1.bt2l\",\n                \".2.bt2l\",\n                \".3.bt2l\",\n                \".4.bt2l\",\n                \".rev.1.bt2l\",\n                \".rev.2.bt2l\",\n            ],\n        ),\n    input:\n        ref=ancient(config[\"bowtie2_patho_db\"]),\n    log:\n        f\"{config['bowtie2_patho_db']}_BOWTIE2_BUILD.log\",\n    shell:\n        \"\"bowtie2-build-l --threads {threads} {input.ref} {input.ref} > {log} 2>&1\nHere is a simplified version of this code:\nbowtie2-build-l --threads 1 resources/library.fna resources/library.fna > logs/Bowtie2_Build.log 2>&1\nWARNING: No need to execute that line of code as we haven’t downloaded the real databases to the server for this workshop and it would overwrite the index files! The trick for this rule is that the input file is declared as ancient with the “ancient” function of snakemake, making snakemake not rerun the rule if the output files already exists. But if you only run the bash command provided, you will overwrite the files.\n\n9.2.1 Regarding real life projects with the actual databases\nFor real life projects, these are the Bowtie2 databases we have made available for download:\n\nPathogenome: Bowtie2 index and helping files for following up on microbial pathogens\nBowtie2_Full_NT: Bowtie2 index for full NCBI NT (for quick follow up of prokaryotes and eukaryotes; also contains helping files for building the Malt database)\n\nFor more information and links to download the databases, please refer to the official GitHub of aMeta.\nWARNING: if you are using the Bowtie2_Full_NT database, make sure that you have unzipped the files before running the pipeline, otherwise it will automatically overwrite the index files to regenerate them and will need a huge amount of memory and time to succeed, which you probably don’t want to happen ;-)"
  },
  {
    "objectID": "align.html#bowtie2-alignment",
    "href": "align.html#bowtie2-alignment",
    "title": "9  Fast alignment with Bowtie2",
    "section": "9.3 Bowtie2 alignment",
    "text": "9.3 Bowtie2 alignment\nrule Bowtie2_Pathogenome_Alignment:\n    output:\n        bam=\"results/BOWTIE2/{sample}/AlignedToPathogenome.bam\",\n        bai=\"results/BOWTIE2/{sample}/AlignedToPathogenome.bam.bai\",\n    input:\n        fastq=\"results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz\",\n        db=rules.Bowtie2_Index.output,\n    params:\n        PATHO_DB=lambda wildcards, input: config[\"bowtie2_patho_db\"],\n    log:\n        \"logs/BOWTIE2/{sample}.log\",\n    shell:\n        \"\"\"bowtie2 --large-index -x {params.PATHO_DB} --end-to-end --threads 10 --very-sensitive -U {input.fastq} 2> {log} | samtools view -bS -q 1 -h -@ 10 - | samtools sort -@ 10 -o {output.bam} >> {log};\"\"\"\n        \"\"\"samtools index {output.bam}\"\"\"\nHere is a simplified version of this code:\nbowtie2 --large-index -x resources/library.fna --end-to-end --threads 10 --very-sensitive -U results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz 2> logs/BOWTIE2/{sample}.log | samtools view -bS -q 1 -h -@ 10 - | samtools sort - -@ 10 -o results/BOWTIE2/{sample}/AlignedToPathogenome.bam >> logs/BOWTIE2/{sample}.log\nsamtools index results/BOWTIE2/{sample}/AlignedToPathogenome.bam\nAgain, no need to run that line of code. You can find how the output is supposed to look like in the folder with expected results. Basically, this line asks Bowtie2 to align each fastq file to the Bowtie2 database. This will generate a sam file containing the information of the alignment for each DNA sequence and the reference genome to which it aligns to. The sam file is then directly changed into a bam file using samtools, sorted and later indexed so that it is ready to be used. In order to run this line, you would still need to replace {sample} with the name of each sample that you have provided in the samples.tsv file and run this line for each of them."
  },
  {
    "objectID": "damage.html",
    "href": "damage.html",
    "title": "aMeta Workshop",
    "section": "",
    "text": "We run MapDamage:\nrule MapDamage:\n    \"\"\"Run Mapdamage on extracted pathogens\"\"\"\n    output:\n        dir=directory(\"results/MAPDAMAGE/{sample}\"),\n    input:\n        pathogen_tax_id=\"results/KRAKENUNIQ/{sample}/taxID.pathogens\",\n        bam=\"results/BOWTIE2/{sample}/AlignedToPathogenome.bam\",\n        bai=\"results/BOWTIE2/{sample}/AlignedToPathogenome.bam.bai\",\n    params:\n        pathogenome_path=os.path.dirname(config[\"pathogenomesFound\"]),\n        PATHO_DB=config[\"bowtie2_patho_db\"],\n        options=config[\"options\"].get(\"MapDamage\", \"\"),\n    threads: 10\n    message:\n        \"MapDamage: RUNNING MAPDAMAGE ON PATHOGENS IDENTIFIED IN SAMPLE {input.bam}\"\n    shell:\n        \"mkdir {output.dir}; \"\n        \"if [ -s {input.pathogen_tax_id} ]; then \"\n        'cat {input.pathogen_tax_id} | parallel -j {threads} \"grep -w {{}} {params.pathogenome_path}/seqid2taxid.pathogen.map | cut -f1 > {output.dir}/{{}}.seq_ids\" ; '\n        \"for i in $(cat {input.pathogen_tax_id}); do xargs --arg-file={output.dir}/${{i}}.seq_ids samtools view -bh {input.bam} --write-index -@ {threads} -o {output.dir}/${{i}}.tax.bam; done >> {log} 2>&1; \"\n        \"find {output.dir} -name '*.tax.bam' | parallel -j {threads} \\\"mapDamage {params.options} -i {{}} -r {params.PATHO_DB} --merge-reference-sequences -d {output.dir}/mapDamage_{{}}\\\" >> {log} 2>&1 || true; \"\n        \"for filename in {output.dir}/*.tax.bam; do newname=`echo $filename | sed 's/tax\\.//g'`; mv $filename $newname; done >> {log} 2>&1; \"\n        \"mv {output.dir}/mapDamage_{output.dir}/* {output.dir} >> {log} 2>&1; \"\n        \"rm -r {output.dir}/mapDamage_results >> {log} 2>&1; \"\n        \"else echo NO MICROBES TO AUTHENTICATE > {output.dir}/README.txt; fi\"\nHere is a simplified version of the code:\ncat {input.pathogen_tax_id} | parallel -j {threads} \"grep -w {{}} {params.pathogenome_path}/seqid2taxid.pathogen.map | cut -f1 > {output.dir}/{{}}.seq_ids\" ; '\n        \"for i in $(cat {input.pathogen_tax_id}); do xargs --arg-file={output.dir}/${{i}}.seq_ids samtools view -bh {input.bam} --write-index -@ {threads} -o {output.dir}/${{i}}.tax.bam; done >> {log} 2>&1; \"\n        \"find {output.dir} -name '*.tax.bam' | parallel -j {threads} \\\"mapDamage {params.options} -i {{}} -r {params.PATHO_DB} --merge-reference-sequences -d {output.dir}/mapDamage_{{}}\\\" >> {log} 2>&1 || true; \"\n        \"for filename in {output.dir}/*.tax.bam; do newname=`echo $filename | sed 's/tax\\.//g'`; mv $filename $newname; done >> {log} 2>&1; \"\n        \"mv {output.dir}/mapDamage_{output.dir}/* {output.dir} >> {log} 2>&1; \"\n        \"rm -r {output.dir}/mapDamage_results >> {log} 2>&1; \""
  },
  {
    "objectID": "malt.html#build-a-custom-malt-database-based-on-the-previous-results-from-the-filtering-of-krakenuniq",
    "href": "malt.html#build-a-custom-malt-database-based-on-the-previous-results-from-the-filtering-of-krakenuniq",
    "title": "11  MALT alignment and processing",
    "section": "11.1 Build a custom MALT database based on the previous results from the filtering of KrakenUniq",
    "text": "11.1 Build a custom MALT database based on the previous results from the filtering of KrakenUniq\nFirst we need to build the custom database with the rule Build_Malt_DB. This is done with the use of a python script. Inside the script, python will use the input and output variables provided by snakemake.\nrule Build_Malt_DB:\n    output:\n        seqid2taxid_project=\"results/MALT_DB/seqid2taxid.project.map\",\n        seqids_project=\"results/MALT_DB/seqids.project\",\n        project_headers=\"results/MALT_DB/project.headers\",\n        project_fasta=\"results/MALT_DB/library.project.fna\",\n        db=directory(\"results/MALT_DB/maltDB.dat\"),\n    input:\n        unique_taxids=ancient(\"results/KRAKENUNIQ_ABUNDANCE_MATRIX/unique_species_taxid_list.txt\"),\n    params:\n        seqid2taxid=config[\"malt_seqid2taxid_db\"],\n        nt_fasta=config[\"malt_nt_fasta\"],\n        accession2taxid=config[\"malt_accession2taxid\"],\n    threads: 20\n    log:\n        \"logs/BUILD_MALT_DB/BUILD_MALT_DB.log\",\n    conda:\n        \"../envs/malt.yaml\"\n    envmodules:\n        *config[\"envmodules\"][\"malt\"],\n    benchmark:\n        \"benchmarks/BUILD_MALT_DB/BUILD_MALT_DB.benchmark.txt\"\n    message:\n        \"Build_Malt_DB: BUILDING MALT DATABASE USING SPECIES DETECTED BY KRAKENUNIQ\"\n    script:\n        \"../scripts/malt-build.py\"\nHere is a simplified version of this code:\npython ../scripts/malt-build.py \\\n    --seqid2taxid ${seqid2taxid} \\\n    --nt_fasta ${nt_fasta} \\\n    --accession2taxid ${accession2taxid} \\\n    --unique_taxids results/KRAKENUNIQ_ABUNDANCE_MATRIX/unique_species_taxid_list.txt \\\n    --output_dir results/MALT_DB/ \\\n    --threads 20 &> logs/BUILD_MALT_DB/BUILD_MALT_DB.log"
  },
  {
    "objectID": "malt.html#align-the-fastq-files-using-malt",
    "href": "malt.html#align-the-fastq-files-using-malt",
    "title": "11  MALT alignment and processing",
    "section": "11.2 Align the FastQ files using MALT",
    "text": "11.2 Align the FastQ files using MALT\nrule Malt:\n    output:\n        rma6=\"results/MALT/{sample}.trimmed.rma6\",\n        sam=\"results/MALT/{sample}.trimmed.sam.gz\",\n    input:\n        fastq=ancient(\"results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz\"),\n        db=ancient(\"results/MALT_DB/maltDB.dat\"),\n    params:\n        gunzipped_sam=\"results/MALT/{sample}.trimmed.sam\",\n    threads: 20\n    log:\n        \"logs/MALT/{sample}.log\",\n    conda:\n        \"../envs/malt.yaml\"\n    envmodules:\n        *config[\"envmodules\"][\"malt\"],\n    benchmark:\n        \"benchmarks/MALT/{sample}.benchmark.txt\"\n    message:\n        \"Malt: RUNNING MALT ALIGNMENTS FOR SAMPLE {input.fastq}\"\n    shell:\n        \"unset DISPLAY; malt-run -at SemiGlobal -m BlastN -i {input.fastq} -o {output.rma6} -a {params.gunzipped_sam} -t {threads} -d {input.db} &> {log}\"\nHere is a simplified version of this code:\nunset DISPLAY\nmalt-run -at SemiGlobal -m BlastN -i results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz \\\n    -o results/MALT/{sample}.trimmed.rma6 \\\n    -a results/MALT/{sample}.trimmed.sam \\\n    -t 20 -d results/MALT_DB/maltDB.dat \\\n    &> logs/MALT/{sample}.log\ngzip results/MALT/{sample}.trimmed.sam"
  },
  {
    "objectID": "malt.html#quantify-the-microbial-abundance-using-the-sam-file-generated-by-the-malt-alignment",
    "href": "malt.html#quantify-the-microbial-abundance-using-the-sam-file-generated-by-the-malt-alignment",
    "title": "11  MALT alignment and processing",
    "section": "11.3 Quantify the microbial abundance using the sam file generated by the MALT alignment",
    "text": "11.3 Quantify the microbial abundance using the sam file generated by the MALT alignment\nrule Malt_QuantifyAbundance:\n    output:\n        out_dir=directory(\"results/MALT_QUANTIFY_ABUNDANCE/{sample}\"),\n        counts=\"results/MALT_QUANTIFY_ABUNDANCE/{sample}/sam_counts.txt\",\n    input:\n        sam=ancient(\"results/MALT/{sample}.trimmed.sam.gz\"),\n    params:\n        unique_taxids=\"results/KRAKENUNIQ_ABUNDANCE_MATRIX/unique_species_taxid_list.txt\",\n        exe=WORKFLOW_DIR / \"scripts/malt_quantify_abundance.py\",\n    log:\n        \"logs/MALT_QUANTIFY_ABUNDANCE/{sample}.log\",\n    benchmark:\n        \"benchmarks/MALT_QUANTIFY_ABUNDANCE/{sample}.benchmark.txt\"\n    message:\n        \"Malt_QuantifyAbundance: QUANTIFYING MICROBIAL ABUNDANCE USING MALT SAM-ALIGNMENTS FOR SAMPLE {input.sam}\"\n    shell:\n        \"mkdir -p {output.out_dir}; \"\n        \"{params.exe} {input.sam} {params.unique_taxids} > {output.counts} 2> {log}\"\nHere is a simplified version of this code:\nmkdir -p results/MALT_QUANTIFY_ABUNDANCE/{sample}; \nWORKFLOW_DIR/scripts/malt_quantify_abundance.py results/MALT/{sample}.trimmed.sam.gz results/KRAKENUNIQ_ABUNDANCE_MATRIX/unique_species_taxid_list.txt > results/MALT_QUANTIFY_ABUNDANCE/{sample}/sam_counts.txt 2> logs/MALT_QUANTIFY_ABUNDANCE/{sample}.log"
  },
  {
    "objectID": "malt.html#compute-the-malt-abundance-matrix-using-the-sam-files-generated-by-malt",
    "href": "malt.html#compute-the-malt-abundance-matrix-using-the-sam-files-generated-by-malt",
    "title": "11  MALT alignment and processing",
    "section": "11.4 Compute the MALT abundance matrix using the SAM files generated by MALT",
    "text": "11.4 Compute the MALT abundance matrix using the SAM files generated by MALT\nrule Malt_AbundanceMatrix_Sam:\n    output:\n        out_dir=directory(\"results/MALT_ABUNDANCE_MATRIX_SAM\"),\n        abundance_matrix=\"results/MALT_ABUNDANCE_MATRIX_SAM/malt_abundance_matrix_sam.txt\",\n    input:\n        sam_counts=expand(\n            \"results/MALT_QUANTIFY_ABUNDANCE/{sample}/sam_counts.txt\", sample=SAMPLES\n        ),\n    log:\n        \"logs/MALT_ABUNDANCE_MATRIX_SAM/MALT_ABUNDANCE_MATRIX_SAM.log\",\n    params:\n        exe=WORKFLOW_DIR / \"scripts/malt_abundance_matrix.R\",\n    conda:\n        \"../envs/r.yaml\"\n    envmodules:\n        *config[\"envmodules\"][\"r\"],\n    benchmark:\n        \"benchmarks/MALT_ABUNDANCE_MATRIX_SAM/MALT_ABUNDANCE_MATRIX_SAM.benchmark.txt\"\n    message:\n        \"Malt_AbundanceMatrix_Sam: COMPUTING MALT MICROBIAL ABUNDANCE MATRIX FROM SAM-FILES\"\n    shell:\n        \"Rscript {params.exe} results/MALT_QUANTIFY_ABUNDANCE {output.out_dir} &> {log}\"\nHere is a simplified version of this code:\nmkdir -p results/MALT_ABUNDANCE_MATRIX_SAM;\nRscript WORKFLOW_DIR/scripts/malt_abundance_matrix.R results/MALT_QUANTIFY_ABUNDANCE results/MALT_ABUNDANCE_MATRIX_SAM/ > logs/MALT_ABUNDANCE_MATRIX_SAM/MALT_ABUNDANCE_MATRIX_SAM.log 2>&1"
  },
  {
    "objectID": "malt.html#compute-the-malt-abundance-matrix-using-the-rma6-files-generated-by-malt",
    "href": "malt.html#compute-the-malt-abundance-matrix-using-the-rma6-files-generated-by-malt",
    "title": "11  MALT alignment and processing",
    "section": "11.5 Compute the MALT abundance matrix using the rma6 files generated by MALT",
    "text": "11.5 Compute the MALT abundance matrix using the rma6 files generated by MALT\nrule Malt_AbundanceMatrix_Rma6:\n    output:\n        out_dir=directory(\"results/MALT_ABUNDANCE_MATRIX_RMA6\"),\n        abundance_matrix=\"results/MALT_ABUNDANCE_MATRIX_RMA6/malt_abundance_matrix_rma6.txt\",\n    input:\n        rma6=expand(\"results/MALT/{sample}.trimmed.rma6\", sample=SAMPLES),\n    params:\n        exe=WORKFLOW_DIR / \"scripts/rma-tabuliser\",\n    log:\n        \"logs/MALT_ABUNDANCE_MATRIX_RMA6/MALT_ABUNDANCE_MATRIX_RMA6.log\",\n    envmodules:\n        *config[\"envmodules\"][\"malt\"],\n    conda:\n        \"../envs/malt.yaml\"\n    benchmark:\n        \"benchmarks/MALT_ABUNDANCE_MATRIX_RMA6/MALT_ABUNDANCE_MATRIX_RMA6.benchmark.txt\"\n    message:\n        \"Malt_AbundanceMatrix_Rma6: COMPUTING MALT MICROBIAL ABUNDANCE MATRIX FROM RMA6-FILES\"\n    shell:\n        \"{params.exe} -d $(dirname {input.rma6}) -r 'S' &> {log}; \"\n        \"mv results/MALT/count_table.tsv {output.out_dir}; \"\n        \"mv {output.out_dir}/count_table.tsv {output.abundance_matrix}\"\nHere is a simplified version of this code:\nrma-tabuliser -d results/MALT/{sample}.trimmed.rma6 -r 'S' &> logs/MALT_ABUNDANCE_MATRIX_RMA6/MALT_ABUNDANCE_MATRIX_RMA6.log;\nmv results/MALT/count_table.tsv results/MALT_ABUNDANCE_MATRIX_RMA6/;\nmv results/MALT_ABUNDANCE_MATRIX_RMA6/count_table.tsv results/MALT_ABUNDANCE_MATRIX_RMA6/malt_abundance_matrix_rma6.txt"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "aMeta Workshop",
    "section": "",
    "text": "To plot the authentication score and summarise the output of the workflow, we use the following code:\nrule Plot_Authentication_Score:\n\"\"\"Plot authentication score\"\"\"\n    output:\n        heatmap=\"results/overview_heatmap_scores.pdf\",    \n    input:\n        scores=expand(\"results/AUTHENTICATION/.{sample}_done\",sample=SAMPLES)\n    message:\n        \"Plot_Authentication_Score: PLOTTING HEATMAP OF AUTHENTICATION SCORES\"\n    params:\n        exe=WORKFLOW_DIR / \"scripts/plot_score.R\",\n    shell:\n        \"Rscript {params.exe} results/AUTHENTICATION $(dirname {output.heatmap}) &> {log}\"\nHere is a simplified version of this code:\n\"Rscript {params.exe} results/AUTHENTICATION $(dirname {output.heatmap})\""
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Pochon, Zoé, Nora Bergfeldt, Emrah Kırdök, Mário Vicente, Thijessen\nNaidoo, Tom van der Valk, N. Ezgi Altınışık, et al. 2022. “aMeta:\nAn Accurate and Memory-Efficient Ancient Metagenomic Profiling\nWorkflow.” bioRxiv. https://doi.org/10.1101/2022.10.03.510579."
  }
]