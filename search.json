[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "aMeta Workshop",
    "section": "",
    "text": "Welcome to the aMeta: an accurate and memory-efficient ancient Metagenomic profiling workflow course that will be held in Mersin University, Department of Biotechnology in the scope of the European Union funded NEOMATRIX collaboration project.\nIn this web page, we aim to publish the tutorial material of aMeta ancient metagenomic profiling workflow.\naMeta (Pochon et al. 2022) is designed to analyse the microbial content in ancient specimens from wide variety of source organisms (including humans).\nPlease find the whole pipline through this github repo\nextending\n\n\n\n\nPochon, Zoé, Nora Bergfeldt, Emrah Kırdök, Mário Vicente, Thijessen Naidoo, Tom van der Valk, N. Ezgi Altınışık, et al. 2022. “aMeta: An Accurate and Memory-Efficient Ancient Metagenomic Profiling Workflow.” bioRxiv. https://doi.org/10.1101/2022.10.03.510579."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "2  About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "course-style.html#using-conda-package-manager",
    "href": "course-style.html#using-conda-package-manager",
    "title": "3  How to follow the Course?",
    "section": "3.1 Using conda package manager",
    "text": "3.1 Using conda package manager\nEach rule can conain a conda environment. So, if you will run snakemake with --use-conda flag, the specified environment will be installed given you have the conda package manager already installed:\n    conda:\n        \"../envs/fastqc.yaml\""
  },
  {
    "objectID": "course-style.html#using-environment-modules",
    "href": "course-style.html#using-environment-modules",
    "title": "3  How to follow the Course?",
    "section": "3.2 Using environment modules",
    "text": "3.2 Using environment modules\nIf you want to use the environmental modules, then you need to use envmodules flag with the snakemake. In this case, you would like to modify the envirınmental module file.\n envmodules:\n        *config[\"envmodules\"][\"fastqc\"]"
  },
  {
    "objectID": "course-style.html#benchmarks",
    "href": "course-style.html#benchmarks",
    "title": "3  How to follow the Course?",
    "section": "3.3 Benchmarks",
    "text": "3.3 Benchmarks\nYou could find benchmarking stats with under the benchmarking folder.\n    benchmark:\n        \"benchmarks/FASTQC_BEFORE_TRIMMING/{sample}.benchmark.txt\""
  },
  {
    "objectID": "course-style.html#logs",
    "href": "course-style.html#logs",
    "title": "3  How to follow the Course?",
    "section": "3.4 Logs",
    "text": "3.4 Logs\nIf you have problems with your snakemake run, each rule will have a log file. So you can check the errors:\n    log:\n        \"logs/FASTQC_BEFORE_TRIMMING/{sample}.log\","
  },
  {
    "objectID": "qc.html#quality-control-before-trimming",
    "href": "qc.html#quality-control-before-trimming",
    "title": "4  Quality control and trimming",
    "section": "4.1 Quality control before trimming",
    "text": "4.1 Quality control before trimming\nThe pipeline starts with quality control step. We will first run FastQC tool to check the qualitu of the fastq files.\nThis is the rule in the workflow/rules/qc.smk file. Input files are retrieved from the samplesheet that you provided. And you will have two output files html and zip files. The html file contains the final report.\n\nrule FastQC_BeforeTrimming:\n    \"\"\"Run fastq before trimming\"\"\"\n    output:\n        html=\"results/FASTQC_BEFORE_TRIMMING/{sample}_fastqc.html\",\n        zip=\"results/FASTQC_BEFORE_TRIMMING/{sample}_fastqc.zip\",\n    input:\n        fastq=lambda wildcards: samples.loc[wildcards.sample].fastq,\n    message:\n        \"FastQC_BeforeTrimming: RUNNING QUALITY CONTROL WITH FASTQC FOR SAMPLE {input.fastq} BEFORE TRIMMING ADAPTERS\"\n    threads: 2\n    shell:\n        \"fastqc {input.fastq} --threads {threads} --nogroup --outdir results/FASTQC_BEFORE_TRIMMING &> {log}\"\n\nHere is a simplified version of this code:\n\nfastqc FASTQ --threads 2 --nogroup --outdir results/FASTQC_BEFORE_TRIMMING/\n\n## Quality trimming step\nAfter we run quality controls. We will run cutadapt tool to remove the leftover adapters and low quality bases. Here is the overview of this rule.\nYou can add or remove adapters using the config/config.yaml file.\nIn this file the inputs will be retrieved again from the samplesheet file. And the outputs will be placed to the results/CUTADAPT_ADAPTER_TRIMMING/ folder.\nrule Cutadapt_Adapter_Trimming:\n    output:\n        fastq=\"results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz\",\n    input:\n        fastq=lambda wildcards: samples.loc[wildcards.sample].fastq,\n    params:\n        adapters=\" \".join([f\"-a {x}\" for x in ADAPTERS]),\n    message:\n        \"Cutadapt_Adapter_Trimming: TRIMMING ADAPTERS FOR SAMPLE {input.fastq} WITH CUTADAPT\"\n    threads: 1\n    shell:\n        \"cutadapt {params.adapters} --minimum-length 30 -o {output.fastq} {input.fastq} &> {log}\"\n\nHere is how we will run it for one file:\ncutadapt -a ADAPTERS --minimum-length 30 -o results/CUTADAPT_ADAPTER_TRIMMING/sample"
  },
  {
    "objectID": "qc.html#quality-control-after-trimming",
    "href": "qc.html#quality-control-after-trimming",
    "title": "4  Quality control and trimming",
    "section": "4.2 Quality control after trimming",
    "text": "4.2 Quality control after trimming\nThen we will run the quality control after the trimming step:\nrule FastQC_AfterTrimming: output: html=“results/FASTQC_AFTER_TRIMMING/{sample}.trimmed_fastqc.html”, zip=“results/FASTQC_AFTER_TRIMMING/{sample}.trimmed_fastqc.zip”, input: fastq=“results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz”, threads: 2 message: “FastQC_AfterTrimming: RUNNING QUALITY CONTROL WITH FASTQC FOR SAMPLE {input.fastq} AFTER TRIMMING ADAPTERS” shell: “fastqc {input.fastq} –threads {threads} –nogroup –outdir results/FASTQC_AFTER_TRIMMING &> {log}”\nHere is how we will run it for one file:\nfastqc results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz --threads 2 --nogroup --outdir results/FASTQC_AFTER_TRIMMING"
  },
  {
    "objectID": "qc.html#combine-quality-control-outputs",
    "href": "qc.html#combine-quality-control-outputs",
    "title": "4  Quality control and trimming",
    "section": "4.3 Combine quality control outputs",
    "text": "4.3 Combine quality control outputs\nAt last we will combine all the fastqc reports into one file using Multifastqc tool:\nrule MultiQC:\n    \"\"\"Run MultiQC\"\"\"\n    output:\n        html=\"results/MULTIQC/multiqc_report.html\",\n    input:\n        unpack(multiqc_input),\n    params:\n        config=os.path.join(WORKFLOW_DIR, \"envs\", \"multiqc_config.yaml\"),\n    envmodules:\n        *config[\"envmodules\"][\"multiqc\"],\n    benchmark:\n        \"benchmarks/MULTIQC/MULTIQC.benchmark.txt\"\n    message:\n        \"MultiQC: COMBINING QUALITY CONTROL METRICS WITH MULTIQC\"\n    shell:\n        'echo {input} | tr \" \" \"\\n\" > {output.html}.fof;'\n        \"multiqc -c {params.config} -l {output.html}.fof --verbose --force --outdir results/MULTIQC &> {log}\""
  },
  {
    "objectID": "krakenuniq.html#run-krakenuniq",
    "href": "krakenuniq.html#run-krakenuniq",
    "title": "5  KrakenUniq database",
    "section": "5.1 Run KrakenUniq",
    "text": "5.1 Run KrakenUniq\nThis is the rule in the workflow/rules/krakenuniq.smk file. Input files are retrieved from the output of the Quality Control step.\nrule KrakenUniq:\n    \"\"\"Run KrakenUniq on trimmed fastq data\"\"\"\n    output:\n        report=\"results/KRAKENUNIQ/{sample}/krakenuniq.output\",\n        seqs=\"results/KRAKENUNIQ/{sample}/sequences.krakenuniq\",\n    input:\n        fastq=\"results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz\",\n    threads: 10\n    message:\n        \"KrakenUniq: PERFORMING TAXONOMIC CLASSIFICATION OF SAMPLE {input.fastq} WITH KRAKENUNIQ\"\n    shell:\n        \"krakenuniq --preload --db {params.DB} --fastq-input {input.fastq} --threads {threads} --output {output.seqs} --report-file {output.report} --gzip-compressed --only-classified-out &> {log}\"\nHere is a simplified version of this code:\nkrakenuniq --db $DBNAME --fastq-input $SAMPLE --threads 10 --output $SAMPLE.sequences.krakenuniq --report-file $SAMPLE.krakenuniq.output --gzip-compressed --only-classified-out\nAfter running KrakenUniq, the next step is to filter the output. The input is the krakenuniq.output generated in the previous step.\nrule Filter_KrakenUniq_Output:\n    output:\n        filtered=\"results/KRAKENUNIQ/{sample}/krakenuniq.output.filtered\",\n        pathogens=\"results/KRAKENUNIQ/{sample}/krakenuniq.output.pathogens\",\n        pathogen_tax_id=\"results/KRAKENUNIQ/{sample}/taxID.pathogens\",\n    input:\n        krakenuniq=\"results/KRAKENUNIQ/{sample}/krakenuniq.output\",\n        pathogenomesFound=config[\"pathogenomesFound\"],\n    message:\n        \"Filter_KrakenUniq_Output: APPLYING DEPTH AND BREADTH OF COVERAGE FILTERS TO KRAKENUNIQ OUTPUT FOR SAMPLE {input}\"\n    shell:\n        \"\"\"{params.exe} {input.krakenuniq} {params.n_unique_kmers} {params.n_tax_reads} {input.pathogenomesFound} &> {log}; \"\"\"\n        \"\"\"cut -f7 {output.pathogens} | tail -n +2 > {output.pathogen_tax_id}\"\"\"\ncd $OUTPUT; Rscript $PATH_TO_SCRIPTS/pipeline.R\ncut -f7 $OUTPUT/krakenuniq.output.pathogens | tail -n +2 > $OUTPUT/taxID.pathogens\ncat $OUTPUT/taxID.pathogens | parallel \"${PATH_TO_KRAKENUNIQ}/./krakenuniq-extract-reads {} $OUTPUT/sequences.krakenuniq ${SAMPLE} > $OUTPUT/{}.temp.fq\"\necho \"MeanReadLength\" > $OUTPUT/mean.reads.length; cd $OUTPUT\nfor i in $(cat taxID.pathogens); do awk '{if(NR%4==2) print length($1)}' ${i}.temp.fq | awk '{ sum += $0 } END { if (NR > 0) print sum / NR }' >> mean.reads.length; done; rm *.temp.fq\n\npaste krakenuniq.output.pathogens mean.reads.length > krakenuniq.output.pathogens_with_mean_read_length\ncat krakenuniq.output.pathogens_with_mean_read_length\nWe then visualize our filtered output using Krona:\nrule KrakenUniq2Krona:\n    output:\n        tax_ids=\"results/KRAKENUNIQ/{sample}/krakenuniq.output.filtered_taxIDs_kmers1000.txt\",\n        seqs=\"results/KRAKENUNIQ/{sample}/sequences.krakenuniq_kmers1000.txt\",\n        krona=\"results/KRAKENUNIQ/{sample}/sequences.krakenuniq_kmers1000.krona\",\n        html=\"results/KRAKENUNIQ/{sample}/taxonomy.krona.html\",\n    input:\n        report=\"results/KRAKENUNIQ/{sample}/krakenuniq.output.filtered\",\n        seqs=\"results/KRAKENUNIQ/{sample}/sequences.krakenuniq\",\n    params:\n        exe=WORKFLOW_DIR / \"scripts/krakenuniq2krona.py\",\n        DB=f\"--tax {config['krona_db']}\" if \"krona_db\" in config else \"\",\n    message:\n        \"KrakenUniq2Krona: VISUALIZING KRAKENUNIQ RESULTS WITH KRONA FOR SAMPLE {input.report}\"\n    shell:\n        \"{params.exe} {input.report} {input.seqs} &> {log}; \"\n        \"cat {output.seqs} | cut -f 2,3 > {output.krona}; \"\n        \"ktImportTaxonomy {output.krona} -o {output.html} {params.DB} &>> {log}\"\n\nLastly, we create an Abundance matrix:\nrule KrakenUniq_AbundanceMatrix:\n    output:\n        out_dir=directory(\"results/KRAKENUNIQ_ABUNDANCE_MATRIX\"),\n        unique_species=\"results/KRAKENUNIQ_ABUNDANCE_MATRIX/unique_species_taxid_list.txt\",\n        unique_species_names=\"results/KRAKENUNIQ_ABUNDANCE_MATRIX/unique_species_names_list.txt\",\n        abundance_matrix=\"results/KRAKENUNIQ_ABUNDANCE_MATRIX/krakenuniq_abundance_matrix.txt\",\n        abundance_plot=\"results/KRAKENUNIQ_ABUNDANCE_MATRIX/krakenuniq_absolute_abundance_heatmap.pdf\",\n    input:\n        expand(\"results/KRAKENUNIQ/{sample}/krakenuniq.output.filtered\", sample=SAMPLES),\n    params:\n        exe=WORKFLOW_DIR / \"scripts/krakenuniq_abundance_matrix.R\",\n        exe_plot=WORKFLOW_DIR / \"scripts/plot_krakenuniq_abundance_matrix.R\",\n        n_unique_kmers=config[\"n_unique_kmers\"],\n        n_tax_reads=config[\"n_tax_reads\"],\n    message:\n        \"KrakenUniq_AbundanceMatrix: COMPUTING KRAKENUNIQ MICROBIAL ABUNDANCE MATRIX\"\n    shell:\n        \"Rscript {params.exe} results/KRAKENUNIQ {output.out_dir} {params.n_unique_kmers} {params.n_tax_reads} &> {log};\"\n        \"Rscript {params.exe_plot} {output.out_dir} {output.out_dir} &> {log}\"\nprintf \"\\n\"; echo \"PERFORMING ALIGNMENT TO PATHO-GENOME\"\ntime bowtie2 --large-index -x $PATHO_GENOME --threads 10 --end-to-end --very-sensitive -U $SAMPLE | samtools view -bS -q 1 -h -@ 10 - > $OUTPUT/test_sample_AlignedToSpecies.bam\nsamtools sort $OUTPUT/test_sample_AlignedToSpecies.bam -@ 10 > $OUTPUT/test_sample_AlignedToSpecies.sorted.bam; samtools index $OUTPUT/test_sample_AlignedToSpecies.sorted.bam\nsamtools markdup -r $OUTPUT/test_sample_AlignedToSpecies.sorted.bam $OUTPUT/test_sample_AlignedToSpecies.sorted.dedup.bam; samtools index $OUTPUT/test_sample_AlignedToSpecies.sorted.dedup.bam\n \nprintf \"\\n\"; echo \"EXTRACTING ALIGNMENTS FOR CANDIDATES\"\ncat $OUTPUT/taxID.pathogens | parallel \"grep -w {} ${PATH_TO_PATHO_GENOME}/seqid2taxid.pathogen.map | cut -f1 > ${OUTPUT}/{}.seq.ids\"\nfor i in $(cat $OUTPUT/taxID.pathogens); do samtools view -bh $OUTPUT/test_sample_AlignedToSpecies.sorted.dedup.bam -@ 10 $(cat $OUTPUT/${i}.seq.ids | tr \"\\n\" \" \") > $OUTPUT/${i}.output.bam; done\n \nprintf \"\\n\"; echo \"RUNNING MAPDAMAGE ANCIENT STATUS ANALYSIS\"\nfind . -name '*.output.bam' | parallel \"mapDamage -i {} -r ${PATHO_GENOME} --merge-reference-sequences -d ${OUTPUT}/results_{}\"\n \nprintf \"\\n\"; echo \"ASSIGN ANCIENT STATUS\"\nRscript $PATH_TO_SCRIPTS/ancient_status.R 0.05 0.9 $OUTPUT\nprintf \"\\n\"; echo \"COMPUTE DEPTH AND BREADTH OF COVERAGE FROM ALIGNMENTS\"\necho \"NUMBER_OF_READS\" > DepthOfCoverage.${FASTQ_FILE}.txt; echo \"GENOME_LENGTH\" > GenomeLength.${FASTQ_FILE}.txt; echo \"BREADTH_OF_COVERAGE\" > BreadthOfCoverage.${FASTQ_FILE}.txt\nfor j in $(cut -f7 final_output.txt | sed '1d')\ndo\necho \"Organism $j\"\nif [ -s ${j}.output.bam ] && [ \"$(samtools view ${j}.output.bam | wc -l)\" -ne \"0\" ];\nthen\nsamtools sort ${j}.output.bam > ${j}.output.sorted.bam; samtools depth ${j}.output.sorted.bam | cut -f1 | uniq > Genomes_${j}.txt\nGENOME_LENGTH=$(grep -wFf Genomes_${j}.txt $PATH_TO_PATHO_GENOME/GenomeLength.txt | cut -f2 | awk '{ sum += $1; } END { print sum; }')\nif [ -s ${j}.output.sorted.bam ];\nthen\nNUMBER_OF_READS=$(samtools view ${j}.output.sorted.bam | wc -l); NUMBER_OF_COVERED_POSITIONS=$(samtools depth ${j}.output.sorted.bam | wc -l)\nelse\necho \"NA\" >> DepthOfCoverage.${FASTQ_FILE}.txt; echo \"NA\" >> BreadthOfCoverage.${FASTQ_FILE}.txt; echo \"NA\" >> GenomeLength.${FASTQ_FILE}.txt\ncontinue\nfi\necho $NUMBER_OF_READS >> DepthOfCoverage.${FASTQ_FILE}.txt; echo $GENOME_LENGTH >> GenomeLength.${FASTQ_FILE}.txt\necho \"scale=10 ; ($NUMBER_OF_COVERED_POSITIONS / $GENOME_LENGTH)*100\" | bc >> BreadthOfCoverage.${FASTQ_FILE}.txt\nelse\necho \"NA\" >> DepthOfCoverage.${FASTQ_FILE}.txt; echo \"NA\" >> BreadthOfCoverage.${FASTQ_FILE}.txt; echo \"NA\" >> GenomeLength.${FASTQ_FILE}.txt\ncontinue\nfi\ndone; rm Genomes_*.txt; paste final_output.txt DepthOfCoverage.${FASTQ_FILE}.txt GenomeLength.${FASTQ_FILE}.txt BreadthOfCoverage.${FASTQ_FILE}.txt > final_output_corrected.txt\n \ncp -r $OUTPUT /proj/snic2018-8-150/uppstore2018095/private/NBIS_Demo; cat final_output_corrected.txt\nprintf \"\\n\"; echo \"PIPELINE FINISHED SUCCESSFULLY\"\n\n5.1.1 Kraken2 vs KrakenUniq\nKraken2 is incredibly fast but unfortunately has a high False Positive Rate. KrakenUniq reduces the fraction of false discoveries (compared to Kraken2) by reporting the number of unique k-mers that is analogous to breadth of coverage information. Therefore filtering KrakenUniq output by number of unique k-mers, we get a more reliable list of candidates compared to Kraken2 (the output from Kraken2 can only be filtered by microbial abundance, i.e. depth of coverage). Considering filtered KrakenUniq output as a ground truth, we investigated how many records from Kraken2 output should be retained in order to capture all the species identified with KareknUniq, i.e. how long down the Kraken2 list one needs to go in order to detect all the species reported by KrakenUniq.\nWhat we can conclude after having screened ~100 samples is that a typical sample contains ~100 species reported (after filtering) by KrakenUniq. This corresponds to approximately ~1000 species in Kraken2 output (or 0.01% assigned reads). This implies that if we select top 1000 species in Kraken2, 900 species will probably be false positives and only 10% of all species will be true positives. If we consider KrakenUniq output as a ground truth."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Pochon, Zoé, Nora Bergfeldt, Emrah Kırdök, Mário Vicente, Thijessen\nNaidoo, Tom van der Valk, N. Ezgi Altınışık, et al. 2022. “aMeta:\nAn Accurate and Memory-Efficient Ancient Metagenomic Profiling\nWorkflow.” bioRxiv. https://doi.org/10.1101/2022.10.03.510579."
  }
]