[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "aMeta Workshop",
    "section": "",
    "text": "Welcome to the aMeta: an accurate and memory-efficient ancient Metagenomic profiling workflow course that will be held in Mersin University, Department of Biotechnology in the scope of the European Union funded NEOMATRIX collaboration project.\nIn this web page, we aim to publish the tutorial material of aMeta ancient metagenomic profiling workflow.\naMeta (Pochon et al. 2022) is designed to analyse the microbial content in ancient specimens from wide variety of source organisms (including humans).\nPlease find the whole pipline through this github repo\nextending\n\n\n\n\nPochon, Zoé, Nora Bergfeldt, Emrah Kırdök, Mário Vicente, Thijessen Naidoo, Tom van der Valk, N. Ezgi Altınışık, et al. 2022. “aMeta: An Accurate and Memory-Efficient Ancient Metagenomic Profiling Workflow.” bioRxiv. https://doi.org/10.1101/2022.10.03.510579."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "2  About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "course-style.html#using-conda-package-manager",
    "href": "course-style.html#using-conda-package-manager",
    "title": "3  How to follow the Course?",
    "section": "3.1 Using conda package manager",
    "text": "3.1 Using conda package manager\nEach rule can conain a conda environment. So, if you will run snakemake with --use-conda flag, the specified environment will be installed given you have the conda package manager already installed:\n    conda:\n        \"../envs/fastqc.yaml\""
  },
  {
    "objectID": "course-style.html#using-environment-modules",
    "href": "course-style.html#using-environment-modules",
    "title": "3  How to follow the Course?",
    "section": "3.2 Using environment modules",
    "text": "3.2 Using environment modules\nIf you want to use the environmental modules, then you need to use envmodules flag with the snakemake. In this case, you would like to modify the envirınmental module file.\n envmodules:\n        *config[\"envmodules\"][\"fastqc\"]"
  },
  {
    "objectID": "course-style.html#benchmarks",
    "href": "course-style.html#benchmarks",
    "title": "3  How to follow the Course?",
    "section": "3.3 Benchmarks",
    "text": "3.3 Benchmarks\nYou could find benchmarking stats with under the benchmarking folder.\n    benchmark:\n        \"benchmarks/FASTQC_BEFORE_TRIMMING/{sample}.benchmark.txt\""
  },
  {
    "objectID": "course-style.html#logs",
    "href": "course-style.html#logs",
    "title": "3  How to follow the Course?",
    "section": "3.4 Logs",
    "text": "3.4 Logs\nIf you have problems with your snakemake run, each rule will have a log file. So you can check the errors:\n    log:\n        \"logs/FASTQC_BEFORE_TRIMMING/{sample}.log\","
  },
  {
    "objectID": "qc.html#quality-control-before-trimming",
    "href": "qc.html#quality-control-before-trimming",
    "title": "4  Quality control and trimming",
    "section": "4.1 Quality control before trimming",
    "text": "4.1 Quality control before trimming\nThe pipeline starts with quality control step. We will first run FastQC tool to check the qualitu of the fastq files.\nThis is the rule in the workflow/rules/qc.smk file. Input files are retrieved from the samplesheet that you provided. And you will have two output files html and zip files. The html file contains the final report.\nrule FastQC_BeforeTrimming:\n    \"\"\"Run fastq before trimming\"\"\"\n    output:\n        html=\"results/FASTQC_BEFORE_TRIMMING/{sample}_fastqc.html\",\n        zip=\"results/FASTQC_BEFORE_TRIMMING/{sample}_fastqc.zip\",\n    input:\n        fastq=lambda wildcards: samples.loc[wildcards.sample].fastq,\n    message:\n        \"FastQC_BeforeTrimming: RUNNING QUALITY CONTROL WITH FASTQC FOR SAMPLE {input.fastq} BEFORE TRIMMING ADAPTERS\"\n    threads: 2\n    shell:\n        \"fastqc {input.fastq} --threads {threads} --nogroup --outdir results/FASTQC_BEFORE_TRIMMING &> {log}\"\n\nHere is a simplified version of this code:\nfastqc FASTQ --threads 2 --nogroup --outdir results/FASTQC_BEFORE_TRIMMING/"
  },
  {
    "objectID": "qc.html#quality-trimming-step",
    "href": "qc.html#quality-trimming-step",
    "title": "4  Quality control and trimming",
    "section": "4.2 Quality trimming step",
    "text": "4.2 Quality trimming step\nAfter we run quality controls. We will run cutadapt tool to remove the leftover adapters and low quality bases. Here is the overview of this rule.\nYou can add or remove adapters using the config/config.yaml file.\nIn this file the inputs will be retrieved again from the samplesheet file. And the outputs will be placed to the results/CUTADAPT_ADAPTER_TRIMMING/ folder.\nrule Cutadapt_Adapter_Trimming:\n    output:\n        fastq=\"results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz\",\n    input:\n        fastq=lambda wildcards: samples.loc[wildcards.sample].fastq,\n    params:\n        adapters=\" \".join([f\"-a {x}\" for x in ADAPTERS]),\n    message:\n        \"Cutadapt_Adapter_Trimming: TRIMMING ADAPTERS FOR SAMPLE {input.fastq} WITH CUTADAPT\"\n    threads: 1\n    shell:\n        \"cutadapt {params.adapters} --minimum-length 30 -o {output.fastq} {input.fastq} &> {log}\"\n\nHere is how we will run it for one file:\ncutadapt -a ADAPTERS --minimum-length 30 -o results/CUTADAPT_ADAPTER_TRIMMING/sample"
  },
  {
    "objectID": "qc.html#quality-control-after-trimming",
    "href": "qc.html#quality-control-after-trimming",
    "title": "4  Quality control and trimming",
    "section": "4.3 Quality control after trimming",
    "text": "4.3 Quality control after trimming\nThen we will run the quality control after the trimming step:\nrule FastQC_AfterTrimming:\n    output:\n        html=\"results/FASTQC_AFTER_TRIMMING/{sample}.trimmed_fastqc.html\",\n        zip=\"results/FASTQC_AFTER_TRIMMING/{sample}.trimmed_fastqc.zip\",\n    input:\n        fastq=\"results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz\",\n    threads: 2\n    message:\n        \"FastQC_AfterTrimming: RUNNING QUALITY CONTROL WITH FASTQC FOR SAMPLE {input.fastq} AFTER TRIMMING ADAPTERS\"\n    shell:\n        \"fastqc {input.fastq} --threads {threads} --nogroup --outdir results/FASTQC_AFTER_TRIMMING &> {log}\"\nHere is how we will run it for one file:\nfastqc results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz --threads 2 --nogroup --outdir results/FASTQC_AFTER_TRIMMING"
  },
  {
    "objectID": "qc.html#combine-quality-control-outputs",
    "href": "qc.html#combine-quality-control-outputs",
    "title": "4  Quality control and trimming",
    "section": "4.4 Combine quality control outputs",
    "text": "4.4 Combine quality control outputs\nAt last we will combine all the fastqc reports into one file using Multifastqc tool:\nrule MultiQC:\n    \"\"\"Run MultiQC\"\"\"\n    output:\n        html=\"results/MULTIQC/multiqc_report.html\",\n    input:\n        unpack(multiqc_input),\n    params:\n        config=os.path.join(WORKFLOW_DIR, \"envs\", \"multiqc_config.yaml\"),\n    envmodules:\n        *config[\"envmodules\"][\"multiqc\"],\n    benchmark:\n        \"benchmarks/MULTIQC/MULTIQC.benchmark.txt\"\n    message:\n        \"MultiQC: COMBINING QUALITY CONTROL METRICS WITH MULTIQC\"\n    shell:\n        'echo {input} | tr \" \" \"\\n\" > {output.html}.fof;'\n        \"multiqc -c {params.config} -l {output.html}.fof --verbose --force --outdir results/MULTIQC &> {log}\""
  },
  {
    "objectID": "krakenuniq.html#run-krakenuniq",
    "href": "krakenuniq.html#run-krakenuniq",
    "title": "5  KrakenUniq database",
    "section": "5.1 Run KrakenUniq",
    "text": "5.1 Run KrakenUniq\nThis is the rule in the workflow/rules/krakenuniq.smk file. Input files are retrieved from the output of the Quality Control step.\nrule KrakenUniq:\n    \"\"\"Run KrakenUniq on trimmed fastq data\"\"\"\n    output:\n        report=\"results/KRAKENUNIQ/{sample}/krakenuniq.output\",\n        seqs=\"results/KRAKENUNIQ/{sample}/sequences.krakenuniq\",\n    input:\n        fastq=\"results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz\",\n    threads: 10\n    message:\n        \"KrakenUniq: PERFORMING TAXONOMIC CLASSIFICATION OF SAMPLE {input.fastq} WITH KRAKENUNIQ\"\n    shell:\n        \"krakenuniq --preload --db {params.DB} --fastq-input {input.fastq} --threads {threads} --output {output.seqs} --report-file {output.report} --gzip-compressed --only-classified-out &> {log}\"\nHere is a simplified version of this code:\nkrakenuniq --db $DBNAME --fastq-input $SAMPLE --threads 10 --output $SAMPLE.sequences.krakenuniq --report-file $SAMPLE.krakenuniq.output --gzip-compressed --only-classified-out\nAfter running KrakenUniq, the next step is to filter the output. The input is the krakenuniq.output generated in the previous step.\nrule Filter_KrakenUniq_Output:\n    output:\n        filtered=\"results/KRAKENUNIQ/{sample}/krakenuniq.output.filtered\",\n        pathogens=\"results/KRAKENUNIQ/{sample}/krakenuniq.output.pathogens\",\n        pathogen_tax_id=\"results/KRAKENUNIQ/{sample}/taxID.pathogens\",\n    input:\n        krakenuniq=\"results/KRAKENUNIQ/{sample}/krakenuniq.output\",\n        pathogenomesFound=config[\"pathogenomesFound\"],\n    message:\n        \"Filter_KrakenUniq_Output: APPLYING DEPTH AND BREADTH OF COVERAGE FILTERS TO KRAKENUNIQ OUTPUT FOR SAMPLE {input}\"\n    shell:\n        \"\"\"{params.exe} {input.krakenuniq} {params.n_unique_kmers} {params.n_tax_reads} {input.pathogenomesFound} &> {log}; \"\"\"\n        \"\"\"cut -f7 {output.pathogens} | tail -n +2 > {output.pathogen_tax_id}\"\"\"\ncd $OUTPUT; Rscript $PATH_TO_SCRIPTS/pipeline.R\ncut -f7 $OUTPUT/krakenuniq.output.pathogens | tail -n +2 > $OUTPUT/taxID.pathogens\ncat $OUTPUT/taxID.pathogens | parallel \"${PATH_TO_KRAKENUNIQ}/./krakenuniq-extract-reads {} $OUTPUT/sequences.krakenuniq ${SAMPLE} > $OUTPUT/{}.temp.fq\"\necho \"MeanReadLength\" > $OUTPUT/mean.reads.length; cd $OUTPUT\nfor i in $(cat taxID.pathogens); do awk '{if(NR%4==2) print length($1)}' ${i}.temp.fq | awk '{ sum += $0 } END { if (NR > 0) print sum / NR }' >> mean.reads.length; done; rm *.temp.fq\n\npaste krakenuniq.output.pathogens mean.reads.length > krakenuniq.output.pathogens_with_mean_read_length\ncat krakenuniq.output.pathogens_with_mean_read_length\nWe then visualize our filtered output using Krona:\nrule KrakenUniq2Krona:\n    output:\n        tax_ids=\"results/KRAKENUNIQ/{sample}/krakenuniq.output.filtered_taxIDs_kmers1000.txt\",\n        seqs=\"results/KRAKENUNIQ/{sample}/sequences.krakenuniq_kmers1000.txt\",\n        krona=\"results/KRAKENUNIQ/{sample}/sequences.krakenuniq_kmers1000.krona\",\n        html=\"results/KRAKENUNIQ/{sample}/taxonomy.krona.html\",\n    input:\n        report=\"results/KRAKENUNIQ/{sample}/krakenuniq.output.filtered\",\n        seqs=\"results/KRAKENUNIQ/{sample}/sequences.krakenuniq\",\n    params:\n        exe=WORKFLOW_DIR / \"scripts/krakenuniq2krona.py\",\n        DB=f\"--tax {config['krona_db']}\" if \"krona_db\" in config else \"\",\n    message:\n        \"KrakenUniq2Krona: VISUALIZING KRAKENUNIQ RESULTS WITH KRONA FOR SAMPLE {input.report}\"\n    shell:\n        \"{params.exe} {input.report} {input.seqs} &> {log}; \"\n        \"cat {output.seqs} | cut -f 2,3 > {output.krona}; \"\n        \"ktImportTaxonomy {output.krona} -o {output.html} {params.DB} &>> {log}\"\n\nLastly, we create an Abundance matrix:\nrule KrakenUniq_AbundanceMatrix:\n    output:\n        out_dir=directory(\"results/KRAKENUNIQ_ABUNDANCE_MATRIX\"),\n        unique_species=\"results/KRAKENUNIQ_ABUNDANCE_MATRIX/unique_species_taxid_list.txt\",\n        unique_species_names=\"results/KRAKENUNIQ_ABUNDANCE_MATRIX/unique_species_names_list.txt\",\n        abundance_matrix=\"results/KRAKENUNIQ_ABUNDANCE_MATRIX/krakenuniq_abundance_matrix.txt\",\n        abundance_plot=\"results/KRAKENUNIQ_ABUNDANCE_MATRIX/krakenuniq_absolute_abundance_heatmap.pdf\",\n    input:\n        expand(\"results/KRAKENUNIQ/{sample}/krakenuniq.output.filtered\", sample=SAMPLES),\n    params:\n        exe=WORKFLOW_DIR / \"scripts/krakenuniq_abundance_matrix.R\",\n        exe_plot=WORKFLOW_DIR / \"scripts/plot_krakenuniq_abundance_matrix.R\",\n        n_unique_kmers=config[\"n_unique_kmers\"],\n        n_tax_reads=config[\"n_tax_reads\"],\n    message:\n        \"KrakenUniq_AbundanceMatrix: COMPUTING KRAKENUNIQ MICROBIAL ABUNDANCE MATRIX\"\n    shell:\n        \"Rscript {params.exe} results/KRAKENUNIQ {output.out_dir} {params.n_unique_kmers} {params.n_tax_reads} &> {log};\"\n        \"Rscript {params.exe_plot} {output.out_dir} {output.out_dir} &> {log}\"\nprintf \"\\n\"; echo \"PERFORMING ALIGNMENT TO PATHO-GENOME\"\ntime bowtie2 --large-index -x $PATHO_GENOME --threads 10 --end-to-end --very-sensitive -U $SAMPLE | samtools view -bS -q 1 -h -@ 10 - > $OUTPUT/test_sample_AlignedToSpecies.bam\nsamtools sort $OUTPUT/test_sample_AlignedToSpecies.bam -@ 10 > $OUTPUT/test_sample_AlignedToSpecies.sorted.bam; samtools index $OUTPUT/test_sample_AlignedToSpecies.sorted.bam\nsamtools markdup -r $OUTPUT/test_sample_AlignedToSpecies.sorted.bam $OUTPUT/test_sample_AlignedToSpecies.sorted.dedup.bam; samtools index $OUTPUT/test_sample_AlignedToSpecies.sorted.dedup.bam\n \nprintf \"\\n\"; echo \"EXTRACTING ALIGNMENTS FOR CANDIDATES\"\ncat $OUTPUT/taxID.pathogens | parallel \"grep -w {} ${PATH_TO_PATHO_GENOME}/seqid2taxid.pathogen.map | cut -f1 > ${OUTPUT}/{}.seq.ids\"\nfor i in $(cat $OUTPUT/taxID.pathogens); do samtools view -bh $OUTPUT/test_sample_AlignedToSpecies.sorted.dedup.bam -@ 10 $(cat $OUTPUT/${i}.seq.ids | tr \"\\n\" \" \") > $OUTPUT/${i}.output.bam; done\n \nprintf \"\\n\"; echo \"RUNNING MAPDAMAGE ANCIENT STATUS ANALYSIS\"\nfind . -name '*.output.bam' | parallel \"mapDamage -i {} -r ${PATHO_GENOME} --merge-reference-sequences -d ${OUTPUT}/results_{}\"\n \nprintf \"\\n\"; echo \"ASSIGN ANCIENT STATUS\"\nRscript $PATH_TO_SCRIPTS/ancient_status.R 0.05 0.9 $OUTPUT\nprintf \"\\n\"; echo \"COMPUTE DEPTH AND BREADTH OF COVERAGE FROM ALIGNMENTS\"\necho \"NUMBER_OF_READS\" > DepthOfCoverage.${FASTQ_FILE}.txt; echo \"GENOME_LENGTH\" > GenomeLength.${FASTQ_FILE}.txt; echo \"BREADTH_OF_COVERAGE\" > BreadthOfCoverage.${FASTQ_FILE}.txt\nfor j in $(cut -f7 final_output.txt | sed '1d')\ndo\necho \"Organism $j\"\nif [ -s ${j}.output.bam ] && [ \"$(samtools view ${j}.output.bam | wc -l)\" -ne \"0\" ];\nthen\nsamtools sort ${j}.output.bam > ${j}.output.sorted.bam; samtools depth ${j}.output.sorted.bam | cut -f1 | uniq > Genomes_${j}.txt\nGENOME_LENGTH=$(grep -wFf Genomes_${j}.txt $PATH_TO_PATHO_GENOME/GenomeLength.txt | cut -f2 | awk '{ sum += $1; } END { print sum; }')\nif [ -s ${j}.output.sorted.bam ];\nthen\nNUMBER_OF_READS=$(samtools view ${j}.output.sorted.bam | wc -l); NUMBER_OF_COVERED_POSITIONS=$(samtools depth ${j}.output.sorted.bam | wc -l)\nelse\necho \"NA\" >> DepthOfCoverage.${FASTQ_FILE}.txt; echo \"NA\" >> BreadthOfCoverage.${FASTQ_FILE}.txt; echo \"NA\" >> GenomeLength.${FASTQ_FILE}.txt\ncontinue\nfi\necho $NUMBER_OF_READS >> DepthOfCoverage.${FASTQ_FILE}.txt; echo $GENOME_LENGTH >> GenomeLength.${FASTQ_FILE}.txt\necho \"scale=10 ; ($NUMBER_OF_COVERED_POSITIONS / $GENOME_LENGTH)*100\" | bc >> BreadthOfCoverage.${FASTQ_FILE}.txt\nelse\necho \"NA\" >> DepthOfCoverage.${FASTQ_FILE}.txt; echo \"NA\" >> BreadthOfCoverage.${FASTQ_FILE}.txt; echo \"NA\" >> GenomeLength.${FASTQ_FILE}.txt\ncontinue\nfi\ndone; rm Genomes_*.txt; paste final_output.txt DepthOfCoverage.${FASTQ_FILE}.txt GenomeLength.${FASTQ_FILE}.txt BreadthOfCoverage.${FASTQ_FILE}.txt > final_output_corrected.txt\n \ncp -r $OUTPUT /proj/snic2018-8-150/uppstore2018095/private/NBIS_Demo; cat final_output_corrected.txt\nprintf \"\\n\"; echo \"PIPELINE FINISHED SUCCESSFULLY\"\n\n5.1.1 Kraken2 vs KrakenUniq\nKraken2 is incredibly fast but unfortunately has a high False Positive Rate. KrakenUniq reduces the fraction of false discoveries (compared to Kraken2) by reporting the number of unique k-mers that is analogous to breadth of coverage information. Therefore filtering KrakenUniq output by number of unique k-mers, we get a more reliable list of candidates compared to Kraken2 (the output from Kraken2 can only be filtered by microbial abundance, i.e. depth of coverage). Considering filtered KrakenUniq output as a ground truth, we investigated how many records from Kraken2 output should be retained in order to capture all the species identified with KareknUniq, i.e. how long down the Kraken2 list one needs to go in order to detect all the species reported by KrakenUniq.\nWhat we can conclude after having screened ~100 samples is that a typical sample contains ~100 species reported (after filtering) by KrakenUniq. This corresponds to approximately ~1000 species in Kraken2 output (or 0.01% assigned reads). This implies that if we select top 1000 species in Kraken2, 900 species will probably be false positives and only 10% of all species will be true positives. If we consider KrakenUniq output as a ground truth."
  },
  {
    "objectID": "align.html#bowtie2",
    "href": "align.html#bowtie2",
    "title": "6  Fast alignment with Bowtie2",
    "section": "6.1 Bowtie2",
    "text": "6.1 Bowtie2\nThe pipeline has a side branch for rapid analysis of the results based on an alignment with Bowtie2 and a post-mortem damage estimation with MapDamage. For this project, we won’t be able to use the real Bowtie2 databases, because there were too heavy for the resources allocated for this workshop. So we are only going to look at the code but not run it and the output will be provided for you in the folder containing the expected results."
  },
  {
    "objectID": "align.html#bowtie2-build-l-to-build-the-index-files",
    "href": "align.html#bowtie2-build-l-to-build-the-index-files",
    "title": "6  Fast alignment with Bowtie2",
    "section": "6.2 Bowtie2-build-l to build the index files",
    "text": "6.2 Bowtie2-build-l to build the index files\nIn order to run a Bowtie2 alignment, one needs a complete Bowtie2 database, in other words a .fna (fasta) file that has been indexed using the command bowtie2-build-l. This is the first part of the pipeline for the alignment step. You can therefore provide your own merged fna file for Bowtie2 to index if you wish to. However, if you are using one of our own custom databases, this step will be automatically ignored because its output already exists.\nYou can find the Bowtie2_build rule generating the index files below. It takes as input the path to the library.fna file that you have provided for the variable bowtie2_patho_db in your config.yaml file. For more information, refer to the main aMeta GitHub in the section about the config file. The rule then generates the index files of the Bowtie2 database if they don’t already exist.\nrule Bowtie2_Index:\n    output:\n        expand(\n            f\"{config['bowtie2_patho_db']}{{ext}}\",\n            ext=[\n                \".1.bt2l\",\n                \".2.bt2l\",\n                \".3.bt2l\",\n                \".4.bt2l\",\n                \".rev.1.bt2l\",\n                \".rev.2.bt2l\",\n            ],\n        ),\n    input:\n        ref=ancient(config[\"bowtie2_patho_db\"]),\n    log:\n        f\"{config['bowtie2_patho_db']}_BOWTIE2_BUILD.log\",\n    shell:\n        \"\"bowtie2-build-l --threads {threads} {input.ref} {input.ref} > {log} 2>&1\nHere is a simplified version of this code:\nbowtie2-build-l --threads 1 resources/library.fna resources/library.fna > logs/Bowtie2_Build.log 2>&1\nWARNING: No need to execute that line of code as we haven’t downloaded the real databases to the server for this workshop and it would overwrite the index files! The trick for this rule is that the input file is declared as ancient with the “ancient” function of snakemake, making snakemake not rerun the rule if the output files already exists. But if you only run the bash command provided, you will overwrite the files.\n\n6.2.1 Regarding real life projects with the actual databases\nFor real life projects, these are the Bowtie2 databases we have made available for download:\n\nPathogenome: Bowtie2 index and helping files for following up on microbial pathogens\nBowtie2_Full_NT: Bowtie2 index for full NCBI NT (for quick followup of prokaryotes and eukaryotes; also contains helping files for building the Malt database)\n\nFor more information and links to download the databases, please refer to the official GitHub of aMeta.\nWARNING: if you are using the Bowtie2_Full_NT database, make sure that you have unzipped the files before running the pipeline, otherwise it will automatically overwrite the index files to regenerate them and will need a huge amount of memory and time to succeed, which you probably don’t want to happen ;-)"
  },
  {
    "objectID": "align.html#bowtie2-alignment",
    "href": "align.html#bowtie2-alignment",
    "title": "6  Fast alignment with Bowtie2",
    "section": "6.3 Bowtie2 alignment",
    "text": "6.3 Bowtie2 alignment\nrule Bowtie2_Pathogenome_Alignment:\n    output:\n        bam=\"results/BOWTIE2/{sample}/AlignedToPathogenome.bam\",\n        bai=\"results/BOWTIE2/{sample}/AlignedToPathogenome.bam.bai\",\n    input:\n        fastq=\"results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz\",\n        db=rules.Bowtie2_Index.output,\n    params:\n        PATHO_DB=lambda wildcards, input: config[\"bowtie2_patho_db\"],\n    log:\n        \"logs/BOWTIE2/{sample}.log\",\n    shell:\n        \"\"\"bowtie2 --large-index -x {params.PATHO_DB} --end-to-end --threads 10 --very-sensitive -U {input.fastq} 2> {log} | samtools view -bS -q 1 -h -@ 10 - | samtools sort -@ 10 -o {output.bam} >> {log};\"\"\"\n        \"\"\"samtools index {output.bam}\"\"\"\nHere is a simplified version of this code:\nbowtie2 --large-index -x resources/library.fna --end-to-end --threads 10 --very-sensitive -U results/CUTADAPT_ADAPTER_TRIMMING/{sample}.trimmed.fastq.gz 2> logs/BOWTIE2/{sample}.log | samtools view -bS -q 1 -h -@ 10 - | samtools sort - -@ 10 -o results/BOWTIE2/{sample}/AlignedToPathogenome.bam >> logs/BOWTIE2/{sample}.log\nsamtools index results/BOWTIE2/{sample}/AlignedToPathogenome.bam\nAgain, no need to run that line of code. You can find how the output is supposed to look like in the folder with expected results. Basically, this line asks Bowtie2 to align each fastq file to the Bowtie2 database. This will generate a sam file containing the information of alignment for each DNA sequence and the reference genome to which it aligns to. The sam file is then directly changed into a bam file using samtools, sorted and later indexed so that it is ready to be used. In order to run this line, you would still need to replace {sample} with the name of each sample that you have provided in the samples.tsv file and run this line for each of them."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Pochon, Zoé, Nora Bergfeldt, Emrah Kırdök, Mário Vicente, Thijessen\nNaidoo, Tom van der Valk, N. Ezgi Altınışık, et al. 2022. “aMeta:\nAn Accurate and Memory-Efficient Ancient Metagenomic Profiling\nWorkflow.” bioRxiv. https://doi.org/10.1101/2022.10.03.510579."
  }
]